## 2.3 Data Acquisition: CDLI

### 2.3.1 CDLI Data

The Cuneiform Digital Library Initiative, was created by Bob Englund (UCLA) in the early two thousands. [CDLI][] is a central repository for meta-data, images, and transliterations of cuneiform objects (translations are offered only for a small minority of texts). Today more than 335,000 objects are listed in the [CDLI][] catalog, with tens of thousands of photographs and line drawings. Each object in [CDLI][] receives a unique ID number, and these numbers are widely used today in print and in online projects. Initially, [CDLI][] focused primarily on administrative texts from the third millennium, and this is still the area of its greatest strength. Currently, approximately 121,000 texts are available in transliteration in [CDLI][]. Part of this corpus was produced by the [CDLI][] team at UCLA, others were contributed by partners or were imported from other projects such as [ETCSL][] (for Sumerian literary texts) and [DCCLT][] (for lexical texts).  The photographs on [CDLI][] were largely produced in cooperative projects with museums all over the world, where [CDLI][] staff or partners would go to scan an entire collection or major parts of a collection. These images are copyright of the museum where the object is held and there is no wholesale downloading of the entire image set.

Many of the fields in the [CDLI][] catalog either use a restricted vocabulary (period, genre) or have been standardized to a great degree (provenance, author's name, owner, museum number), greatly facilitating search. 

The issue of standardization is much more difficult for linguistic data in transliteration. Here, Sumerian and Akkadian pose rather different challenges. For Sumerian, there are two main issues. First, Sumerologists tend to use different sets of conventions for representing Sumerian words in the Latin alphabet. The word for "to give" is read **šum₂** by some, but **sum** by others. Similarly, the word for "ox" is read either **gud** or **gu₄**. These readings (**šum₂** vs **sum** or **gud** vs. **gu₄**) represent the same word and render the same sign - they simply differ in modern transliteration conventions. Variation in such conventions has grown recently by the introduction of a new set of readings by P. Attinger (Bern), which has received wide following, in particular in Germany. Such variation in sign readings is based on the one hand on differing interpretations of the data from ancient sign lists (which provide transcriptions of Sumerian words in Akkadian) and on the other hand on the definition of what an ideal transliteration should do (whether it should represent the abstract lexeme, or rather its concrete pronunciation, or something in between). For the [CDLI][] search engine, which is based on a FileMaker database, such variation presents a problem when searching for (Sumerian) words. The solution has been to strictly impose a set of "preferred sign readings" (available on the web site), a policy that has been carried out with great consistency. Second, Sumerian has no good standard for word segmentation. In the [CDLI][] data set one may find the word **ninda-i₃-de₂-a** (a pastry) transliterated as **ninda-i₃-de₂-a**, **ninda i₃-de₂-a**, **ninda i₃ de₂-a**, **nig₂-i₃-de₂-a**, **nig₂ i₃ de₂-a**, etcetera (**nig₂** and **ninda** are two different words, written by the same sign and there is no full agreement which of these readings is to be used in this particular word). None of these various renderings is necessarily "wrong", because we know fairly little about  the formation and segmentation of Sumerian nouns. For computational approaches this variation poses an important challenge.

For Akkadian the variation in reading conventions plays a much smaller role; for most dialects of Akkadian (with the exception of Old Akkadian) scholars generally agree on transliteration conventions and word segmentation is hardly ever a problem for Akkadian. For search engines, however, Akkadian is much more difficult to deal with because the same word may be spelled in many different ways. Without lemmatization, there is no way a machine can tell that ***ša-ar-ru-um***, ***šar-ru*** and **LUGAL**, etc. all represent the same lemma for "king" in syllabic and logographic writing. 

Since [CDLI][] does not offer lemmatization, searching for words on this site is much more popular (and more useful) for Sumerian than it is for Akkadian. Sumerian words usually include the root of the word (written logographically) with prefixes and/or suffices attached. Although spelling variations exist (e.g. **dag-si**, **da-ag-ši-um**, and **da-ag-zi-um**, all representing the same word for saddle hook or saddle bag), they play a much smaller role than in Akkadian.

### 2.3.2 Downloading

There are various ways in which one can acquire [CDLI][] data. The website includes a Downloads page where one can get access to a daily clone of the catalog and the entire set of transliterations. Alternatively, one can perform a search on the [CDLI][] search page and request a download of the data (transliteration or catalog and transliteration data) by pushing a button. This works well for a few or several dozens of texts, but not for very large data sets.

The daily data dump is available from https://cdli.ucla.edu/bulk_data/ or from [Github][] at https://github.com/cdli-gh/data/raw/master/. Currently, the set of transliterations is offered in one big file, named `cdliàtf_unblocked.atf `. The catalog is split into two files because of file-size limitations at [Github][]; they are named `cdli_catalogue_1of2.csv` and `cdli_catalogue_2of2.csv`, respectively. The files need to be concatenated before they can be used.

The script for downloading these files (2_3_Data_Acquisition_CDLI.ipynb) is not essentially different from the script for downloading [ORACC][] files, as discussed in section 2.1. 

### 2.3.3 Manipulating

After downloading, the catalog file can be ingested in a `pandas` DataFrame for further manipulation. At the moment of writing, a slight complication prevented `pandas` from doing so directly. It appears that one record was accidentally added on the same line as the preceding entry - resulting in a line with 115 fields (rather than the standard 63). The `pandas` package does not deal well with `csv` files that are irregularly shaped and issued an error message when trying to load the dataset with the `from_csv()` function. This issue can be circumvented by reading the dataset with the `csv` library and discarding any field beyond field 63, resulting in the loss of the one erratic entry.

The catalog can be used to create a sub-set of the [CDLI][] transliteration file by finding the P numbers (text IDs) that belong to, for instance, texts from Ebla, or texts dating to the ED IIIa period (as in the example below).

The field "id_text" holds the text ID number as a string, without the preceding "P" and without padding zeroes to the left. The text ID "P001023" is thus represented as "1023".  The function `zfill()` adds the padding zeroes to create a six-digit number as a string.

```python
ed3a = cat.loc[cat["period"].str[:7] == "ED IIIa"]
pnos = list(ed3a["id_text"].str.zfill(6))
with open("cdlidata/cdliatf_unblocked.atf", encoding="utf8") as c: 
    lines = c.readlines()
keep = False
ed3a_atf = []
for line in lines:
    if line[0] == "&": 
        if line[2:8] in pnos: 
            keep = True
        else: 
            keep = False
    if keep: 
        ed3a_atf.append(line)
```

This will represent the ED IIIa corpus as a list, where each line in the original ATF document is represented by one row. The following code will transform this list into a format where each text is a row in a `pandas` DataFrame, with the text ID in column 1, and the transliteration in column 2 (as a single string, without line numbers or line demarcations).

```python
docs = []
doc = []
d = ""
id_text = None
for line in ed3a_atf:
    if line[0] == "&":  # line beginning with & marks a new document
        if id_text:     # add the preceding document to the list of documents
            doc = [id_text, d.strip()]
            docs.append(doc)
            d = ""
        id_text = line[1:8] 						# retrieve the P number
    elif line [0] in ["#", "$", "<", ">", "@"]:  	# skip all non-transliteration lines
        continue
    else:
        start = line.find(". ")						# retrieve the position of the end of the line number
        line = line[start + 1:].rstrip()
        d = d + line								# variable "d" will contain the transliteration as one string
doc = [id_text, d.strip()]  # take care of the last text in the list
docs.append(doc)
ed3a_df = pd.DataFrame(docs)
ed3a_df.columns = ["id_text", "transliteration"]
```



## 2.4 Data Acquisition BDTNS

The Database of Neo-Sumerian Text ([BDTNS][]) was created by Manuel Molina (Consejo Superior de Investigaciones Científicas). The site provides a detailed catalog of the administrative, legal, and epistolary  tablets from the so-called Ur III period (21st century BCE). Molina estimates that museums and private collections all over the world may hold at least 120,000 such documents, not including the holdings of the Iraq Museum (Baghdad). Currently, almost 65% of those documents are available through [BDTNS][] in transliteration, and/or in photograph and line drawing. 

There is a considerable overlap in the data sets offered by [CDLI][] and [BDTNS][]. In the majority of cases the photographs and line drawings on [BDTNS][] are imported from [CDLI][]. The initial core of the [BDTNS][] transliterations was provided by Remco de Maaijer and Bram Jagersma, who prepared tens of thousands of Ur III texts and distributed those data freely. These same transliterations are also found on [CDLI][]. Close cooperation between the two projects has led to further exchange of data.

Still, [BDTNS][] is not simply a duplicate of the Ur III data in [CDLI][]. Most Ur III scholars today prefer [BDTNS][] over [CDLI][] because the smaller focus of the Spanish project implies that there is more attention to detail and that more effort is made to update the record. One example is the book *Der König und sein Kreis* (2012)[^1] in which Paola Paoletti studied in detail several hundred documents from the so-called treasure archive. This archive reports on the manufacturing of luxury objects made of precious metals and leather and includes many rare words. Since the archive (like almost all Ur III archives) is scattered in museums all over the world, most of these texts were published as singular documents or in small groups. Studying the entire group frequently allowed Paoletti to arrive at a more satisfying reading and understanding than the original editor's. The [BDTNS][] editions of these texts reflect Paoletti's improvements, but the [CDLI][] editions do not.

The [BDTNS][] data can be downloaded by hand through the Search option in the Catalogue & Transliterations drop-down menu.  One can search by a variety of criteria (including word and grapheme strings) and then download the search results by clicking on the Export button. The export page will provide options for the type of information to include (various types of meta-data and/or transliterations). By searching for a blank string one may export the entire data set. The export yields two files: one for the meta-data and one for the  transliterations, both in raw text (`.txt`) format.

The [BDTNS][] transliteration files need some cleaning before they can be used for computational text analysis. First, they contain "vertical TABs", represented by ^K, \v, or \x0b (depending on which program is used for reading the file). These "vertical TABS" are used between lines that belong to the same text and they will lead to somewhat unpredictable results when we try to ingest and manipulate the data in `pandas`. In order to replace those we need the `re` package (for Regular Expressions).

```python
import re
with open("query_text_19_03_1-210747.txt", encoding="utf8") as b: 
    bdtns = b.readlines()
for n, line in enumerate(bdtns): 
    bdtns[n] = re.sub("\v", "\n", line)   # replace all vertical TABs (\v) by the new line character.
```

A second peculiarity of the [BDTNS][] data set is the way so-called x-values are represented. In Assyriology, x-values are sign readings that have not (yet) received a proper index number. For instance, the (very common) word  for "to cut (reeds)" is written either with the sign **zi** or with the sign **SIG₇**. Based on the distribution of those spellings (**SIG₇** only in Umma), M. Molina and M. Such-Guttiérez (2004)[^2] concluded that both spellings write the same word /**zi**/. On that basis the new reading **/zi/** for the sign **SIG₇** was introduced (and is now commonly accepted among Sumerologists). In such cases one may transliterate **ziₓ(SIG₇)** where the SIG₇ between brackets explains which sign is represented by **ziₓ** (and thus the principle of a one-to-one mapping of a transliterated token to a cuneiform sign is maintained). In the [BDTNS][] export file this is represented as follows: 

> > ​	o. 2     gi ziX-a 12 sar-⌈ta⌉ (=SIG7)		"cut reed per 12 *sar* of field"

The indexed ₓ is represented by a capital X (as in ziX), and the sign explanation is added at the end of the line. 

In order to use this data for computational purposes (for instance computing sign frequencies) it is necessary to move the sign specification and to transform this into

> ​	o. 2     gi ziₓ(SIG7)-a 12 sar-⌈ta⌉ 

It is possible to do so with a so-called regular expression (or regex) which will search a string for the occurrence of a pattern. The `sub()` (substitute) function of the `re` (regular expressions) library  allows one to reorder and manipulate the found data. The first expression will replace a capital X by a subscript x (ₓ), but only when preceded by a letter valid in Sumerian or Akkadian. This will exclude cases where X indicates an illegible sign or an illegible inscribed sign (as in KA×X).

```python
bdtns = [re.sub('([abdegŋhḫijklmnpqrstuwzšṣṭABDEGŊHḪIJKLMNPQRSTUWZŠṢṬ])X', '\\1ₓ', line) for line in bdtns]
bdtns = [re.sub('ₓ([^=]*)\(=([^\)]*)\)(?:\s|$)', 'ₓ(\\2)\\1', line) for line in lines]
```
*The second expression is more complicated. It contains the following elements

- ₓ		 This will match one subscripted x (Unicode 2093).
- (\[^=]*)	This is a group (indicated by the brackets) that will match 0 or more characters that are not the equal sign.
- \\(=	      This will match the literal opening bracket (the preceding backslash is an escape, indicating that this is not the beginning of a new group) followed by an equal sign.
- (\[^\)]*)          This is the second group, matching 0 or more characters that are not a closing bracket.
- \\)                 This matches the literal closing bracket.
- (?:\s|$)        The positive look-ahead (indicated by (?:)) that checks to that what follows is either a space (\s) or the end of the string (\$).

In our example above the uppercase letter X has been replaced by an subscripted ₓ, so that the string now looks thus:

> o. 2     gi ziₓ-a 12 sar-⌈ta⌉ (=SIG7)
>
> In this string

- ₓ matches "ₓ"
- (\[^=]*) matches "-a 12 sar-⌈ta⌉ " 
- \\(=  matches "(="
- (\[^\)]*) matches "SIG7"
- \\) matches ")"
- (?:\s|$)  matches because this is the end of the string

There are various books and web sites that discuss the intricacies of regular expressions. Regular expressions are very powerful, and particularly helpful for data cleaning. The web site [regex101](regex101.com) is very useful for testing regular expressions; it gives reference information about all the special characters and operators, as well as a visual interpretation of the expression on a test string..



[^1]: Paola Paoletti, *Der König und sein Kreis: das staatliche Schatzarchiv der III. Dynastie von Ur*, Biblioteca del próximo oriente antiguo 10. Madrid: 2012.

[^2]: Molina, Manuel and Such-Gutiérrez, Marcos, On Terms for Cutting Plants and Noses in Ancient Sumer: *Journal of Near Eastern Studies* 63 (2004) 1-16