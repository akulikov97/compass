## 2.3 Data Acquisition: CDLI

### 2.3.1 CDLI Data

The Cuneiform Digital Library Initiative, was created by Bob Englund (UCLA) in the early twothousands. [CDLI][] is a central repository for meta-data, images, and transliterations of cuneiform objects (translations are offered only for a small minority of texts). Today more than 335,000 objects are listed in the [CDLI][] catalog, with tens of thousands of photographs and line drawings. Each object in [CDLI][] receives a unique ID number, and these numbers are widely used today in print and in other online projects. Initially, CDLI focused primarily on administrative texts from the third millennium, and this is still the area of its greatest strength. Currently, approximately 121,000 texts are available in transliteration in [CDLI][]. Part of this corpus was produced by the [CDLI][] team at UCLA, others were contributed by partners or were imported from other projects such as [ETCSL][] (for Sumerian literary texts) and [DCCLT][] (for lexical texts).  The photographs on [CDLI][] were largely produced in cooperative projects with museums all over the world, where [CDLI][] staff or partners would go to scan an entire collection or major parts of a collection. These images are copyright of the museum where the object is held and there is no wholesale downloading of the entire image set.

Many of the fields in the [CDLI][] catalog either use a restricted vocabulary (period, genre) or have been standardized to a great degree (provenance, author's name, owner, museum number), greatly facilitating search. 

The issue of standardization is much more difficult for linguistic data in transliteration. Here, Sumerian and Akkadian pose rather different challenges. For Sumerian, there are two main issues. First Sumerologists tend to use different sets of conventions for repreneting Sumerian words in the Latin alphabet. The word for "to give" is read **šum₂** by some, but **sum** by others. Similarly, the word for "ox" is read either **gud** or **gu₄**. These readings represent the same word and render the same sign - they simply differ in modern transliteration conventions. Variation in such conventions has grown recently by the introduction of a new set of readings by P. Attinger (Bern), which has received wide following, in particular in Germany. Such variation in sign readings is based on the one hand on differing interpretations of the data from ancient sign lists (which provide transcriptions of Sumerian words in Akkadian) and on the other hand on the definition of what an ideal transliteration should do (whether it should represent the abstract lexeme, or rather its concrete pronunciation, or something in between). For the [CDLI][] search engine, which is based on a FileMaker database, such variation presents a problem when searching for (Sumerian) words. The solution has been to strictly impose a set of "preferred sign readings" (available on the web site), a policy that has been carried out with great consistency. Second, Sumerian has no good standard for word segmentation. In the [CDLI][] data set one may find the word **ninda-i₃-de₂-a** (a pastry) transliterated as **ninda-i₃-de₂-a**, **ninda i₃-de₂-a**, **ninda i₃ de₂-a**, **nig₂-i₃-de₂-a**, **nig₂ i₃ de₂-a**, etcetera (**nig₂** and **ninda** are written by the same sign and there is no full agreement which of these readings is to be used in this particular word ). None of these various renderings is necessarily "wrong", because we know fairly little about  the formation and segmentation of Sumerian nouns. For computational approaches this variation poses an important challenge.

For Akkadian the variation in reading conventions plays a much smaller role; for most dialects of Akkadian (with the exception of Old Akkadian) scholars generally agree on transliteration values and word segmentation is hardly ever a problem for Akkadian. For search engines, however, Akkadian is much more difficult to deal with because the same word may be spelled in many different ways. Without lemmatization, there is no way a machine can tell that *ša-ar-ru-um*, *šar-ru* and LUGAL, etc. represent the same lemma for "king" in syllabic and logographic writing. 

Since [CDLI][] does not offer lemmatization, searching for words on this site is much more popular (and more useful) for Sumerian than it is for Akkadian. Sumerian words usually include the root of the word (written logographically) with prefixes and/or suffices attached. 

### 2.3.2 Downloading

There are various ways in which one can acquire [CDLI][] data. The website includes a Downloads page where one can get access to a daily clone of the catalog and the entire set of transliterations. Alternatively, one can perform a search on the [CDLI][] search page and request a download of the data (transliteration or catalog and transliteration data) by pushing a button. This works well for a few or several dozens of texts, but not for very large data sets.

The daily data dump is available from https://cdli.ucla.edu/bulk_data/ or from [Github][] at https://github.com/cdli-gh/data/raw/master/. Currently, the set of transliterations is offered in one big file, named `cdliàtf_unblocked.atf `. The catalog is split into two files because of file-size limitations at [Github][]; they are named `cdli_catalogue_1of2.csv` and `cdli_catalogue_2of2.csv`, respectively. The files need to be concatenated before they can be used.

The script for downloading these files (2_3_Data_Acquisition_CDLI.ipynb) is not essentially different from the script for downloading [ORACC][] files, as discussed in section 2.1. 

### 2.3.3 Manipulating

After downloading the catalog file can be ingested in a `pandas` DataFrame for further manipulation. At the moment of writing, a slight complication prevented `pandas`from doing so directly. It appears that one record was accidentally added on the same line as the preceding entry - resulting in a line with 115 fields (rather than the standard 63). The `pandas`package does not deal well with `csv`files that are irregularly shaped and issued an error message when trying to load the dataset with the `from_csv()` function. This issue can be circumvented by reading the dataset with the `csv`library and discarding any field beyond field 63, resulting in the loss of the one erratic entry.

The catalog can be used to create a sub-set of the [CDLI][] transliteration file by finding the P numbers (text IDs) that belong to, for instance, texts from Ebla, or texts dating to the ED IIIa period (as in the example below).

The field "id_text" holds the text ID number as a string, without the preceding "P" and without padding zeroes to the left. The text ID "P001023" is thus represented as "1023".  The function `zfill()` adds the padding zeroes to create a six-digit number as a string.

```python
ed3a = cat.loc[cat["period"].str[:7] == "ED IIIa"]
pnos = list(ed3a["id_text"].str.zfill(6))
with open("cdlidata/cdliatf_unblocked.atf", encoding="utf8") as c: 
    lines = c.readlines()
keep = False
ed3a_atf = []
for line in lines:
    if line[0] == "&": 
        if line[2:8] in pnos: 
            keep = True
        else: 
            keep = False
    if keep: 
        ed3a_atf.append(line)
```

This will represent the ED IIIa corpus as a list, where each line in the original ATF document is represented by one row. The following code will transform this list into a format where each text is a row in a `pandas` DataFrame, with the text ID in column 1, and the transliteration in column 2 (as a single string, without line numbers or line demarcations).

```python
docs = []
doc = []
d = ""
id_text = None
for line in ed3a_atf:
    if line[0] == "&":
        if id_text:
            doc = [id_text, d.strip()]
            docs.append(doc)
            d = ""
        id_text = line[1:8]
    elif line [0] in ["#", "$", "<", ">", "@"]:
        continue
    else:
        start = line.find(". ")
        line = line[start + 1:].rstrip()
        d = d + line
doc = [id_text, d.strip()]  # take care of the last text in the list
docs.append(doc)
ed3a_df = pd.DataFrame(docs)
ed3a_df.columns = ["id_text", "transliteration"]
```



## 2.4 Data Acquisition BDTNS

The Database of Neo-Sumerian Text ([BDTNS][]) was created by Manuel Molina (Consejo Superior de Investigaciones Científicas). The site provides a detailed catalog of the administrative, legal, and epistolary  tablets from the so-called Ur III period (21st century BCE). Molina estimates that museum and provate collections all over the world may hold at least 120,000 such documents, not including the holdings of the Iraq Museum (Baghdad). Currently, almost 65% of those documents are available through [BDTNS][] in transliteration, and/or in photograph and line drawing. 

There is a considerable overlap in the data sets offered by [CDLI][] and [BDTNS][]. In the majority of cases the photographs and line drawings on [BDTNS][] are imported from [CDLI][]. The initial core of the [BDTNS][] transliterations was provided by Remco de Maaijer and Bram Jagersma, who prepared several thousands of Ur III texts and distributed those data freely. These same transliterations are also found on [CDLI][]. Close cooperation between the two projects has led to further exchange of data.

Still, [BDTNS][] is not simply a duplicate of the Ur III data in [CDLI][]. Most Ur III scholars today prefer [BDTNS][] over [CDLI][] because the smaller focus of the Spanish implies that there is more attention to detail and to updating the record. One example is the book *Der König und sein Kreis* (2012) in which Paola Paoletti studied in detail several hundred documents from the so-called treasure archive. This archive reports on the manufacturing of luxury objects made of precious metals and leather and includes many rare words. Since the archive (like almost all Ur III archives) is scattered in museums all over the world, most of these texts were published as singular documents or in small groups. Studying the the entire group frequently allowed Paoletti to arrive at a more satisfying reading and understanding than the original editor's. The [BDTNS][] editions of these texts reflect Paoletti's improvements, but the [CDLI][] editions do not.

The [BDTNS][] data can be downloaded by hand through the Search option in the Catalogue & Transliterations drop-down menu.  One can search by a variety of criteria (including word and grapheme strings) and then download the search results by clicking on the Export button. The export page will provide options for the type of information to include (various types of meta-data and/or transliterations). By searching for a blank string one may export the entire data set. The export yields two files: one for the meta-data and one for the  transliterations, both in `.txt`format.

The [BDTNS][] transliteration files need some cleaning before they can be used for computational text analysis. First, they contain "vertical TABs", represented by ^K, \v, or \x0b. These "vertical TABS" are used between lines that belong to the same text. In order to replace those we need the `re`package (for Regular Expressions).

```python
import re
with open("query_text_19_03_1-210747.txt", encoding="utf8") as b: 
    bdtns = b.readlines()
for n, line in enumerate(bdtns): 
    bdtns[n] = re.sub("\v", "n", line)
```

