## 2.3 Data Acquisition: CDLI

### 2.3.1 CDLI Data

The Cuneiform Digital Library Initiative, or [CDLI][] is a central repository for meta-data, images, and transliterations of cuneiform tablets (translations are offered only for a small minority of texts). Today more than 300,000 are listed in the [CDLI][] catalog, with tens of thousands of photographs and line drawings. Each object in [CDLI][] receives a unique ID number, and these numbers are widely used today in print and in other online projects. Initially, CDLI focused primarily on administrative texts from the third millennium, and this is still the area of its greatest strength. Part of the transliterations were produced by the [CDLI][] team at UCLA, others were contributed by partners or were imported from other projects such as [ETCSL][] (for Sumerian literary texts) and [DCCLT][] (for lexical texts).  The photographs on [CDLI][] were largely produced in cooperative projects with museums all over the world, where [CDLI][] staff or partners would go to scan an entire collection or major parts of a collection. These images are copyright of the museum where the object is held and there is no wholesale downloading of the entire image set.

The PI of [CDLI][] , Bob Englund, has made a great effort in harmonizing the transliteration standard, in particular for Sumerian. In Sumerology today, the word for "to give" is read **šum₂** by some, but **sum** by others. Similarly, the word for "ox" is read either **gud** or **gu₄**. These readings represent the same word and render the same sign - they simply differ in modern transliteration conventions. Variation in such conventions has grown recently by the introduction of a new set of readings by P. Attinger (Bern), which received wide following, in particular in Germany. For the [CDLI][] search engine, which is based on a FileMaker database, such variation presents a problem when searching for (Sumerian) words. The solution has been to strictly impose a set of "preferred sign readings" (available on the web site). 

For Akkadian the variation in reading conventions plays a much smaller role. For search engines, however, Akkadian is much more difficult to deal with because the same word may be spelled in many different ways. Without lemmatization, there is no way a machine can tell that *ša-ar-ru-um*, *šar-ru* and LUGAL represent the same lemma in syllabic and logographic writing. 

*Talk in general about standardization/restricted vocab in CDLI*

Since [CDLI][] does not offer lemmatization, searching for words on this site is much more popular (and more useful) for Sumerian than it is for Akkadian. Sumerian words usually include the root of the word (written logographically) with prefixes and/or suffices attached. A difficult aspect of the (Sumerian) data set is the absence of a standard for word segmentation. One may find the word **ninda-i₃-de₂-a** (a pastry) transliterated as **ninda-i₃-de₂-a**, **ninda i₃-de₂-a**, **ninda i₃ de₂-a**, **nig₂-i₃-de₂-a**, **nig₂ i₃ de₂-a**, etcetera (**nig₂** and **ninda** are written by the same sign and there is no full agreement which of these is to be used in this particular word ). None of these various renderings is necessarily "wrong", because we know fairly little about  the formation and segmentation of Sumerian nouns. For computational approaches this variation poses an important challenge.

### 2.3.2 Downloading

There are various ways in which one can acquire [CDLI][] data. The website includes a Downloads page where one can get access to a daily clone of the catalog and the entire set of transliterations (currently ## texts). Alternatively, one can perform a search on the [CDLI][] search page and request a download of the data (transliteration or catalog and transliteration data) by pushing a button. This works well for a few or several dozens of texts, but not for very large data sets.

The daily data dump may also be acquired by a script, either from https://cdli.ucla.edu/bulk_data/ or from [Github][] at https://github.com/cdli-gh/data/raw/master/. Currently, the set of transliterations is offered in one big file, named `cdliàtf_unblocked.atf `. The catalog is split into two files because of file-zise limitations at [Github][]; they are named `cdli_catalogue_1of2.csv` and `cdli_catalogue_2of2.csv`, respectively. The files need to be concatenated before they can be used.

The script for downloading these files is not essentially different from the script for downloading [ORACC][] files, as discussed in section 2.1. 

### 2.3.3 Manipulating

After downloading the catalog file can be ingested in a `pandas` DataFrame for further manipulation. At the moment of writing, a slight complication prevented `pandas`from doing so directly. It appears that one record was accidentally added in the same line as the preceding entry - resulting in a line with 115 fields (rather than the standard 63). The `pandas`package does not deal well with `csv`files that are irregularly shaped and issued an error message when trying to load the dataset with the `from_csv()` function. This issue can be circumvented by reading the dataset with the `csv`library and discarding any field beyond field 63.

The catalog can be used to create a sub-set of the [CDLI][] transliteration file by finding the P numbers (text IDs) that belong to, for instance, texts from Ebla, or texts dating to the ED IIIa period.

**Following snippet needs to be tested!**

```python
ed3a = cat.loc[cat["period"] == "ED IIIa (ca. 2600-2500 BC)"]
pnos = list(ed3a["id_text"])
with open("cdlidata/cdliatf_unblocked.atf", encoding="utf8") as c: 
    lines = c.readlines()
keep = False
ed3a_atf = []
for line in lines:
    if line[0] == "&": 
        if line[1:8] in pnos: 
            keep = True
        else: 
            keep = False
    if keep: 
        ed3a_aft.append(line)
```

Since many of the [CDLI][] meta-data fields (such as provenance, )