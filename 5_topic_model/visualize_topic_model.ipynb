{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Topic Model\n",
    "\n",
    "This is a work-in-progress Notebook providing different types of visualization for a topic model. The first visualization (`pyLDAvis`) allows a researcher to interact with the *vocabulary* of the corpus and its distribution over topics. pyLDAvis is an out-of-the-box package, a wrapper around the R-library `LDAvis`.\n",
    "\n",
    "The second visualization employs the `bokeh` library and allows a researcher to interact with (and inspect) *documents* and the distribution of topics over documents. \n",
    "\n",
    "The data used in this notebook are scraped from [ORACC](http://oracc.org). Most of the techniques used here, however, may be applied to any set of documents.\n",
    "\n",
    "# Dependencies and Versions\n",
    "pyLDAvis 2.0 is incompatible with Pandas 0.19 (check for pyLDAvis 2.1.0 or later). The MDS and tSNE computations necessary for plotting the distribution of documents in the topic model need scikit-learn 0.18. This notebook was written for Python 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "from gensim import corpora, models, utils\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from bokeh.models import ColumnDataSource, OpenURL, TapTool, HoverTool, CustomJS, Title\n",
    "from bokeh.models.widgets import Slider\n",
    "from bokeh.plotting import figure, output_file, output_notebook, save, show, reset_output\n",
    "from bokeh.layouts import widgetbox, column\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the texts\n",
    "[For test purposes one may select only the first 100 documents. Remove the hashmark (#) from the first line of the following cell if you wish to do that]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pickled = 'output/data_for_topic_model.p'\n",
    "df = pd.read_pickle(pickled)\n",
    "texts = df['lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-filter\n",
    "The variable `posfilter` holds the last two characters of lemmatized words with allowed Part of Speech tags. If, for instance, you wish to select Verbs, Adjectives, and Nouns (in Akkadian), posfilter will be `[']n', 'aj', ']v']`. Note that one-character pos-tags need the right bracket!\n",
    "The POS labels are:\n",
    "* \"n\", #Nouns\n",
    "* \"v\", #Verbs\n",
    "* \"aj\", #Adjectives\n",
    "* \"av\", #Adverbs\n",
    "* \"an\", #Agricultural Name\n",
    "* \"cn\", #Celestial Name\n",
    "* \"dn\", #Divine Name\n",
    "* \"en\", #Ethnicity Name\n",
    "* \"fn\", #Field Name\n",
    "* \"gn\", #Geographical Name (lands, etc.)\n",
    "* \"ln\", #Lineage Name (ancestral clan)\n",
    "* \"mn\", #Month Name\n",
    "* \"on\", #Object Name\n",
    "* \"pn\", #Personal Name\n",
    "* \"qn\", #Quarter (of a city) Name\n",
    "* \"rn\", #Royal Name\n",
    "* \"sn\", #Settlement Name\n",
    "* \"tn\", #Temple Name\n",
    "* \"wn\", #Watercourse Name\n",
    "* \"yn\", #Year Name\n",
    "* \"nu\", #Numeral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "posfilter = [']n', ']v', 'aj']\n",
    "#include nouns, verbs, and adjectives, not numerals, prepositions or proper nouns\n",
    "texts = [[word for word in text if word[-2:] in posfilter] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "\n",
    "Stop words are very frequent words that are not able to distinguish between topics. This includes, for instance, prepositions - but those can also be filtered out by the POS filter. The following nouns and verbs are too frequent to contribute to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stoplist = [\n",
    "'šarru[king]n',\n",
    "'bēlu[lord]n',\n",
    "'libbu[interior]n',\n",
    "'muhhu[skull]n',\n",
    "'ardu[slave]n',\n",
    "'šulmu[completeness]n',\n",
    "'šapāru[send]v',\n",
    "'alāku[go]v',\n",
    "'qabû[say]v',\n",
    "'pānu[front]n',\n",
    "'māru[son]n',\n",
    "'bītu[house]n',\n",
    "'epēšu[do]v',\n",
    "'wabālu[bring]v',\n",
    "'šakānu[put]v',\n",
    "'amāru[see]v',\n",
    "'bašû[exist]v',\n",
    "'našû[lift]v',\n",
    "'izuzzu[stand]v',\n",
    "'ūmu[day]n',\n",
    "'ṭābu[good]aj',\n",
    "'mādu[many]aj',\n",
    "'nadānu[give]v',\n",
    "'tadānu[give]v',\n",
    "'ṣehru[small]aj',\n",
    "'mimmû[all]n',\n",
    "'gimru[totality]n',\n",
    "'gabbu[totality]n',\n",
    "'šâlu[ask]v',\n",
    "'šemû[hear]v',\n",
    "'ūmu[day]n',\n",
    "'awātu[word]n',\n",
    "'erēbu[enter]v'\n",
    "]\n",
    "texts = [[word for word in text if word not in stoplist] for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out texts that have too few words left\n",
    "Identify texts that have less than 10 lemmas left and use that selection for the list `texts` and for the dataframe `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bo = [len(text)>9 for text in texts]\n",
    "df = df[bo]\n",
    "texts = [texts[i] for i in range(0, len(texts)) if bo[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many documents did we start with, and how many do we have left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(bo), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "create the gensim Dictionary and filter for words that are too common or too rare (no_above may be set too low here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "## CHECK - is this done correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Model\n",
    "\n",
    "Set the seed, indicate the number of topics (default set to 10) and run the model.\n",
    "\n",
    "## TODO use slider "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics = int(input(\"Number of topics: \") or 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 15\n",
    "np.random.seed(seed)\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus, num_topics=ntopics, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the top words and their probabilities in all topics. Note: the topic numbers here are not the ones used below in the visualizations! (The topics are the same, but not their numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ldamodel.show_topics(ntopics, formatted = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Move all visualization to separate notebook. Compute the model, show topics and show/explain document/topic and topic/term tables.\n",
    "\n",
    "# pyLDAvis\n",
    "Use pyLDAvis to visualize the topic model. By default, pyLDAvis will order the topics by [prevalence](https://github.com/bmabey/pyLDAvis/issues/59) (topic 1 is the most prevalent topic). That means that the topic numbers in the visualization do not agree with the topic numbers in the lda model. To prevent this behaviour one may use `sort_topics=False` in the `prepare` command. The advantage of ordering the topics by prevalence, however, is that new instances of the lda model are more comparable (that is, the same topic will receive the same number). Note that the library was written in Java for R, and so the numbering in the visualization begins with 1 (not with 0). The topic numbers in the Document/Topic and Topic/Term matrices below will be adjusted to be compatible with the pyLDAvis visualization.\n",
    "\n",
    "PyLDAvis needs a large output box. The `%%html` lines below create such a box (for the code see [here](http://stackoverflow.com/questions/18770504/resize-ipython-notebook-output-window)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:1000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "if not os.path.exists('vis'):\n",
    "    os.makedirs('vis')\n",
    "pyLDAvis.save_html(vis, 'vis/lda_terms.html')\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document/Topic Probability\n",
    "The function `get_document_topics()` will list the probability of the topics in a single document. In order to get all the topics set the argument `minimum_probability` to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ldamodel.get_document_topics(corpus[1], minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document/Topic Probability Table\n",
    "A Document/Topiuc probability table is a table (DataFrame), where each row represents a document and each column a topuic. Each cell has the probability of a particular topic in a particular document. The sum of each row is 1 (probability distribution).\n",
    "\n",
    "In order to create a full Document/Topic probability table we iterate over the entire corpus with the `get_document_topics()` function. This creates a list of lists (`list_of_doctopics`) where each list represents the probability of each topic in a document. The probability is represented in a tuple (topic_number, probability). The `list_of_probabilities` preserves only the probabilities. This list of lists is transformed into a DataFrame, whith as index the index of the original DataFrame with the tokenized data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "list_of_doctopics = [ldamodel.get_document_topics(corpus[i], minimum_probability=0) for i in range(len(corpus))]\n",
    "list_of_probabilities = [[probability for label,probability in distribution] for distribution in list_of_doctopics]\n",
    "d_t_df = pd.DataFrame(list_of_probabilities)\n",
    "d_t_df = d_t_df.set_index(df.index)\n",
    "d_t_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renumber Topics\n",
    "Rename the topics (columns) to start with 1, in accordance with the pyLDAvis visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "topics = [i+1 for i in range(ntopics)]\n",
    "d_t_df.columns = topics\n",
    "#d_t_df['text_name'] = df['text_name']\n",
    "d_t_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Topic / Term table\n",
    "This is a table with N rows (the number of topics) and M columns (the number of individual terms in the Dictionary). The table indicates the probability of each term in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "topic_term = ldamodel.show_topics(ntopics, formatted=False, num_words=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `topic_term` is a list of list. Each topic is represented by a list of tuples in the form `(word, probability)`. The following code pulls out the probabilities for each word in each topic (`topic_term[i][1]`) and creates a list of DataFrames with the words as index (rows) and the probabilities as the only column. The DataFrames are concatenated to a single DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "topic_term_list = [pd.DataFrame(topic_term[i][1]).set_index(0) for i in range(0, ntopics)]\n",
    "t_t_df_ = pd.concat(topic_term_list, axis=1, ignore_index=True)\n",
    "t_t_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns to start with 1, and Transpose to Topic/Term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t_t_df_.columns = topics\n",
    "t_t_df = t_t_df_.T\n",
    "t_t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#just checking\n",
    "t_t_df['ēkallu[palace]n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Documents 1: Using MDS\n",
    "While pyLDAvis is an excellent tool for exploring the topic/term aspect of a topic model (the words and their probabilities in each topic) it does not provide access to the document/topic aspect (the probability distribution of topics in each document). The visualization below plots all the documents according to their (cosine) distances (using Multi-Dimensional Scaling) in the Document/Term DataFrame. Each document (data point in the visualization) is colored according to the most prevalent topic and the size of the dot represents the probability of the most prevalent topic in that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the distances between each of the documents. Use either the Document/Topic Dataframe or the Document/Term Dataframe (constructed below) to measure distance.\n",
    "\n",
    "Since the data is already in list format, CountVectorizer does not need to preprocess or tokenize. The only way to prevent CountVectorizer from doing so is by creating dummy functions for the preprocessor and the tokenizer. These functions simply return the argument they receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "dtm = cv.fit_transform(texts)\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns = cv.get_feature_names(), index = df.index.values)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dist = squareform(pdist(dtm_df, 'cosine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the position of each document using Multi-Dimensional Scaling. The variable `pos` holds the `x` and `y`  coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, max_iter=3000,\n",
    "       random_state=seed, dissimilarity=\"precomputed\", n_jobs=1)\n",
    "pos = mds.fit_transform(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists of x and y values (coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mds_x = [x for x, y in pos]\n",
    "mds_y = [y for x, y in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists of the most prevalent topic, the probability of the most prevalent topic, and the text name for each document. These lists are used in the tooltips of the Bokeh visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prevalent_topic = d_t_df.idxmax(axis=1)\n",
    "probability = d_t_df.max(axis=1)\n",
    "text_name = list(df['text_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(prevalent_topic), len(probability), len(text_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Colors\n",
    "\n",
    "Create a colormap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "colormap = {0: 'grey', 1: \"orange\", 2: \"olive\", 3: \"firebrick\", \n",
    "          4: \"gold\", 5: \"red\", 6: \"fuchsia\", 7: \"green\", \n",
    "          8: \"blue\", 9: \"purple\", 10: \"aqua\", 11: \"yellow\", \n",
    "          12: \"indigo\", 13: \"blueviolet\", 14: \"beige\", 15:\"navy\", 16: 'chocolate',\n",
    "          17: 'azure', 18: 'coral', 19: 'crimson', 20: 'darkblue', 21: 'darkkhaki', \n",
    "          22: 'darkseagreen', 23: 'darkturquoise', 24: 'deeppink', 25: 'black'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary, to be used by Bokeh for drawing the visualization. In the dictionary each key is a feature and each value is a list with the values of that feature for each data point. All lists (all values) should be of equal length (the number of data points). The features include x and y coordinates, color, size, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "d_mds = dict(\n",
    "        x=mds_x,\n",
    "        y=mds_y,\n",
    "        id_text=list(df.id_text),\n",
    "        size = probability/max(probability)*15,\n",
    "        probability = probability,\n",
    "        topic = prevalent_topic,\n",
    "        color = [colormap[n] for n in prevalent_topic],\n",
    "        alpha = [0.5] * len(mds_x),\n",
    "        text_name = text_name\n",
    "    )\n",
    "instructions = [\n",
    "    \"Highlight (color) one or two topics by moving the sliders. If both sliders are 0, all topics are colored.\",\n",
    "    \"Hover over a data point for more information. Click on a data point to go to the document edition.\",\n",
    "    \"Use the toolbar to zoom, pan, reset, or save as .png.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JavaScript Code\n",
    "Interactive features in Bokeh, such as sliders, use a callback function that is activated when a certain event takes place. This event can be a mouse movement, a click, or a change in the slider. Custom callback functions need to be written in JavaScript.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "        var data = source.data;\n",
    "        topic = data['topic']\n",
    "        for (i = 0; i < topic.length; i++) {\n",
    "            data.alpha[i] = 0.5\n",
    "            data.color[i] = cm[topic[i]]\n",
    "            if ((topic1.value == 0) && (topic2.value == 0)) {\n",
    "                continue;\n",
    "            } else if ((topic[i] == topic1.value) || (topic[i] == topic2.value)) {\n",
    "                continue;\n",
    "            } else {\n",
    "                data.color[i] = 'grey'\n",
    "                data.alpha[i] = '0.1'\n",
    "            } \n",
    "        }\n",
    "        source.change.emit();\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the visualization. The visualization provides various tools for further exploration:\n",
    "- tooltips (provides topic, probability, text name and URL)\n",
    "- box zoom\n",
    "- wheel zoom\n",
    "- pan\n",
    "- reset\n",
    "- link to document edition\n",
    "- save the visualization\n",
    "\n",
    "In addition, the visualization has two sliders that allow the user to select two topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def drawviz(data, title, outputfile):\n",
    "    source_mds = ColumnDataSource(data=data)\n",
    "    p = figure(\n",
    "        plot_width=1000, plot_height=1000,\n",
    "        tools=\"tap,pan,wheel_zoom,box_zoom,reset,save\", \n",
    "        title=title)\n",
    "    p.add_tools(HoverTool(\n",
    "        tooltips=[\n",
    "            (\"url\", \"http://oracc.org/\" + \"@id_text\"),\n",
    "            ((\"topic, probability\"), (\"@topic, @probability\")),\n",
    "            (\"text name\", \"@text_name\")\n",
    "        ]\n",
    "        ))\n",
    "\n",
    "    p.circle('x', \n",
    "         'y', \n",
    "         color='color', \n",
    "         fill_alpha='alpha', \n",
    "         size='size', \n",
    "         source=source_mds\n",
    "         )\n",
    "    p.axis.visible = False\n",
    "\n",
    "    slider1 = Slider(start=0, end=ntopics, value=0, step=1, title=\"Topic A\")\n",
    "    slider2 = Slider(start=0, end=ntopics, value=0, step=1, title=\"Topic B\")\n",
    "\n",
    "    callback = CustomJS(args=dict(source=source_mds, topic1 = slider1, \n",
    "                              topic2 = slider2, cm = colormap), code = code)\n",
    "    slider1.js_on_change('value', callback)\n",
    "    slider2.js_on_change('value', callback)\n",
    "    \n",
    "    url = \"http://oracc.museum.upenn.edu/@id_text\"\n",
    "    taptool = p.select(type=TapTool)\n",
    "    taptool.callback = OpenURL(url=url)\n",
    "\n",
    "    for line in instructions:\n",
    "        p.add_layout(Title(text=line), 'below')\n",
    "\n",
    "    layout = column(slider1, slider2, p)\n",
    "    show(layout)\n",
    "    output_file(outputfile)\n",
    "    save(layout);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Projection with MDS. Size of the circle represents prevalence of the topic.\"\n",
    "outputfile = 'vis/mds1.html'\n",
    "drawviz(d_mds, title, 'vis/mds1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: plotting based on Document/Topic table\n",
    "The following visualization uses the same approach, but takes the document/topic table as the basis for distance measurements. Documents that share approximately the same distribution of topics will be plotted n the same region. Since the sum of each row in the document/topic table is 1 the distance matrix is computed with euclidean distance (not cosine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dist_dt = squareform(pdist(d_t_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, max_iter=3000,\n",
    "       random_state=seed, dissimilarity=\"precomputed\", n_jobs=1)\n",
    "pos = mds.fit_transform(dist_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mds2 = d_mds # the data source is the same as for the previous visualization, except for the x and y coordinates.\n",
    "d_mds2['x'] = [x for x, y in pos]\n",
    "d_mds2['y'] = [y for x, y in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Projection with MDS, based on Document/Topic distribution. Size of the circle represents prevalence of the topic.\"\n",
    "outputfile = 'vis/mds2.html'\n",
    "drawviz(d_mds2, title, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Documents 2: Using TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE based on Document/Term Matrix (Cosine distance)\n",
    "\n",
    "Cosine distances have been computed earlier; the matrix is stored in the variable `dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = dist\n",
    "tsne = TSNE(n_components = 2, random_state=0, metric=\"precomputed\")\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tsne = d_mds # the data source is the same as for the previous visualization, except for the x and y coordinates.\n",
    "d_tsne['x'] = [x for x, y in X_tsne]\n",
    "d_tsne['y'] = [y for x, y in X_tsne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Projection with tSNE. Size of the circle represents prevalence of the topic.\"\n",
    "outputfile = 'vis/tsne1.html'\n",
    "drawviz(d_tsne, title, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE based on Document/Topic Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = dist_dt\n",
    "tsne = TSNE(n_components = 2, random_state=0, metric=\"precomputed\")\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tsne2 = d_mds # the data source is the same as for the previous visualization, except for the x and y coordinates.\n",
    "d_tsne2['x'] = [x for x, y in X_tsne]\n",
    "d_tsne2['y'] = [y for x, y in X_tsne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Projection with tSNE, based on Document/Topic distribution. Size of the circle represents prevalence of the topic.\"\n",
    "outputfile = 'vis/tsne2.html'\n",
    "drawviz(d_tsne2, title, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
