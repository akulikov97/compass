# 5 Topic Modeling State Archives of Assyria

*Topic modeling* is an unsupervised technique for retrieving lists of words that represent the main themes (topics) available in a corpus of documents.  The State Archives of Assyria ([SAAo](http:oracc.org/saao)) contain a wide variety of texts, including letters on military or political matters, letters by scholars on ritual affairs, astronomical reports, administrative texts and contracts, and divinatory queries. Each group of texts may be characterized by a particular vocabulary, a set of words that is typically used to talk about astronomy, war, health,  or divination. Or, to be more precise, the *probability* of a word like sisu[horse]N is higher in military contexts than it is in the context of an inquiry about the king's health. Topic Modeling will distribute the probability of each word that appears in the corpus over *N* topics. The value of *N* is chosen by the user. The process is non-deterministic in that a repeated run of the same script leads to different results. The 'topics essentially consist of hte words with the highest scoring probability in that topic.

Topic modeling may thus be used to divide a large corpus into groups (documents that share a preference for words scoring high in a particular topic) or to get a rough idea of what a corpus is about - what themes (topics) are present in the corpus.

Topic modeling takes each document in a corpus as a so-called Bag of Words, that is, it abstracts from morphology, word order, and syntax, taking into account only the frequencies of  words in that document. Words that frequently appear together in the same documents are more likely to appear in the same topic.

The main topic modeling technique is called LDA, or Latent Dirichlet Allocation. LDA results in two tables with probability distributions. The Topic/Term table indicates for each term (or word) available in the corpus the probability that this word belongs to a particular topic. If there are N topics and the corpus has M unique terms, the Topic/Term table is a M by N matrix. The sum of each row (representing a topic) in the matrix is 1, that is, the probability of the word is distributed over all topics [**#CHECK is this correct?**]. Probabilities in this matrix are never 0 or 1 - each word has some probability (even if minimal) to appear in each topic. The second table is the Document/Topic table which indicates for each topic the probability that it appears in a particular document. If there are N topics and D documents, this is a N by D matrix.  