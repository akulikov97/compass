{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Topic Model\n",
    "The present notebook will use the data collected in section 5.1 to compute and explore a topic model. In essence, a topic model consists of two sets of probability distributions. First, the probability that a *word* belongs to a certain topic (Topic/Term probability). Second, the probability of a *topic* to appear in a document (Document/Topic probability). These probabilities are never 0, that is, each word has some probability (even if very small) to be part of each topic and each topic has some probability (even if very small) to appear in a document.\n",
    "\n",
    "A standard way to inspect a topic model is to look at the top-ten words of a topic (the ten words with the highest probability in each particular topic). Similarly, we can pull the ten most important documents for each topic (the documents in which this topic has the highest probability).\n",
    "\n",
    "For a more thorough analysis we may create full probability tables: a topic/term probability table and a document/topic probability table. These tables give a fuller account of the model and will be used for the visualizations (5.3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, utils\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the texts\n",
    "[For test purposes one may select only the first 100 documents. Remove the hashmark (#) from the first line of the following cell if you wish to do that]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pickled = 'output/data_for_topic_model.p'\n",
    "df = pd.read_pickle(pickled)\n",
    "texts = df['lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-filter\n",
    "The variable `posfilter` holds the last two characters of lemmatized words with allowed Part of Speech tags. If, for instance, you wish to select Verbs, Adjectives, and Nouns (in Akkadian), posfilter will be `[']n', 'aj', ']v']`. Note that one-character pos-tags need the right bracket!\n",
    "The POS labels are:\n",
    "* \"n\", #Nouns\n",
    "* \"v\", #Verbs\n",
    "* \"aj\", #Adjectives\n",
    "* \"av\", #Adverbs\n",
    "* \"an\", #Agricultural Name\n",
    "* \"cn\", #Celestial Name\n",
    "* \"dn\", #Divine Name\n",
    "* \"en\", #Ethnicity Name\n",
    "* \"fn\", #Field Name\n",
    "* \"gn\", #Geographical Name (lands, etc.)\n",
    "* \"ln\", #Lineage Name (ancestral clan)\n",
    "* \"mn\", #Month Name\n",
    "* \"on\", #Object Name\n",
    "* \"pn\", #Personal Name\n",
    "* \"qn\", #Quarter (of a city) Name\n",
    "* \"rn\", #Royal Name\n",
    "* \"sn\", #Settlement Name\n",
    "* \"tn\", #Temple Name\n",
    "* \"wn\", #Watercourse Name\n",
    "* \"yn\", #Year Name\n",
    "* \"nu\", #Numeral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "posfilter = [']n', ']v', 'aj']\n",
    "#include nouns, verbs, and adjectives, not numerals, prepositions or proper nouns\n",
    "texts = [[word for word in text if word[-2:] in posfilter] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "\n",
    "Stop words are very frequent words that are not able to distinguish between topics. This includes, for instance, prepositions - but those can also be filtered out by the POS filter. The following nouns and verbs are too frequent to contribute to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stoplist = [\n",
    "'šarru[king]n',\n",
    "'bēlu[lord]n',\n",
    "'libbu[interior]n',\n",
    "'muhhu[skull]n',\n",
    "'ardu[slave]n',\n",
    "'šulmu[completeness]n',\n",
    "'šapāru[send]v',\n",
    "'alāku[go]v',\n",
    "'qabû[say]v',\n",
    "'pānu[front]n',\n",
    "'māru[son]n',\n",
    "'bītu[house]n',\n",
    "'epēšu[do]v',\n",
    "'wabālu[bring]v',\n",
    "'šakānu[put]v',\n",
    "'amāru[see]v',\n",
    "'bašû[exist]v',\n",
    "'našû[lift]v',\n",
    "'izuzzu[stand]v',\n",
    "'ūmu[day]n',\n",
    "'ṭābu[good]aj',\n",
    "'mādu[many]aj',\n",
    "'nadānu[give]v',\n",
    "'tadānu[give]v',\n",
    "'ṣehru[small]aj',\n",
    "'mimmû[all]n',\n",
    "'gimru[totality]n',\n",
    "'gabbu[totality]n',\n",
    "'šâlu[ask]v',\n",
    "'šemû[hear]v',\n",
    "'ūmu[day]n',\n",
    "'awātu[word]n',\n",
    "'erēbu[enter]v'\n",
    "]\n",
    "texts = [[word for word in text if word not in stoplist] for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out texts that have too few words left\n",
    "Identify texts that have less than 10 lemmas left and use that selection for the list `texts` and for the dataframe `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bo = [len(text)>9 for text in texts]\n",
    "df = df[bo]\n",
    "texts = [texts[i] for i in range(0, len(texts)) if bo[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(bo), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many documents did we start with, and how many do we have left?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "create the gensim Dictionary and filter for words that are too common or too rare (no_above may be set too low here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "## CHECK - is this done correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Model\n",
    "\n",
    "Set the seed, indicate the number of topics (default set to 10) and run the model.\n",
    "\n",
    "The visualization (section 5.3) will fail if the number of topics is higher than 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics = int(input(\"Number of topics: \") or 10)\n",
    "if ntopics > 25:\n",
    "    ntopics = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 15\n",
    "np.random.seed(seed)\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus, num_topics=ntopics, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the top 10 words and their probabilities in all topics. Note: the topic numbers here are not the ones used in the visualizations in 5.3! (The topics are the same, but not their numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ldamodel.show_topics(ntopics, formatted = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document/Topic Probability\n",
    "The function `get_document_topics()` will list the probability of the topics in a single document. In order to get all the topics set the argument `minimum_probability` to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ldamodel.get_document_topics(corpus[1], minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document/Topic Probability Table\n",
    "A Document/Topiuc probability table is a table (DataFrame), where each row represents a document and each column a topuic. Each cell has the probability of a particular topic in a particular document. The sum of each row is 1 (probability distribution).\n",
    "\n",
    "In order to create a full Document/Topic probability table we iterate over the entire corpus with the `get_document_topics()` function. This creates a list of lists (`list_of_doctopics`) where each list represents the probability of each topic in a document. The probability is represented in a tuple (topic_number, probability). The `list_of_probabilities` preserves only the probabilities. This list of lists is transformed into a DataFrame, whith as index the index of the original DataFrame with the tokenized data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "list_of_doctopics = [ldamodel.get_document_topics(corpus[i], minimum_probability=0) for i in range(len(corpus))]\n",
    "list_of_probabilities = [[probability for label,probability in distribution] for distribution in list_of_doctopics]\n",
    "d_t_df = pd.DataFrame(list_of_probabilities)\n",
    "d_t_df = d_t_df.set_index(df.index)\n",
    "d_t_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above table to find the ten highest scoring documents per topic with the pandas function 'nlargest'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.merge(df['designation'], d_t_df, left_index=True, right_index=True)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctop = []\n",
    "for i in range(ntopics):\n",
    "    t = new.nlargest(10, i)[['designation', i]]\n",
    "    t['topic'] = i\n",
    "    t = t.rename(columns = {i :'probability'})\n",
    "    doctop.append(t)\n",
    "doctop_df = pd.concat(doctop, axis=0)\n",
    "doctop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renumber Topics\n",
    "Rename the topics (columns) to start with 1, in accordance with the pyLDAvis visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "topics = [i+1 for i in range(ntopics)]\n",
    "d_t_df.columns = topics\n",
    "d_t_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Topic / Term table\n",
    "This is a table with N rows (the number of topics) and M columns (the number of individual terms in the Dictionary). The table indicates the probability of each term in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term = ldamodel.show_topics(ntopics, formatted=False, num_words=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `topic_term` is a list of list. Each topic is represented by a list of tuples in the form `(word, probability)`. The following code pulls out the probabilities for each word in each topic (`topic_term[i][1]`) and creates a list of DataFrames with the words as index (rows) and the probabilities as the only column. The DataFrames are concatenated to a single DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "topic_term_list = [pd.DataFrame(topic_term[i][1]).set_index(0) for i in range(0, ntopics)]\n",
    "t_t_df_ = pd.concat(topic_term_list, axis=1, ignore_index=True)\n",
    "t_t_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns to start with 1, and Transpose to Topic/Term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t_t_df_.columns = topics\n",
    "t_t_df = t_t_df_.T\n",
    "t_t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just checking\n",
    "t_t_df['ēkallu[palace]n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Export data for viz. in 5.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
