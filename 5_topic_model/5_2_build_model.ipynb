{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Topic Model\n",
    "The present notebook will use the data collected in section 5.1 to compute and explore a topic model. In essence, a topic model consists of two sets of probability distributions. First, the probability that a *word* belongs to a certain topic (Topic/Term probability). Second, the probability of a *topic* to appear in a document (Document/Topic probability). These probabilities are never 0, that is, each word has some probability (even if very small) to be part of each topic and each topic has some probability (even if very small) to appear in a document.\n",
    "\n",
    "A standard way to inspect a topic model is to look at the top-ten words of a topic (the ten words with the highest probability in each particular topic). Similarly, we can pull the ten most important documents for each topic (the documents in which this topic has the highest probability).\n",
    "\n",
    "For a more thorough analysis we may create full probability tables: a topic/term probability table and a document/topic probability table. These tables give a fuller account of the model and will be used for the visualizations (5.3). \n",
    "\n",
    "# TODO\n",
    "Look at https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ for ideas about hyperparameters, perplexity, convergence, etc. and https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/ for visualizations.\n",
    "\n",
    "Also look into saving a model to disk in Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, utils\n",
    "import gensim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the texts\n",
    "[For test purposes one may select only the first 100 documents. Remove the hashmark (#) from the first line of the following cell if you wish to do that]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pickled = 'output/data_for_topic_model.p'\n",
    "df = pd.read_pickle(pickled)\n",
    "texts = df['lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-filter\n",
    "The variable `posfilter` holds the last two characters of lemmatized words with allowed Part of Speech tags. If, for instance, you wish to select Verbs, Adjectives, and Nouns (in Akkadian), posfilter will be `[']n', 'aj', ']v']`. Note that one-character pos-tags need the right bracket!\n",
    "The POS labels are:\n",
    "* \"n\", #Nouns\n",
    "* \"v\", #Verbs\n",
    "* \"aj\", #Adjectives\n",
    "* \"av\", #Adverbs\n",
    "* \"an\", #Agricultural Name\n",
    "* \"cn\", #Celestial Name\n",
    "* \"dn\", #Divine Name\n",
    "* \"en\", #Ethnicity Name\n",
    "* \"fn\", #Field Name\n",
    "* \"gn\", #Geographical Name (lands, etc.)\n",
    "* \"ln\", #Lineage Name (ancestral clan)\n",
    "* \"mn\", #Month Name\n",
    "* \"on\", #Object Name\n",
    "* \"pn\", #Personal Name\n",
    "* \"qn\", #Quarter (of a city) Name\n",
    "* \"rn\", #Royal Name\n",
    "* \"sn\", #Settlement Name\n",
    "* \"tn\", #Temple Name\n",
    "* \"wn\", #Watercourse Name\n",
    "* \"yn\", #Year Name\n",
    "* \"nu\", #Numeral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "posfilter = [']n', ']v', 'aj']\n",
    "#include nouns, verbs, and adjectives, not numerals, prepositions or proper nouns\n",
    "texts = [[word for word in text if word[-2:] in posfilter] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "\n",
    "Stop words are very frequent words that are not able to distinguish between topics. This includes, for instance, prepositions - but those can also be filtered out by the POS filter. The following nouns and verbs are too frequent to contribute to the analysis. Note that this list of stop words was assembled for the SAAo corpus - another corpus may require a different list, or none at all. In a cell further below the dictionary is built - leaving out words that appear in more than 80 percent of the documents (or whatever the 'no_above' parameter is set too) making the use of a stop word list mostyly unnecessary. The only advantage of an explicit list of stop words is that it makes it possible to filter out documents or text fragments that remain with too few words to be meaningful.\n",
    "\n",
    "The 'stoplist' cell can be omitted entirely or adapted to your purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stoplist = [\n",
    "'šarru[king]n',\n",
    "'bēlu[lord]n',\n",
    "'libbu[interior]n',\n",
    "'muhhu[skull]n',\n",
    "'ardu[slave]n',\n",
    "'šulmu[completeness]n',\n",
    "'šapāru[send]v',\n",
    "'alāku[go]v',\n",
    "'qabû[say]v',\n",
    "'pānu[front]n',\n",
    "'māru[son]n',\n",
    "'bītu[house]n',\n",
    "'epēšu[do]v',\n",
    "'wabālu[bring]v',\n",
    "'šakānu[put]v',\n",
    "'amāru[see]v',\n",
    "'bašû[exist]v',\n",
    "'našû[lift]v',\n",
    "'izuzzu[stand]v',\n",
    "'ūmu[day]n',\n",
    "'ṭābu[good]aj',\n",
    "'mādu[many]aj',\n",
    "'nadānu[give]v',\n",
    "'tadānu[give]v',\n",
    "'ṣehru[small]aj',\n",
    "'mimmû[all]n',\n",
    "'gimru[totality]n',\n",
    "'gabbu[totality]n',\n",
    "'šâlu[ask]v',\n",
    "'šemû[hear]v',\n",
    "'ūmu[day]n',\n",
    "'awātu[word]n',\n",
    "'erēbu[enter]v'\n",
    "]\n",
    "texts = [[word for word in text if word not in stoplist] for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out texts that have too few words left\n",
    "Identify texts that have at least 10 lemmas left and use that as a mask to filter  the list `texts` as well as the dataframe `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bo = [len(text)>9 for text in texts]\n",
    "df = df[bo]\n",
    "texts = [texts[i] for i in range(0, len(texts)) if bo[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many documents did we start with, and how many do we have left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4976, 3006)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bo), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "create the gensim Dictionary and filter for words that are too common or too rare (no_above may be set too low here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "dictionary.save('output/ldadict')\n",
    "## CHECK - is this done correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "corpora.MmCorpus.serialize('output/ldacorpus', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Model\n",
    "\n",
    "Set the seed, indicate the number of topics (default set to 10) and run the model.\n",
    "\n",
    "The visualization (section 5.3) will fail if the number of topics is higher than 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Number of topics:  10\n"
     ]
    }
   ],
   "source": [
    "ntopics = int(input(\"Number of topics: \") or 10)\n",
    "if ntopics > 25:\n",
    "    ntopics = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 15\n",
    "np.random.seed(seed)\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus, num_topics=ntopics, id2word = dictionary, passes=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the top 10 words and their probabilities in all topics. Note: the topic numbers here are not the ones used in the visualizations in 5.3! (The topics are the same, but not their numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = 'output/ldasaved'\n",
    "ldamodel.save(saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ldamodel.show_topics(ntopics, formatted = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document/Topic Probability\n",
    "The function `get_document_topics()` will list the probability of the topics in a single document. In order to get all the topics set the argument `minimum_probability` to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ldamodel.get_document_topics(corpus[1], minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document/Topic Probability Table\n",
    "A Document/Topiuc probability table is a table (DataFrame), where each row represents a document and each column a topuic. Each cell has the probability of a particular topic in a particular document. The sum of each row is 1 (probability distribution).\n",
    "\n",
    "In order to create a full Document/Topic probability table we iterate over the entire corpus with the `get_document_topics()` function. This creates a list of lists (`list_of_doctopics`) where each list represents the probability of each topic in a document. The probability is represented in a tuple (topic_number, probability). The `list_of_probabilities` preserves only the probabilities. This list of lists is transformed into a DataFrame, whith as index the index of the original DataFrame with the tokenized data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "list_of_doctopics = [ldamodel.get_document_topics(corpus[i], minimum_probability=0) for i in range(len(corpus))]\n",
    "list_of_probabilities = [[probability for label,probability in distribution] for distribution in list_of_doctopics]\n",
    "d_t_df = pd.DataFrame(list_of_probabilities)\n",
    "d_t_df = d_t_df.set_index(df.index)\n",
    "d_t_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above table to find the ten highest scoring documents per topic with the pandas function 'nlargest'. First add the 'designation' as a separate column to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_t_df_w_desig = pd.merge(df['designation'], d_t_df, left_index=True, right_index=True)\n",
    "d_t_df_w_desig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code goes through the (numbered) columns of the table which hold the probabilities of each of the topics (the columns) in each of the documents (the rows). The highest ten probabilities are selected, together with a brief descriptipon of the text (designation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctop = []\n",
    "for i in range(ntopics):\n",
    "    t = d_t_df_w_desig.nlargest(10, i)[['designation', i]]\n",
    "    t['topic'] = i\n",
    "    t = t.rename(columns = {i :'probability'})\n",
    "    doctop.append(t)\n",
    "doctop_df = pd.concat(doctop, axis=0)\n",
    "doctop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renumber Topics\n",
    "Rename the topics (columns) to start with 1, in accordance with the pyLDAvis visualization.\n",
    "\n",
    "The LDAvis package, used in notebook 5.3, was originally written for statistical programming language R; pyLDAvis is a python wrapper. In R indexing starts with 1, rather than 0 (as is the case in Python). In order to prevent confusion, we will rename all topics according to the pyLDAvis convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "topics = [i+1 for i in range(ntopics)]\n",
    "d_t_df.columns = topics\n",
    "d_t_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Topic / Term table\n",
    "This is a table with N rows (the number of topics) and M columns (the number of individual terms in the Dictionary). The table indicates the probability of each term in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term = ldamodel.show_topics(ntopics, formatted=False, num_words=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `topic_term` is a list of tuples in the format (index, \\[list\\]). The index is the topic number, the list is again a list of tuples in the form `(lemma, probability)` in descending order of probability. This means that the lemmas are in a different order in each of the lists.\n",
    "\n",
    "The command `pd.DataFrame(topic_term[i][1])` creates a DataFrame with two columns for the topic `i`. The first column (column 0) has the lemmas, the second column the probabilities. By using the command `set_index(0)` the lemmas become the index (rather than a column) and this index can be used to concatenate the DataFrames (one for each topic) with the `sort = True` option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "topic_term_list = [pd.DataFrame(topic_term[i][1]).set_index(0) for i in range(ntopics)]\n",
    "t_t_df_ = pd.concat(topic_term_list, axis=1, ignore_index=True, sort=True)\n",
    "t_t_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns to start with 1, and Transpose to Topic/Term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t_t_df_.columns = topics\n",
    "t_t_df = t_t_df_.T\n",
    "t_t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just checking\n",
    "t_t_df['ēkallu[palace]n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = {'dictionary': dictionary,\n",
    "                  'corpus' : corpus,\n",
    "                  'ldamodel' : ldamodel,\n",
    "                  't_t_df' : t_t_df,\n",
    "                  'd_t_df' : d_t_df,\n",
    "               'df' : df,\n",
    "              'ntopics' : ntopics,\n",
    "              'texts' : texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/topic_model.p', 'wb') as w:\n",
    "    pickle.dump(topic_model, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
