{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Catalog Data from `catalogue.json`\n",
    "\n",
    "The file `catalogue.json` contains all the catalog data for an [ORACC](http://oracc.org) project (for general information, see the [Oracc Open Data](http://oracc.org/doc/opendata) page). The `zip` that contains all JSON files of a particular project can be found at `http://build-oracc.museum.upenn.edu/json/[PROJECT].zip`. In the URL replace [PROJECT] with your project name (e.g. `dcclt`). For sub-projects the URL pattern is `http://build-oracc.museum.upenn.edu/json/[PROJECT]-[SUBPROJECT].zip`. \n",
    "\n",
    "The main node in a `catalogue.json` file is called `members`. This node contains the information of all the fields and all the entries in the project catalog. This information is put in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "import requests\n",
    "import errno\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Create Directories, if Necessary\n",
    "The two directories needed for this script are `jsonzip` and `output`. If they do not exist they are created, else: do nothing.\n",
    "\n",
    "For the code, see [Stack Overflow](http://stackoverflow.com/questions/18973418/os-mkdirpath-returns-oserror-when-directory-does-not-exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['jsonzip', 'output']\n",
    "for d in directories:\n",
    "    try:\n",
    "        os.mkdir(d)\n",
    "    except OSError as exc:\n",
    "        if exc.errno !=errno.EEXIST:\n",
    "            raise\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Input Project Names\n",
    "We can download and manipulate multiple [ORACC](http://oracc.org) `zip` files at the same time. Note, however that different [ORACC](http://oracc.org) projects use different fields in their catalogs; not all catalogs are mutually compatible.\n",
    "\n",
    "Provide project abbreviations, separated by a comma. Note that subprojects must be processed separately, they are not included in the main project. A subproject is named `[PROJECT]/[SUBPROJECT]`, for instance `saao/saa01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = input('Project(s): ').lower().strip() # lowercase user input and strip accidental spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Split the List of Projects\n",
    "Split the list of projects and create a list of project names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = projects.split(',')               # split at each comma and make a list called `p`\n",
    "p = [x.strip() for x in p]        # strip spaces left and right of each entry in `p`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Download the ZIP files\n",
    "Download all the `json` files from `http://build-oracc.museum.upenn.edu/json/`. The file is called `[PROJECT].zip` (for instance: `dcclt.zip`). For subprojects the file is called `[PROJECT-SUBPROJECT].zip` (for instance `cams-gkab.zip`). \n",
    "\n",
    "For larger projects (such as [DCCLT](http://oracc.org/dcclt)) the `zip` file may be 25Mb or more. Downloading may take some time and it may be necessary to chunk the downloading process. The `iter_content()` function in the `requests` library takes care of that. For the chunking code see [this page](https://www.smallsurething.com/how-to-read-a-file-properly-in-python/).\n",
    "\n",
    "If you have downloaded the files by hand (and put them in the `jsonzip` directory) you may skip this cell and jump directly to section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_existent = []\n",
    "CHUNK = 16 * 1024\n",
    "for project in tqdm.tqdm(p):\n",
    "    project = project.replace('/', '-')\n",
    "    url = \"http://build-oracc.museum.upenn.edu/json/\" + project + \".zip\"\n",
    "    file = 'jsonzip/' + project + '.zip'\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        print(\"Downloading \" + url + \" saving as \" + file)\n",
    "        with open(file, 'wb') as f:\n",
    "            for c in r.iter_content(chunk_size=CHUNK):\n",
    "                f.write(c)\n",
    "    else:\n",
    "        print(url + \" does not exist.\")\n",
    "        non_existent.append(project.replace('-', '/'))\n",
    "p = [i for i in p if i not in non_existent] # remove non-existing project names from list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Extract Catalogue Data from `JSON` files\n",
    "The code in this cell will iterate through the list of projects entered above (1.1). For each project the `JSON` zip file, named `[PROJECT].zip` is located in the directory `jsonzip`. Each of these `zip` files includes a file called `catalogue.json`. This file is extracted and read with the command `json.loads()`, which reads the json data and transforms it into a JSON object - a sequence of names and values.\n",
    "\n",
    "The JSON object is transformed into a Pandas Dataframe. The dataframe needs to be transposed (`.T`), so that the P, Q, and X numbers become indexes or row names (rather than column names), and each column represents a field in the catalog.  The individual dataframes (one for each project requested) are concatenated. Since individual [ORACC](http://oracc.org) project catalogs may have different fields, the dataframes may have different column names. By default Pandas concatenation uses an `outer join` so that all column names of all the catalogs are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame() # create an empty dataframe\n",
    "for project in p:\n",
    "    file = \"jsonzip/\" + project.replace(\"/\", \"-\") + \".zip\"\n",
    "    try:\n",
    "        z = zipfile.ZipFile(file)       # create a Zipfile object\n",
    "    except:\n",
    "        print(file + \" does not exist or is not a proper ZIP file\")\n",
    "        continue\n",
    "    try:\n",
    "        st = z.read(project + '/catalogue.json').decode('utf-8')  #read and decode the catalogue.json file of one project\n",
    "                                                                # the result is a string object\n",
    "    except:\n",
    "        print(project + '/catalogue.json' + ' is not available or not complete')\n",
    "        continue\n",
    "    cat = json.loads(st)\n",
    "    cat = cat['members']  # select the 'members' node \n",
    "    for item in cat.values():\n",
    "        item[\"project\"] = project # add project name as separate field\n",
    "    cat_df = pd.DataFrame(cat).T\n",
    "    df = pd.concat([df, cat_df], sort=True)  # sort=True is necessary in case catalogs have a different set of fields\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean the Dataframe\n",
    "The function `fillna('')` will put a blank (instead of `NaN`) in all fields that have no entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Select Relevant Fields\n",
    "[ORACC](http://oracc.org) catalogs may have custom fields, the only fields that are obligatory are `id_text` (the P, Q, or X number that identifies the text, for instance \"P243546\") and `designation` (the human-readable reference, for instance \"VS 17, 012\"). The example code below works with field names that are available in (almost) every catalog. Adjust the code to your data and your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[: , ['designation', 'period', 'provenience',\n",
    "        'museum_no', 'project', 'id_text']]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.1 Save as CSV\n",
    "Save the resulting data set as a `csv` file. `UTF-8` encoding is the encoding with the widest support in text analysis (and also the encoding used by [ORACC](http://oracc.org)). If you intend to use the catalog file in Excel, however, it is better to use `utf-16` encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'catalog.csv'\n",
    "with open('output/' + filename, 'w', encoding='utf-8') as w:\n",
    "    df1.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.2 Save with Pickle\n",
    "One may pickle a file either with the `pickle` library or directly from within `Pandas` with the `to_pickle()` function. A pickled file preserves the data structure of the dataframe, which is an advantage over saving as `csv`. The pickle file is a binary file, so we must open the file with the `wb` (write binary) option and we cannot give an encoding. To open the pickled file one may use the `read_pickle()` function from the `pandas` library, as in\n",
    "\n",
    "```python\n",
    "with open('output/catalog.p', 'rb') as o: \n",
    "    df = pd.read_pickle(o)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"catalog.p\"\n",
    "with open('output/' + filename, 'wb') as w:\n",
    "    df1.to_pickle(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
