{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sargon's Pen Pals\n",
    "This notebook will pull all the names (senders and people mentioned) from letters to Sargon II, who reigned over Assyria 722-705 BCE. These letters were published in the series *State Archives of Assyria* (SAA) volumes 1, 5, and 15 (1987, 1990, and 2001). Electronic versions of these volumes are found in the [ORACC](http://oracc.org) project *State Archives of Assyria online* ([SAAo](http://oracc.org/saao)).\n",
    "\n",
    "In the letters to Sargon (and to other Assyrian kings) the addressee is hardly ever mentioned by name. Instead, the letter opens with \"to the king my lord\". Simo Parpola, the main editor of the SAA series, assigned the letters to kings, based on his vast knowledge of the corpus.\n",
    "\n",
    "The current notebook will use a network approach to evaluate these assignments, by using the names of a few individuals that are known to have been contemporaries of Sargon. If these people are mentioned in a letter, it is likely that the letter is to Sargon. We may also look at second or third degree relationships, to estimate the plausibility that a letter was indeed sent to Sargon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Create Directories, if Necessary\n",
    "The two directories needed for this script are `jsonzip` and `output`. The directories are created with the function `make_dirs()` from the `utils` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['jsonzip', 'output']\n",
    "make_dirs(directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download the JSON ZIP files\n",
    "Using the function `oracc_download()` from the `utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = [\"saao/saa01\", \"saao/saa05\", \"saao/saa15\"]\n",
    "oracc_download(volumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extract Proper Nouns\n",
    "Each of the ZIP files contains a file `gloss-qpn.json` which contains the glossary of proper nouns in that volume. This file is extracted and loaded in `json`. The extracted data are put in a list, each element of the list represents the proper nouns of one SAA volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_l = []\n",
    "for v in volumes:\n",
    "    file = \"jsonzip/\" + v.replace(\"/\", \"-\") + \".zip\"\n",
    "    z = zipfile.ZipFile(file)\n",
    "    filename = v + \"/gloss-qpn.json\"\n",
    "    qpn = z.read(filename).decode('utf-8')         #read and decode the qpn glossary json file\n",
    "    data_json = json.loads(qpn)  \n",
    "    json_l.append(data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Entries and Instances\n",
    "The function `parse()` builds two DataFrames from different elements in the `gloss-qpn.json`. The key `entries` holds all the headwords (lemmas) in the glossary, with information such as Part of Speech (`pos`), original spelling, number of attestations, etc. The elements to be extracted for the Proper Noun DataFrame (`df_pn`) are Citation Form (`cf`), Part of Speech (`pos`), Guide Word (`gw`) and `xis`. The field `xis` holds an ID for this entry. Note that the `xis` ID is unique *within a project* and may well be duplicated in another project. Moreover, the `xis` is *not persistent*. After a new build of the project, the `xis` IDs will be realigned.\n",
    "\n",
    "The key `instances` in `gloss-qpn.json` provides all the instances (in list form) of each headword, using the same `xis` field to identify the headword. The instance is referred to in the format PROJECT:ID_TEXT.ID_LINE.ID_WORD, for instance: `saao/saa01:P243567.9.1`. We can iterate through the field `xis` in `df_pn` to select the headwords that we need. We build a second DataFrame (`dinst_df`) with two columns: the `xis` ID and the text ID. The text ID can be extracted from the reference by taking the part between the colon and the first dot.\n",
    "\n",
    "The two DataFarmes share the field `xis`. In the second DataFrame (`inst_df`) the same `xis` ID may appear multiple times (because the same name may appear in multiple texts, or multiple times in the same text). We can merge the two DataFrames with the `pandas` function `merge()`, merging on `xis` and using the keys from `inst_df`. The DataFrame that is returned will now have a row for each name instance, associated with a text ID (a P, Q, or X number). Each volume of [SAAo](http://oracc.org/saao) will return a separate DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(data_json):\n",
    "    entries = data_json[\"entries\"]\n",
    "    df_pn = pd.DataFrame(entries)\n",
    "    df_pn = df_pn[[\"cf\", \"gw\", \"pos\", \"xis\"]]\n",
    "    df_pn = df_pn.loc[df_pn[\"pos\"].isin([\"PN\", \"RN\"])]\n",
    "    instances = data_json[\"instances\"]\n",
    "    l = []\n",
    "    for i in df_pn[\"xis\"]:\n",
    "        for k in instances[i]:\n",
    "            QPN = k.split(\":\")[1]\n",
    "            QPN = QPN.split(\".\")[0]\n",
    "            d = [i, QPN]\n",
    "            l.append(d)\n",
    "    inst_df = pd.DataFrame(l)\n",
    "    inst_df.columns = [\"xis\", \"id_text\"]\n",
    "    df = inst_df.merge(df_pn, on='xis', how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create and Concatenate the Lists of Name Instances\n",
    "For each of the projects with Sargon letters in [SAAo](http://oracc.org/saao) the `json` data from its `gloss-qpn.json` are sent to the `parse()` function. This function refturns a DataFrame with all name instances, associated with text IDs. The code below collects those DataFrames in the list `pn_l`, concatenates them and then drops the filed `xis`. Since `xis` is project-specific, it has become meaningless in this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_l = []\n",
    "for j in json_l:\n",
    "    pns = parse(j)\n",
    "    pn_l.append(pns)\n",
    "df = pd.concat(pn_l)\n",
    "df = df.drop(\"xis\", axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Node List\n",
    "The Node List is simply the list of all unique headwords. Add to the nodes list whether a name is known as a (Sargon-period) eponym. Save the nodes list as `nodes.csv` in the `output` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_df = df[[\"cf\", \"gw\"]].copy()\n",
    "n_df = n_df.drop_duplicates().reset_index(drop=True)\n",
    "n_df.columns = [\"label\", \"namesake\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"csv/sargoneponyms.csv\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    eponyms = pd.read_csv(f)\n",
    "eponyms[\"eponym\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = n_df.merge(eponyms, on = \"label\", how=\"outer\")\n",
    "nodes_df[\"eponym\"] = nodes_df[\"eponym\"].fillna(False)\n",
    "nodes_df[\"Id\"] = nodes_df[\"label\"].copy()\n",
    "nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/nodes.csv\", mode=\"w\", encoding=\"utf-8\") as w:\n",
    "    nodes_df.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Edge List\n",
    "Transform the Pandas Dataframe into a simple list of list. In order to produce the edge list we use a loop within a loop. The first loop goes through all the items in the list (all names). For each name, it goes through the entire list again, to find items that match the same text ID (P number). This way, the routine finds all pairs of names that appear in each letter.\n",
    "\n",
    "The secondary loop begins at the location of the index of the primary loop. This way, the edge A == B is not duplicated by the edge B == A (since the edges are undirected).\n",
    "\n",
    "If there is a text ID match in the secondary loop, make a list that contains `id_text`, `source`, and `target` - this list represents a single edge. Add this list to the list of lists `edges`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values.tolist()\n",
    "edges = []\n",
    "for idx, item in enumerate(data):\n",
    "    textid = item[0]\n",
    "    source = item[1]\n",
    "    for idx_2, item_2 in enumerate(data[idx:len(data)]):\n",
    "        if item[0] == item_2[0]:\n",
    "            if not item[1] == item_2[1]:   # no SELF == SELF edges\n",
    "                target = item_2[1]\n",
    "                edge = [textid, source, target]\n",
    "                edges.append(edge)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `edges` is a list of lists that can be transformed again into a Dataframe. If the same name is mentioned multiple times in the same letter, that will create duplicate edges. Drop the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = pd.DataFrame(edges, columns= [\"id_text\", \"source\", \"target\"])\n",
    "edges_df = edges_df.drop_duplicates().reset_index(drop=True)\n",
    "edges_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Edge List\n",
    "Save the edge list as `edges.csv` in the `output` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/edges.csv\", mode=\"w\", encoding=\"utf-8\") as w:\n",
    "    edges_df.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
