{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from ipywidgets import interact\n",
    "import re\n",
    "import pickle\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the table with P numbers of tablets that belong to the Treasure Archive and Shoe Archive. Retrieve a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'csv/treasury_shoes2.txt'\n",
    "tr_df = pd.read_csv(file, encoding='utf8')\n",
    "ids = list(tr_df['id_text'])\n",
    "ids.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, download the Ur3 JSON from epsd2. If the file is already in the directory `/jsonzip`, skip this step, but do assign 'epsd2/admin/ur3' to the variable `project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'epsd2/admin/ur3'\n",
    "oracc_download([project]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for parsing the JSON. Words that appear in year names are marked in the field `ftype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject)\n",
    "        if \"label\" in JSONobject:\n",
    "            meta_d[\"label\"] = JSONobject['label']\n",
    "        if \"f\" in JSONobject:\n",
    "            lemma = JSONobject[\"f\"]\n",
    "            if \"ftype\" in JSONobject:\n",
    "                lemma['ftype'] = JSONobject['ftype'] # this picks up YN for year name\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma['label'] = meta_d[\"label\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "        if \"strict\" in JSONobject and JSONobject[\"strict\"] == \"1\":\n",
    "            lemma = {key: JSONobject[key] for key in dollar_keys}\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the parser for each of the P numbers in the list `ids`. The parser function needs the lists `lemm_l` and `dollar_keys`, as well as the dictionary `meta_d`. These objects are manipulated during the parsing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_l = []\n",
    "meta_d = {\"label\": None, \"id_text\": None}\n",
    "dollar_keys = [\"extent\", \"scope\", \"state\"]\n",
    "file = f\"jsonzip/{project.replace('/', '-')}.zip\"\n",
    "try:\n",
    "    z = zipfile.ZipFile(file) \n",
    "except:\n",
    "    print(f\"{file} does not exist or is not a proper ZIP file\")\n",
    "files = z.namelist() # list of all the files in the ZIP file\n",
    "files = [name for name in files if name[-12:-5] in ids] # select only those of the treasury/leather archive\n",
    "for filename in tqdm(files, desc = project):\n",
    "    id_text = project + filename[-13:-5] \n",
    "    meta_d[\"id_text\"] = id_text\n",
    "    try:\n",
    "        st = z.read(filename).decode('utf-8')\n",
    "        data_json = json.loads(st)           \n",
    "        parsejson(data_json)\n",
    "    except:\n",
    "        print(f'{id_text} is not available or not complete')\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parsejson()` fills the lists `lemm_l` with data. Read this list into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(lemm_l).fillna('')\n",
    "keep = ['extent', 'scope', 'state', 'id_word', 'id_text', 'form', 'cf', 'gw', 'pos', 'ftype', 'label']\n",
    "words = words[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove comma's and spaces from Guide words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['gw'] = words['gw'].replace([' ', ','], ['', ''], regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify `id_text` ('P123456' instead of 'epsd2/admin/ur3/P123456') and add a field `id_line`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['id_text'] = [i[-7:] for i in words['id_text']]\n",
    "words['id_line'] = [int(i.split('.')[1]) for i in words['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_nouns = ['RN', 'PN', 'DN', 'AN', 'WN', 'ON', 'TN', 'CN', 'GN', 'SN']\n",
    "physical_break = ['illegible', 'traces', 'missing', 'effaced']\n",
    "logical_break = ['other', 'blank', 'ruling']\n",
    "words['lemma'] = words[\"cf\"] + '[' + words[\"gw\"] + ']' + words[\"pos\"]\n",
    "words.loc[words[\"cf\"] == \"\" , 'lemma'] = words['form'] + '[NA]NA'\n",
    "words.loc[words[\"pos\"] == \"n\" , 'lemma'] = words['form'] + '[]NU'\n",
    "words.loc[words[\"state\"].isin(logical_break), 'lemma'] = \"break_logical\"\n",
    "words.loc[words[\"state\"].isin(physical_break), 'lemma'] = \"break_physical\"\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read list of name forms and Normalized names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normdf = pd.read_csv('Normalized/drehem_norm_names.csv', encoding='utf8')\n",
    "normdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download OGSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracc_download(['ogsl']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List Ur 3 sign equivalencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv = {'ANŠE' : 'GIR₃', \n",
    "        'DUR₂' : 'KU', \n",
    "        'NAM₂' : 'TUG₂', \n",
    "        'TIL' : 'BAD', \n",
    "        'NI₂' : 'IM',\n",
    "        'ŠAR₂' : 'HI', \n",
    "        }\n",
    "w = re.compile(r'\\w+') # replace whole words only - do not replace TILLA with BADLA.\n",
    "           # but do replace |SAL.ANŠE| with |SAL.GIR₃|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse OGSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseogsljson(data_json):\n",
    "    for key, value in data_json[\"signs\"].items():\n",
    "        key = re.sub(w, lambda m: equiv.get(m.group(), m.group()), key)\n",
    "        if \"values\" in value:\n",
    "            for n in value[\"values\"]:\n",
    "                d2[n] = key\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create OGSL dictionary key = sign value, value = sign name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {}  # this empty dictionary is filled by the parsejson() function, called in this cell.\n",
    "file = \"jsonzip/ogsl.zip\"\n",
    "z = zipfile.ZipFile(file) \n",
    "filename = \"ogsl/ogsl-sl.json\"\n",
    "signlist = z.read(filename).decode('utf-8')\n",
    "data_json = json.loads(signlist)                # make it into a json object (essentially a dictionary)\n",
    "parseogsljson(data_json)  \n",
    "with open('output/ogsl_dict.p', 'wb') as p:\n",
    "    pickle.dump(d2, p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = ['{', '}', '-']\n",
    "separators2 = ['.', '+', '|']  # used in compound signs\n",
    "#operators = ['&', '%', '@', '×']\n",
    "flags = \"][?<>⸢⸣⌈⌉*/\" # note that ! is omitted from flags, because it is dealt with separately\n",
    "table = str.maketrans(dict.fromkeys(flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signnames(translit):  \n",
    "    \"\"\"This function takes a string of transliterated cuneiform text and translates that string into a string of\n",
    "    sign names, separated by spaces. In order to work it needs the variables separators, separators2, and table defined above. The variable table\n",
    "    is used by the translate() method to translate all flags (except for !) to None. The function also needs a dictionary, called d2, that has as\n",
    "    keys sign readings and sign names as corresponding values. In case a key is not found, the sign reading is replaced by itself.\"\"\"\n",
    "    signnames_l = []\n",
    "    translit = translit.translate(table).lower()  # remove flags, half brackets, square brackets.\n",
    "    translit = translit.replace('...', 'x')\n",
    "    for s in separators: # split transliteration line into signs   \n",
    "        translit = translit.replace(s, ' ').strip()\n",
    "    s_l = translit.split() # s_l is a list that contains the sequence of transliterated signs without separators or flags\n",
    "    s_l = [d2.get(sign, sign) for sign in s_l] # replace each transliterated sign with its sign name.\n",
    "    # Now take care of some special situations: signs with qualifiers, compound signs.\n",
    "    for sign in s_l:\n",
    "        if '!' in sign: # corrected sign, as in ka!(SAG), get only the corrected reading.\n",
    "            sign = sign.split('!(')[0]\n",
    "            sign = sign.replace('!', '') # remove remaining exclamation marks\n",
    "        elif sign[-1] == ')' and '(' in sign: # qualified sign, as in ziₓ(SIG₇) - get only the qualifier\n",
    "            sign = sign.split('(')[1][:-1]\n",
    "        if '×' in sign: #compound. Compound like |KA×NINDA| to be replaced by |KA×GAR|\n",
    "            sign_l = sign.replace('|', '').split('×')\n",
    "            #replace individual signs of the compound by OGSL names\n",
    "            sign_l = [d2.get(sign, sign) for sign in sign_l] \n",
    "            # if user enters |KA*EŠ| this is transformed to ['KA', '|U.U.U|']. The pipes around U.U.U must be replaced by brackets\n",
    "            sign_l = [f'({sign[1:-1]})' if len(sign) > 1 and sign[0] == '|' else sign for sign in sign_l]\n",
    "            sign = f\"|{'×'.join(sign_l)}|\"  #put the sign together again with enclosing pipes.\n",
    "        elif '.' in sign or '+' in sign: # using elif, so that compounds like |UD×(U.U.U)| are not further analyzed.\n",
    "            for s in separators2:\n",
    "                sign = sign.replace(s, ' ').strip() \n",
    "            sign_l = sign.split()  # compound sign split into multiple signs\n",
    "            sign_l = [d2.get(sign, sign) for sign in sign_l]\n",
    "            for se in separators2:   # in case d2.get returns a compound sign name\n",
    "                sign_l = [si.replace(se, ' ').strip() for si in sign_l]\n",
    "            signnames_l.extend(sign_l)\n",
    "            continue\n",
    "        sign = d2.get(sign, sign)\n",
    "        signnames_l.append(sign)\n",
    "    # add space before and after each line so that each sign representation is enclosed in spaces\n",
    "    signnames = f\" {' '.join(signnames_l).upper()} \" \n",
    "    return signnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new field in the Normalized Names table, representing the sign sequence (sign names) of the transliteration of each name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normdf[\"sign_names\"] = normdf[\"transliteration\"].progress_map(signnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with `sign_names` as key and `normalization` as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normd2 = dict(zip(normdf['sign_names'], normdf['normalization']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to `words` with the sequence of sign names for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['sign_names'] = words['form'].progress_map(signnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `normd2` dictionary to transform sign name sequences into normalized names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words.loc[(words.pos.isin(proper_nouns + ['X'])) & (words.lemma.str.contains('.')), \n",
    "#          'lemma'] = words.progress_apply(lambda x: normd2.get(x['sign_names'], x['lemma']), axis=1)\n",
    "words['lemma'] = words.progress_apply(lambda x: normd2.get(x['sign_names'], x['lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the (temporary) corrections.csv file to apply corrections to the lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = pd.read_csv('Normalized/corrections.csv', encoding='utf8')\n",
    "corr_d = dict(zip(corrections['form'], corrections['corr']))\n",
    "words.loc[words.pos.isin(['PN', 'RN', 'X']), 'lemma'] = words.progress_apply(lambda x: corr_d.get(x['form'], x['lemma']), axis =1)\n",
    "words['pos'] = [w.split(']')[-1] if ']' in w else '' for w in words['lemma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select PNs (and RNs) but skip those that appear in Year Names or Seals. Add lugal (king) sukkalmah (prime minister) and nin (queen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed = {'lugal[king]N', 'nin[queen]N', 'sukkalmah[official]N'}\n",
    "names = set(words.loc[words.pos.isin(['PN', 'RN']), 'lemma'])\n",
    "actors = unnamed | names\n",
    "seal = list(words.loc[words.label.str.contains('seal'), 'id_word'])\n",
    "yn = list(words.loc[words.ftype == 'yn', 'id_word'])\n",
    "exclude = seal + yn\n",
    "PNs = words.loc[(words.lemma.isin(actors)) & (~words.id_word.isin(exclude))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation = {'dumu[child]N' : 'son of', 'adda[father]N' : 'father of',\n",
    "            'ama[mother]N' : 'mother of', 'nin[sister]N' : 'sister of',\n",
    "            'šeš[brother]N' : 'brother of', 'dam[spouse]N' : 'spouse of',\n",
    "            'lu[person]N' : 'representative of', 'dumumunus[daughter]N' : 'daughter of'}\n",
    "profession = ['nar[singer]', 'gala[singer]N', 'lugal[king]N', 'sukkal[secretary]N', 'šakkanak[general]N', \n",
    "             'sukkalmah[official]N', 'šagia[cup-bearer]N']\n",
    "places = [word for word in words['lemma'] if word[-2:] in ['SN', 'GN']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make sure that words before and after belong to the same text!\n",
    "\n",
    "Check that indexes that are referenced belong to the indexes of that text with minimum and maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = []\n",
    "attribute = []\n",
    "no = []\n",
    "for i in PNs.index:\n",
    "    text = words.loc[words.id_text == PNs.loc[i]['id_text']]\n",
    "    mx = text.index[-1]\n",
    "    mn = text.index[0]\n",
    "    r = ''\n",
    "    a = ''\n",
    "    n = 1\n",
    "    for l in [relation, ['u[and]CNJ'], profession, places]:\n",
    "        if i+n < mx:\n",
    "            if text.loc[i+n]['lemma'] in l:\n",
    "                a = f\"{a} {text.loc[i+n]['lemma']}\"\n",
    "                n += 1\n",
    "        else:\n",
    "            break\n",
    "    if words.loc[i+n]['lemma'] == 'maškim[administrator]N': \n",
    "        r = 'representative'\n",
    "    elif words.loc[i+1]['lemma'] == 'zig[rise]V/i': \n",
    "        r = 'expender'\n",
    "    elif i+n+1 <= mx:\n",
    "        if f\"{words.loc[i+n]['lemma']} {words.loc[i+n+1]['lemma']}\" == 'šu[hand]N teŋ[near]V/i': \n",
    "            r = 'recipient'\n",
    "        elif f\"{words.loc[i+n]['lemma']} {words.loc[i+n+1]['lemma']}\" == 'šu[hand]N us[follow]V/t': \n",
    "            r = 'sender'\n",
    "    if i-1 >= mn:\n",
    "        pre = words.loc[i-1]['lemma']\n",
    "        if pre == 'ŋiri[foot]N':\n",
    "            r = 'intermediary'\n",
    "        elif pre == 'ki[place]N' and words.loc[i+n-1]['form'].endswith('-ta'): \n",
    "            r = 'source'\n",
    "        elif pre == 'ki[place]N' and words.loc[i+n-1]['form'].endswith('-še₃'): \n",
    "            r = 'destination'\n",
    "        elif pre == 'arua[offering]N': \n",
    "            r = 'offerer'\n",
    "        elif pre in relation:\n",
    "            r = 'relative'\n",
    "        elif pre == 'u[and]CNJ':\n",
    "            r = 'companion'\n",
    "        elif pre == 'lu[person]N':\n",
    "            r = 'superior'\n",
    "    role.append(r)\n",
    "    attribute.append(a)\n",
    "    no.append(n)\n",
    "PNs['role'] = role       \n",
    "PNs['attribute'] = attribute \n",
    "PNs['no'] = no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://build-oracc.museum.upenn.edu/epsd2/admin/ur3/{}\", target=\"_blank\">{}</a>'\n",
    "PNs2 = PNs.copy()\n",
    "PNs2['id_word'] = [anchor.format(val,val) for val in PNs['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(rows = (1, len(PNs2), 1))\n",
    "def showpns(rows = 25): \n",
    "    return PNs2[['id_word', 'form', 'pos', 'lemma', 'no', 'role', 'attribute']][:rows].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNs.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNs.loc[11475]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 49\n",
    "print(words.loc[i]['lemma']), print(words.shift(-1).loc[i]['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.loc[48:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
