{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from ipywidgets import interact\n",
    "import re\n",
    "import pickle\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the table with P numbers of tablets that belong to the Treasure Archive and Shoe Archive. Retrieve a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'csv/treasury_shoes2.txt'\n",
    "tr_df = pd.read_csv(file, encoding='utf8')\n",
    "ids = list(tr_df['id_text'])\n",
    "ids.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, download the Ur3 JSON from epsd2. If the file is already in the directory `/jsonzip`, skip this step, but do assign 'epsd2/admin/ur3' to the variable `project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'epsd2/admin/ur3'\n",
    "#oracc_download([project]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for parsing the JSON. Words that appear in year names are marked in the field `ftype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject)\n",
    "        if \"label\" in JSONobject:\n",
    "            meta_d[\"label\"] = JSONobject['label']\n",
    "        if \"f\" in JSONobject:\n",
    "            lemma = JSONobject[\"f\"]\n",
    "            if \"ftype\" in JSONobject:\n",
    "                lemma['ftype'] = JSONobject['ftype'] # this picks up YN for year name\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma['label'] = meta_d[\"label\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "        if \"strict\" in JSONobject and JSONobject[\"strict\"] == \"1\":\n",
    "            lemma = {key: JSONobject[key] for key in dollar_keys}\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the parser for each of the P numbers in the list `ids`. The parser function needs the lists `lemm_l` and `dollar_keys`, as well as the dictionary `meta_d`. These objects are manipulated during the parsing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_l = []\n",
    "meta_d = {\"label\": None, \"id_text\": None}\n",
    "dollar_keys = [\"extent\", \"scope\", \"state\"]\n",
    "file = f\"jsonzip/{project.replace('/', '-')}.zip\"\n",
    "try:\n",
    "    z = zipfile.ZipFile(file) \n",
    "except:\n",
    "    print(f\"{file} does not exist or is not a proper ZIP file\")\n",
    "files = z.namelist() # list of all the files in the ZIP file\n",
    "files = [name for name in files if name[-12:-5] in ids] # select only those of the treasury/leather archive\n",
    "for filename in tqdm(files, desc = project):\n",
    "    id_text = project + filename[-13:-5] \n",
    "    meta_d[\"id_text\"] = id_text\n",
    "    try:\n",
    "        st = z.read(filename).decode('utf-8')\n",
    "        data_json = json.loads(st)           \n",
    "        parsejson(data_json)\n",
    "    except:\n",
    "        print(f'{id_text} is not available or not complete')\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parsejson()` fills the lists `lemm_l` with data. Read this list into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(lemm_l).fillna('')\n",
    "keep = ['extent', 'scope', 'state', 'id_word', 'id_text', 'form', 'cf', 'gw', 'pos', 'ftype', 'label']\n",
    "words = words[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove comma's and spaces from Guide words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['gw'] = words['gw'].replace([' ', ','], ['', ''], regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify `id_text` ('P123456' instead of 'epsd2/admin/ur3/P123456') and add a field `id_line`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['id_text'] = [i[-7:] for i in words['id_text']]\n",
    "words['id_line'] = [int(i.split('.')[1]) for i in words['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_nouns = ['RN', 'PN', 'DN', 'AN', 'WN', 'ON', 'TN', 'CN', 'GN', 'SN']\n",
    "physical_break = ['illegible', 'traces', 'missing', 'effaced']\n",
    "logical_break = ['other', 'blank', 'ruling']\n",
    "words['lemma'] = words[\"cf\"] + '[' + words[\"gw\"] + ']' + words[\"pos\"]\n",
    "words.loc[words[\"cf\"] == \"\" , 'lemma'] = words['form'] + '[NA]NA'\n",
    "words.loc[words[\"pos\"] == \"n\" , 'lemma'] = words['form'] + '[]NU'\n",
    "words.loc[words[\"state\"].isin(logical_break), 'lemma'] = \"break_logical\"\n",
    "words.loc[words[\"state\"].isin(physical_break), 'lemma'] = \"break_physical\"\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read list of name forms and Normalized names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normdf = pd.read_csv('Normalized/drehem_norm_names.csv', encoding='utf8')\n",
    "normdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download OGSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracc_download(['ogsl']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List Ur 3 sign equivalencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv = {'ANŠE' : 'GIR₃', \n",
    "        'DUR₂' : 'KU', \n",
    "        'NAM₂' : 'TUG₂', \n",
    "        'TIL' : 'BAD', \n",
    "        'NI₂' : 'IM',\n",
    "        'ŠAR₂' : 'HI', \n",
    "        }\n",
    "w = re.compile(r'\\w+') # replace whole words only - do not replace TILLA with BADLA.\n",
    "           # but do replace |SAL.ANŠE| with |SAL.GIR₃|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse OGSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseogsljson(data_json):\n",
    "    for key, value in data_json[\"signs\"].items():\n",
    "        key = re.sub(w, lambda m: equiv.get(m.group(), m.group()), key)\n",
    "        if \"values\" in value:\n",
    "            for n in value[\"values\"]:\n",
    "                d2[n] = key\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create OGSL dictionary key = sign value, value = sign name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {}  # this empty dictionary is filled by the parsejson() function, called in this cell.\n",
    "file = \"jsonzip/ogsl.zip\"\n",
    "z = zipfile.ZipFile(file) \n",
    "filename = \"ogsl/ogsl-sl.json\"\n",
    "signlist = z.read(filename).decode('utf-8')\n",
    "data_json = json.loads(signlist)                # make it into a json object (essentially a dictionary)\n",
    "parseogsljson(data_json)  \n",
    "with open('output/ogsl_dict.p', 'wb') as p:\n",
    "    pickle.dump(d2, p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = ['{', '}', '-']\n",
    "separators2 = ['.', '+', '|']  # used in compound signs\n",
    "#operators = ['&', '%', '@', '×']\n",
    "flags = \"][?<>⸢⸣⌈⌉*/\" # note that ! is omitted from flags, because it is dealt with separately\n",
    "table = str.maketrans(dict.fromkeys(flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signnames(translit):  \n",
    "    \"\"\"This function takes a string of transliterated cuneiform text and translates that string into a string of\n",
    "    sign names, separated by spaces. In order to work it needs the variables separators, separators2, and table defined above. The variable table\n",
    "    is used by the translate() method to translate all flags (except for !) to None. The function also needs a dictionary, called d2, that has as\n",
    "    keys sign readings and sign names as corresponding values. In case a key is not found, the sign reading is replaced by itself.\"\"\"\n",
    "    signnames_l = []\n",
    "    translit = translit.translate(table).lower()  # remove flags, half brackets, square brackets.\n",
    "    translit = translit.replace('...', 'x')\n",
    "    for s in separators: # split transliteration line into signs   \n",
    "        translit = translit.replace(s, ' ').strip()\n",
    "    s_l = translit.split() # s_l is a list that contains the sequence of transliterated signs without separators or flags\n",
    "    s_l = [d2.get(sign, sign) for sign in s_l] # replace each transliterated sign with its sign name.\n",
    "    # Now take care of some special situations: signs with qualifiers, compound signs.\n",
    "    for sign in s_l:\n",
    "        if '!' in sign: # corrected sign, as in ka!(SAG), get only the corrected reading.\n",
    "            sign = sign.split('!(')[0]\n",
    "            sign = sign.replace('!', '') # remove remaining exclamation marks\n",
    "        elif sign[-1] == ')' and '(' in sign: # qualified sign, as in ziₓ(SIG₇) - get only the qualifier\n",
    "            sign = sign.split('(')[1][:-1]\n",
    "        if '×' in sign: #compound. Compound like |KA×NINDA| to be replaced by |KA×GAR|\n",
    "            sign_l = sign.replace('|', '').split('×')\n",
    "            #replace individual signs of the compound by OGSL names\n",
    "            sign_l = [d2.get(sign, sign) for sign in sign_l] \n",
    "            # if user enters |KA*EŠ| this is transformed to ['KA', '|U.U.U|']. The pipes around U.U.U must be replaced by brackets\n",
    "            sign_l = [f'({sign[1:-1]})' if len(sign) > 1 and sign[0] == '|' else sign for sign in sign_l]\n",
    "            sign = f\"|{'×'.join(sign_l)}|\"  #put the sign together again with enclosing pipes.\n",
    "        elif '.' in sign or '+' in sign: # using elif, so that compounds like |UD×(U.U.U)| are not further analyzed.\n",
    "            for s in separators2:\n",
    "                sign = sign.replace(s, ' ').strip() \n",
    "            sign_l = sign.split()  # compound sign split into multiple signs\n",
    "            sign_l = [d2.get(sign, sign) for sign in sign_l]\n",
    "            for se in separators2:   # in case d2.get returns a compound sign name\n",
    "                sign_l = [si.replace(se, ' ').strip() for si in sign_l]\n",
    "            signnames_l.extend(sign_l)\n",
    "            continue\n",
    "        sign = d2.get(sign, sign)\n",
    "        signnames_l.append(sign)\n",
    "    # add space before and after each line so that each sign representation is enclosed in spaces\n",
    "    signnames = f\" {' '.join(signnames_l).upper()} \" \n",
    "    return signnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new field in the Normalized Names table, representing the sign sequence (sign names) of the transliteration of each name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normdf[\"sign_names\"] = normdf[\"transliteration\"].progress_map(signnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with `sign_names` as key and `normalization` as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normd2 = dict(zip(normdf['sign_names'], normdf['normalization']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to `words` with the sequence of sign names for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['sign_names'] = words['form'].progress_map(signnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `normd2` dictionary to transform sign name sequences into normalized names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words.loc[(words.pos.isin(proper_nouns + ['X'])) & (words.lemma.str.contains('.')), \n",
    "#          'lemma'] = words.progress_apply(lambda x: normd2.get(x['sign_names'], x['lemma']), axis=1)\n",
    "words.loc[words.pos.isin(proper_nouns + ['X']), 'lemma'] = words.progress_apply(lambda x: normd2.get(x['sign_names'], x['lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the (temporary) corrections.csv file to apply corrections to the lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = pd.read_csv('Normalized/corrections.csv', encoding='utf8')\n",
    "corr_d = dict(zip(corrections['form'], corrections['corr']))\n",
    "words.loc[words.pos.isin(['PN', 'RN', 'X', 'DN']), 'lemma'] = words.progress_apply(lambda x: corr_d.get(x['form'], x['lemma']), axis =1)\n",
    "words['pos'] = [w.split(']')[-1] if ']' in w else '' for w in words['lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttype = []\n",
    "for i in ids:\n",
    "    ttype = ''\n",
    "    text = list(words.loc[words.id_text == i, 'lemma'])\n",
    "    if 'mu.DU[delivery]N' in text:\n",
    "        ttype = 'intake'\n",
    "    elif 'zig[rise]V/i' in text:\n",
    "        ttype = 'expenditure'\n",
    "    elif 'teŋ[near]V/i' in text: \n",
    "        ttype = 'transfer'\n",
    "    texttype.append(ttype)\n",
    "treasure = dict(zip(ids, texttype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select PNs (and RNs) but skip those that appear in Year Names or Seals. Add lugal (king) sukkalmah (prime minister) and nin (queen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed = {'lugal[king]N', 'nin[queen]N', 'sukkalmah[official]N'}\n",
    "names = set(words.loc[words.pos.isin(['PN', 'RN', 'DN']), 'lemma'])\n",
    "actors = unnamed | names\n",
    "seal = list(words.loc[words.label.str.contains('seal'), 'id_word'])\n",
    "yn = list(words.loc[words.ftype == 'yn', 'id_word'])\n",
    "exclude = seal + yn\n",
    "PNs = words.loc[(words.lemma.isin(actors)) & (~words.id_word.isin(exclude))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define keywords and the corresponding roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_post = {'mu.DU[delivery]N': 'deliverer', 'maškim[administrator]N' : 'representative', \n",
    "               'zig[rise]V/i' : 'expender',\n",
    "               'šu[hand]N teŋ[near]V/i': 'recipient' , 'šu[hand]N us[follow]V/t': 'sender'}\n",
    "key_pre =  {'ŋiri[foot]N' : 'intermediary', \n",
    "            'arua[offering]N' : 'offerer', 'kišib[seal]N' : 'sealer',\n",
    "            'mu[name]N' : 'reason', 'ki[place]N' : 'source'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the Role of each PN\n",
    "TODO: default role in texttype transfer. Check default roles for other text types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = []\n",
    "attribute = []\n",
    "for i in PNs.index:                           # the index of PNs is identical to the one of words\n",
    "    Pno = PNs.loc[i]['id_text']                    # the text ID (P number)\n",
    "    lineno = PNs.loc[i]['id_line']                 # the line number inwhich the PN appears\n",
    "    position_in_line = int(PNs.loc[i]['id_word'].split('.')[-1]) # the position of the PN in its line (starting at 1)\n",
    "    text = words.loc[words.id_text == Pno]                # the entire text\n",
    "    line = list(text.loc[text.id_line == lineno, 'lemma']) # list of words that appear in the line with the PN\n",
    "    line_i = text.loc[text.id_line == lineno].index  # indexes of words in line\n",
    "    mx = text.index[-1]                    # highest index no. of word in text\n",
    "    mxl = text.loc[mx]['id_line']          # highest line number in text\n",
    "    if lineno < mxl:                       # check that nextline is still in the same text\n",
    "        nextl = words.loc[line_i[-1]+1]['id_line']\n",
    "        nextline = list(text.loc[text.id_line == nextl, 'lemma']) # list of words in nextline\n",
    "    if not nextline:\n",
    "        nextline = ['none']                # nextline needs content\n",
    "    attr = []\n",
    "    r = ''\n",
    "    \n",
    "    if position_in_line == 1:               # PN in first position\n",
    "        r = key_post.get(line[-1], r)    # keyword appears in same line in last position\n",
    "        r = key_post.get(nextline[0], r) # or in the next line in first position\n",
    "        if len(line) > 1:                   # additional words are attributes of the name\n",
    "            attr = line[1:]\n",
    "            lastwords = ' '.join(line[-2:]) # two-word keywords (as in šu ba-ti)\n",
    "            r = key_post.get(lastwords, r)\n",
    "        if len(nextline) > 1:               # two-word keyword appearing in firstposition in next line\n",
    "            firstwords = ' '.join(nextline[0:2])\n",
    "            r = key_post.get(firstwords, r)\n",
    "            \n",
    "    elif position_in_line == 2:             # PN appears in second position with keyword preceding\n",
    "        r = key_pre.get(line[0], r)        # ŋiri₃ PN or ki PN-ta\n",
    "        if line[0] == 'ki[place]N' and list(text.loc[text.id_line == lineno, 'form'])[-1].endswith('-še₃'): \n",
    "            r = 'destination'              # special case: ki PN-še₃\n",
    "        if len(line) > 2:\n",
    "            attr = line[2:]\n",
    "    else:\n",
    "        PN = [w for w in line[:position_in_line-1] if w in list(PNs['lemma'])] \n",
    "        if PN:\n",
    "            if line[position_in_line -2] == 'u[and]CNJ':\n",
    "                r = role[-1]               # same role as previous PN\n",
    "            else:\n",
    "                r = 'relation'             # as in PN dumu PN\n",
    "        else:                              # if there is no preceding PN: drop the name\n",
    "            r = 'none'                     # this may  need refinement\n",
    "    if r == '' :\n",
    "        if treasure[Pno] == 'expenditure': # default role for zi-ga/ba-zi texts\n",
    "            r = 'recipient'\n",
    "        elif treasure[Pno] == 'intake' or treasure[Pno] == 'transfer':    # default role for mu-DU texts\n",
    "            r = 'source'\n",
    "    role.append(r)\n",
    "    attribute.append(' '.join(attr))\n",
    "PNs['role'] = role\n",
    "PNs['attribute'] = attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results in a table with links for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://build-oracc.museum.upenn.edu/epsd2/admin/ur3/{}\", target=\"_blank\">{}</a>'\n",
    "PNs2 = PNs.copy()\n",
    "PNs2['id_word'] = [anchor.format(val,val) for val in PNs['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(rows = (1, len(PNs2), 1))\n",
    "def showpns(rows = 25): \n",
    "    return PNs2.loc[PNs2.id_text == 'P124318', ['id_word', 'form', 'pos', 'lemma', 'role', 'attribute']][:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Edges\n",
    "Needs checking. Deal with roles 'offerer' and 'reason'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake = [i for i in ids if treasure[i] == 'intake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for i in intake:                              # one text ID at a time\n",
    "    people = PNs.index[PNs.id_text == i]    # indexes of all the people in that text\n",
    "    for p in people:                       # iterate over those indexes\n",
    "        source = ''\n",
    "        target = ''\n",
    "        role = PNs.loc[p]['role']\n",
    "        if role in ['intermediary', 'representative']:\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in people if n > p]\n",
    "            if q:\n",
    "                for r in q:\n",
    "                    if PNs.loc[r]['role'] in ['recipient', 'sealer']:\n",
    "                        target = PNs.loc[r]['lemma']\n",
    "                        break\n",
    "        elif role in ['sender', 'deliverer', 'source']:\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in people if n > p]\n",
    "            if q:\n",
    "                for r in q:\n",
    "                    if PNs.loc[r]['role'] in ['recipient', 'sealer', 'intermediary']:\n",
    "                        target = PNs.loc[r]['lemma']\n",
    "                        break\n",
    "        elif role == 'relation':\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in people if n < p]\n",
    "            if q:\n",
    "                target = PNs.loc[q[-1]]['lemma']\n",
    "        if source and target:\n",
    "            edges.append([source, target, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgs = pd.DataFrame(edges)\n",
    "edgs.columns = ['source', 'target', 'id_text']\n",
    "anchor = '<a href=\"http://build-oracc.museum.upenn.edu/epsd2/admin/ur3/{}\", target=\"_blank\">{}</a>'\n",
    "edgs2 = edgs.copy()\n",
    "edgs2['id_text'] = [anchor.format(val,val) for val in edgs['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(rows = (1, len(edgs2), 1))\n",
    "def showedges(rows = 25): \n",
    "    return edgs2[:rows].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mudus = []\n",
    "deliverer = ''\n",
    "for i in intake:\n",
    "    people = PNs.index[PNs.id_text == i].tolist()\n",
    "    text = words.loc[words.id_text == i]\n",
    "    mudu = text.index[text.lemma == 'mu.DU[delivery]N'].tolist()[0]\n",
    "    p = [p for p in people if p < mudu]\n",
    "    if len(p) > 0:\n",
    "        deliverer = [PNs.loc[max(p), 'lemma'], i]\n",
    "    else:\n",
    "        deliverer = ['[...]', i]\n",
    "    mudus.append(deliverer)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.loc[words.id_text == 'P102169']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
