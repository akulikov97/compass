{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veldhuis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from ipywidgets import interact\n",
    "import re\n",
    "import pickle\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the table with P numbers of tablets that belong to the Treasure Archive and Shoe Archive. Retrieve a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'csv/treasury_shoes2.txt'\n",
    "tr_df = pd.read_csv(file, encoding='utf8')\n",
    "ids = list(tr_df['id_text'])\n",
    "ids.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, download the Ur3 JSON from epsd2. If the file is already in the directory `/jsonzip`, skip this step, but do assign 'epsd2/admin/ur3' to the variable `project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving http://build-oracc.museum.upenn.edu/json/epsd2-admin-ur3.zip as jsonzip/epsd2-admin-ur3.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4e6619e8414cf6b3288e59753bbddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epsd2/admin/ur3', max=577877494.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project = 'epsd2/admin/ur3'\n",
    "oracc_download([project]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for parsing the JSON. Words that appear in year names are marked in the field `ftype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject)\n",
    "        if \"label\" in JSONobject:\n",
    "            meta_d[\"label\"] = JSONobject['label']\n",
    "        if \"f\" in JSONobject:\n",
    "            lemma = JSONobject[\"f\"]\n",
    "            if \"ftype\" in JSONobject:\n",
    "                lemma['ftype'] = JSONobject['ftype'] # this picks up YN for year name\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma['label'] = meta_d[\"label\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "        if \"strict\" in JSONobject and JSONobject[\"strict\"] == \"1\":\n",
    "            lemma = {key: JSONobject[key] for key in dollar_keys}\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma[\"id_text\"] = meta_d[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the parser for each of the P numbers in the list `ids`. The parser function needs the lists `lemm_l` and `dollar_keys`, as well as the dictionary `meta_d`. These objects are manipulated during the parsing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ee8deb3fd14b17a4123d3fea45d2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epsd2/admin/ur3', max=288.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsd2/admin/ur3/P143238 is not available or not complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemm_l = []\n",
    "meta_d = {\"label\": None, \"id_text\": None}\n",
    "dollar_keys = [\"extent\", \"scope\", \"state\"]\n",
    "file = f\"jsonzip/{project.replace('/', '-')}.zip\"\n",
    "try:\n",
    "    z = zipfile.ZipFile(file) \n",
    "except:\n",
    "    print(f\"{file} does not exist or is not a proper ZIP file\")\n",
    "files = z.namelist() # list of all the files in the ZIP file\n",
    "files = [name for name in files if name[-12:-5] in ids] # select only those of the treasury/leather archive\n",
    "for filename in tqdm(files, desc = project):\n",
    "    id_text = project + filename[-13:-5] \n",
    "    meta_d[\"id_text\"] = id_text\n",
    "    try:\n",
    "        st = z.read(filename).decode('utf-8')\n",
    "        data_json = json.loads(st)           \n",
    "        parsejson(data_json)\n",
    "    except:\n",
    "        print(f'{id_text} is not available or not complete')\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parsejson()` fills the lists `lemm_l` with data. Read this list into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(lemm_l).fillna('')\n",
    "keep = ['extent', 'scope', 'state', 'id_word', 'id_text', 'form', 'cf', 'gw', 'pos', 'ftype', 'label']\n",
    "words = words[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove comma's and spaces from Guide words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['gw'] = words['gw'].replace([' ', ','], ['', ''], regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify `id_text` ('P123456' instead of 'epsd2/admin/ur3/P123456') and add a field `id_line`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['id_text'] = [i[-7:] for i in words['id_text']]\n",
    "words['id_line'] = [int(i.split('.')[1]) for i in words['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extent</th>\n",
       "      <th>scope</th>\n",
       "      <th>state</th>\n",
       "      <th>id_word</th>\n",
       "      <th>id_text</th>\n",
       "      <th>form</th>\n",
       "      <th>cf</th>\n",
       "      <th>gw</th>\n",
       "      <th>pos</th>\n",
       "      <th>ftype</th>\n",
       "      <th>label</th>\n",
       "      <th>id_line</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n</td>\n",
       "      <td>line</td>\n",
       "      <td>missing</td>\n",
       "      <td>P104232.3</td>\n",
       "      <td>P104232</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>break_physical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>P104232.5.1</td>\n",
       "      <td>P104232</td>\n",
       "      <td>šu</td>\n",
       "      <td>šu</td>\n",
       "      <td>hand</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>r 1</td>\n",
       "      <td>5</td>\n",
       "      <td>šu[hand]N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>P104232.5.2</td>\n",
       "      <td>P104232</td>\n",
       "      <td>ba-ti</td>\n",
       "      <td>teŋ</td>\n",
       "      <td>near</td>\n",
       "      <td>V/i</td>\n",
       "      <td></td>\n",
       "      <td>r 1</td>\n",
       "      <td>5</td>\n",
       "      <td>teŋ[near]V/i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>P104232.6.1</td>\n",
       "      <td>P104232</td>\n",
       "      <td>ša₃</td>\n",
       "      <td>šag</td>\n",
       "      <td>heart</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>r 2</td>\n",
       "      <td>6</td>\n",
       "      <td>šag[heart]N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>P104232.6.2</td>\n",
       "      <td>P104232</td>\n",
       "      <td>puzur₄-iš-{d}da-gan</td>\n",
       "      <td>Puzrišdagan</td>\n",
       "      <td>1</td>\n",
       "      <td>SN</td>\n",
       "      <td></td>\n",
       "      <td>r 2</td>\n",
       "      <td>6</td>\n",
       "      <td>Puzrišdagan[1]SN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>P104232.7.1</td>\n",
       "      <td>P104232</td>\n",
       "      <td>iti</td>\n",
       "      <td>itud</td>\n",
       "      <td>moon</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>r 3</td>\n",
       "      <td>7</td>\n",
       "      <td>itud[moon]N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>P104232.7.2</td>\n",
       "      <td>P104232</td>\n",
       "      <td>ki-siki-{d}nin-a-zu</td>\n",
       "      <td>Kisikininazuk</td>\n",
       "      <td>1</td>\n",
       "      <td>MN</td>\n",
       "      <td></td>\n",
       "      <td>r 3</td>\n",
       "      <td>7</td>\n",
       "      <td>Kisikininazuk[1]MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>P104232.7.3</td>\n",
       "      <td>P104232</td>\n",
       "      <td>min-kam</td>\n",
       "      <td>min</td>\n",
       "      <td>two</td>\n",
       "      <td>NU</td>\n",
       "      <td></td>\n",
       "      <td>r 3</td>\n",
       "      <td>7</td>\n",
       "      <td>min[two]NU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>P104232.8.1</td>\n",
       "      <td>P104232</td>\n",
       "      <td>mu</td>\n",
       "      <td>mu</td>\n",
       "      <td>year</td>\n",
       "      <td>N</td>\n",
       "      <td>yn</td>\n",
       "      <td>r 4</td>\n",
       "      <td>8</td>\n",
       "      <td>mu[year]N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>P104232.8.2</td>\n",
       "      <td>P104232</td>\n",
       "      <td>si-mu-ru-um{ki}</td>\n",
       "      <td>Si.mu.ru.um</td>\n",
       "      <td>00</td>\n",
       "      <td>SN</td>\n",
       "      <td>yn</td>\n",
       "      <td>r 4</td>\n",
       "      <td>8</td>\n",
       "      <td>Si.mu.ru.um[00]SN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  extent scope    state      id_word  id_text                 form  \\\n",
       "0      n  line  missing    P104232.3  P104232                        \n",
       "1                        P104232.5.1  P104232                   šu   \n",
       "2                        P104232.5.2  P104232                ba-ti   \n",
       "3                        P104232.6.1  P104232                  ša₃   \n",
       "4                        P104232.6.2  P104232  puzur₄-iš-{d}da-gan   \n",
       "5                        P104232.7.1  P104232                  iti   \n",
       "6                        P104232.7.2  P104232  ki-siki-{d}nin-a-zu   \n",
       "7                        P104232.7.3  P104232              min-kam   \n",
       "8                        P104232.8.1  P104232                   mu   \n",
       "9                        P104232.8.2  P104232      si-mu-ru-um{ki}   \n",
       "\n",
       "              cf     gw  pos ftype label  id_line               lemma  \n",
       "0                                               3      break_physical  \n",
       "1             šu   hand    N         r 1        5           šu[hand]N  \n",
       "2            teŋ   near  V/i         r 1        5        teŋ[near]V/i  \n",
       "3            šag  heart    N         r 2        6         šag[heart]N  \n",
       "4    Puzrišdagan      1   SN         r 2        6    Puzrišdagan[1]SN  \n",
       "5           itud   moon    N         r 3        7         itud[moon]N  \n",
       "6  Kisikininazuk      1   MN         r 3        7  Kisikininazuk[1]MN  \n",
       "7            min    two   NU         r 3        7          min[two]NU  \n",
       "8             mu   year    N    yn   r 4        8           mu[year]N  \n",
       "9    Si.mu.ru.um     00   SN    yn   r 4        8   Si.mu.ru.um[00]SN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proper_nouns = ['RN', 'PN', 'DN', 'AN', 'WN', 'ON', 'TN', 'CN', 'GN', 'SN']\n",
    "physical_break = ['illegible', 'traces', 'missing', 'effaced']\n",
    "logical_break = ['other', 'blank', 'ruling']\n",
    "words['lemma'] = words[\"cf\"] + '[' + words[\"gw\"] + ']' + words[\"pos\"]\n",
    "words.loc[words[\"cf\"] == \"\" , 'lemma'] = words['form'] + '[NA]NA'\n",
    "words.loc[words[\"pos\"] == \"n\" , 'lemma'] = words['form'] + '[]NU'\n",
    "words.loc[words[\"state\"].isin(logical_break), 'lemma'] = \"break_logical\"\n",
    "words.loc[words[\"state\"].isin(physical_break), 'lemma'] = \"break_physical\"\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read list of name forms and Normalized names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transliteration</th>\n",
       "      <th>normalization</th>\n",
       "      <th>remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-AN-ba-az</td>\n",
       "      <td>A.AN.ba.az[]PN</td>\n",
       "      <td>MVN 13 464 r 10 (copy/photo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-KU-um</td>\n",
       "      <td>A.KU.um[]PN</td>\n",
       "      <td>Aegyptus 10, 270 27 o 7 (copy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-KU.KU-ta</td>\n",
       "      <td>A.KU.KU[]PN</td>\n",
       "      <td>AnOr 12 277 o i 17' (copy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-NI-ta</td>\n",
       "      <td>A.NI[]PN</td>\n",
       "      <td>Babyl. 7 pl. 22 18 o 3 (copy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-U.E₂-nu-tuku</td>\n",
       "      <td>A.U.E₂.nu.tuku[]PN</td>\n",
       "      <td>AnOr 07 150 o 2: A-U.KID-nu-tuku IŠ (copy/photo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>{d}utu</td>\n",
       "      <td>Utu[]DN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>{d}utu-bar-ra</td>\n",
       "      <td>Utubara[]DN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>Ma₂-gur₈-mah</td>\n",
       "      <td>Magurmah[]ON</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>Ma₂-dara₃-abzu</td>\n",
       "      <td>Madaraʾabzu[]ON</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5806</th>\n",
       "      <td>Lugal-uri₅{ki}-e-ki-ag₂</td>\n",
       "      <td>LugalUrimekiʾaŋ[]ON</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5807 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              transliteration         normalization  \\\n",
       "0                  A-AN-ba-az        A.AN.ba.az[]PN   \n",
       "1                     A-KU-um           A.KU.um[]PN   \n",
       "2                  A-KU.KU-ta           A.KU.KU[]PN   \n",
       "3                     A-NI-ta              A.NI[]PN   \n",
       "4              A-U.E₂-nu-tuku    A.U.E₂.nu.tuku[]PN   \n",
       "...                       ...                   ...   \n",
       "5802                   {d}utu               Utu[]DN   \n",
       "5803            {d}utu-bar-ra           Utubara[]DN   \n",
       "5804             Ma₂-gur₈-mah          Magurmah[]ON   \n",
       "5805           Ma₂-dara₃-abzu       Madaraʾabzu[]ON   \n",
       "5806  Lugal-uri₅{ki}-e-ki-ag₂  LugalUrimekiʾaŋ[]ON    \n",
       "\n",
       "                                               remarks  \n",
       "0                         MVN 13 464 r 10 (copy/photo)  \n",
       "1                       Aegyptus 10, 270 27 o 7 (copy)  \n",
       "2                           AnOr 12 277 o i 17' (copy)  \n",
       "3                        Babyl. 7 pl. 22 18 o 3 (copy)  \n",
       "4     AnOr 07 150 o 2: A-U.KID-nu-tuku IŠ (copy/photo)  \n",
       "...                                                ...  \n",
       "5802                                               NaN  \n",
       "5803                                               NaN  \n",
       "5804                                               NaN  \n",
       "5805                                               NaN  \n",
       "5806                                               NaN  \n",
       "\n",
       "[5807 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normdf = pd.read_csv('Normalized/drehem_norm_names.csv', encoding='utf8')\n",
    "normdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download OGSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving http://build-oracc.museum.upenn.edu/json/ogsl.zip as jsonzip/ogsl.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616d3ecd12e347c28e1c2ff7088d29e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='ogsl', max=208726.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oracc_download(['ogsl']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List Ur 3 sign equivalencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv = {'ANŠE' : 'GIR₃', \n",
    "        'DUR₂' : 'KU', \n",
    "        'NAM₂' : 'TUG₂', \n",
    "        'TIL' : 'BAD', \n",
    "        'NI₂' : 'IM',\n",
    "        'ŠAR₂' : 'HI', \n",
    "        }\n",
    "w = re.compile(r'\\w+') # replace whole words only - do not replace TILLA with BADLA.\n",
    "           # but do replace |SAL.ANŠE| with |SAL.GIR₃|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse OGSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseogsljson(data_json):\n",
    "    for key, value in data_json[\"signs\"].items():\n",
    "        key = re.sub(w, lambda m: equiv.get(m.group(), m.group()), key)\n",
    "        if \"values\" in value:\n",
    "            for n in value[\"values\"]:\n",
    "                d2[n] = key\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create OGSL dictionary key = sign value, value = sign name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d2 = {}  # this empty dictionary is filled by the parsejson() function, called in this cell.\n",
    "file = \"jsonzip/ogsl.zip\"\n",
    "z = zipfile.ZipFile(file) \n",
    "filename = \"ogsl/ogsl-sl.json\"\n",
    "signlist = z.read(filename).decode('utf-8')\n",
    "data_json = json.loads(signlist)                # make it into a json object (essentially a dictionary)\n",
    "parseogsljson(data_json)  \n",
    "with open('output/ogsl_dict.p', 'wb') as p:\n",
    "    pickle.dump(d2, p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = ['{', '}', '-']\n",
    "separators2 = ['.', '+', '|']  # used in compound signs\n",
    "#operators = ['&', '%', '@', '×']\n",
    "flags = \"][?<>⸢⸣⌈⌉*/\" # note that ! is omitted from flags, because it is dealt with separately\n",
    "table = str.maketrans(dict.fromkeys(flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signnames(translit):  \n",
    "    \"\"\"This function takes a string of transliterated cuneiform text and translates that string into a string of\n",
    "    sign names, separated by spaces. In order to work it needs the variables separators, separators2, and table defined above. The variable table\n",
    "    is used by the translate() method to translate all flags (except for !) to None. The function also needs a dictionary, called d2, that has as\n",
    "    keys sign readings and sign names as corresponding values. In case a key is not found, the sign reading is replaced by itself.\"\"\"\n",
    "    signnames_l = []\n",
    "    translit = translit.translate(table).lower()  # remove flags, half brackets, square brackets.\n",
    "    translit = translit.replace('...', 'x')\n",
    "    for s in separators: # split transliteration line into signs   \n",
    "        translit = translit.replace(s, ' ').strip()\n",
    "    s_l = translit.split() # s_l is a list that contains the sequence of transliterated signs without separators or flags\n",
    "    s_l = [d2.get(sign, sign) for sign in s_l] # replace each transliterated sign with its sign name.\n",
    "    # Now take care of some special situations: signs with qualifiers, compound signs.\n",
    "    for sign in s_l:\n",
    "        if '!' in sign: # corrected sign, as in ka!(SAG), get only the corrected reading.\n",
    "            sign = sign.split('!(')[0]\n",
    "            sign = sign.replace('!', '') # remove remaining exclamation marks\n",
    "        elif sign[-1] == ')' and '(' in sign: # qualified sign, as in ziₓ(SIG₇) - get only the qualifier\n",
    "            sign = sign.split('(')[1][:-1]\n",
    "        if '×' in sign: #compound. Compound like |KA×NINDA| to be replaced by |KA×GAR|\n",
    "            sign_l = sign.replace('|', '').split('×')\n",
    "            #replace individual signs of the compound by OGSL names\n",
    "            sign_l = [d2.get(sign, sign) for sign in sign_l] \n",
    "            # if user enters |KA*EŠ| this is transformed to ['KA', '|U.U.U|']. The pipes around U.U.U must be replaced by brackets\n",
    "            sign_l = [f'({sign[1:-1]})' if len(sign) > 1 and sign[0] == '|' else sign for sign in sign_l]\n",
    "            sign = f\"|{'×'.join(sign_l)}|\"  #put the sign together again with enclosing pipes.\n",
    "        elif '.' in sign or '+' in sign: # using elif, so that compounds like |UD×(U.U.U)| are not further analyzed.\n",
    "            for s in separators2:\n",
    "                sign = sign.replace(s, ' ').strip() \n",
    "            sign_l = sign.split()  # compound sign split into multiple signs\n",
    "            sign_l = [d2.get(sign, sign) for sign in sign_l]\n",
    "            for se in separators2:   # in case d2.get returns a compound sign name\n",
    "                sign_l = [si.replace(se, ' ').strip() for si in sign_l]\n",
    "            signnames_l.extend(sign_l)\n",
    "            continue\n",
    "        sign = d2.get(sign, sign)\n",
    "        signnames_l.append(sign)\n",
    "    # add space before and after each line so that each sign representation is enclosed in spaces\n",
    "    signnames = f\" {' '.join(signnames_l).upper()} \" \n",
    "    return signnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new field in the Normalized Names table, representing the sign sequence (sign names) of the transliteration of each name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc16ba1d32f4483188e9472a4522d07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5807.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "normdf[\"sign_names\"] = normdf[\"transliteration\"].progress_map(signnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with `sign_names` as key and `normalization` as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normd2 = dict(zip(normdf['sign_names'], normdf['normalization']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to `words` with the sequence of sign names for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02a4176095145f0881788b9242e7385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "words['sign_names'] = words['form'].progress_map(signnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `normd2` dictionary to transform sign name sequences into normalized names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0301b43afeb640348a4738e4bb90b909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#words.loc[(words.pos.isin(proper_nouns + ['X'])) & (words.lemma.str.contains('.')), \n",
    "#          'lemma'] = words.progress_apply(lambda x: normd2.get(x['sign_names'], x['lemma']), axis=1)\n",
    "words.loc[words.pos.isin(proper_nouns + ['X']), 'lemma'] = words.progress_apply(lambda x: normd2.get(x['sign_names'], x['lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the (temporary) corrections.csv file to apply corrections to the lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c3c83ab71a42d8ad79fe96bb9e9fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corrections = pd.read_csv('Normalized/corrections.csv', encoding='utf8')\n",
    "corr_d = dict(zip(corrections['form'], corrections['corr']))\n",
    "words.loc[words.pos.isin(['PN', 'RN', 'X', 'DN']), 'lemma'] = words.progress_apply(lambda x: corr_d.get(x['form'], x['lemma']), axis =1)\n",
    "words['pos'] = [w.split(']')[-1] if ']' in w else '' for w in words['lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttype = []\n",
    "for i in ids:\n",
    "    ttype = ''\n",
    "    text = list(words.loc[words.id_text == i, 'lemma'])\n",
    "    if 'mu.DU[delivery]N' in text:\n",
    "        ttype = 'intake'\n",
    "    elif 'zig[rise]V/i' in text:\n",
    "        ttype = 'expenditure'\n",
    "    elif 'teŋ[near]V/i' in text: \n",
    "        ttype = 'transfer'\n",
    "    texttype.append(ttype)\n",
    "treasure = dict(zip(ids, texttype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select PNs (and RNs) but skip those that appear in Year Names or Seals. Add lugal (king) sukkalmah (prime minister) and nin (queen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed = {'lugal[king]N', 'nin[queen]N', 'sukkalmah[official]N'}\n",
    "names = set(words.loc[words.pos.isin(['PN', 'RN', 'DN']), 'lemma'])\n",
    "actors = unnamed | names\n",
    "seal = list(words.loc[words.label.str.contains('seal'), 'id_word'])\n",
    "yn = list(words.loc[words.ftype == 'yn', 'id_word'])\n",
    "exclude = seal + yn\n",
    "PNs = words.loc[(words.lemma.isin(actors)) & (~words.id_word.isin(exclude))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define keywords and the corresponding roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_post = {'mu.DU[delivery]N': 'deliverer', 'maškim[administrator]N' : 'representative', \n",
    "               'zig[rise]V/i' : 'expender',\n",
    "               'šu[hand]N teŋ[near]V/i': 'recipient' , 'šu[hand]N us[follow]V/t': 'sender'}\n",
    "key_pre =  {'ŋiri[foot]N' : 'intermediary', \n",
    "            'arua[offering]N' : 'offerer', 'kišib[seal]N' : 'sealer', \n",
    "            'gabari[copy]N kišib[seal]N' : 'sealer', 'mu.DU[delivery]N' : 'deliverer',\n",
    "            'mu[name]N' : 'reason', 'ki[place]N' : 'source', 'kiŋ[work]N ak[do]N' : 'producer'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the Role of each PN\n",
    "TODO: default role in texttype transfer. Check default roles for other text types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = []\n",
    "attribute = []\n",
    "for i in PNs.index:                           # the index of PNs is identical to the one of words\n",
    "    Pno = PNs.loc[i]['id_text']                    # the text ID (P number)\n",
    "    lineno = PNs.loc[i]['id_line']                 # the line number inwhich the PN appears\n",
    "    text = words.loc[words.id_text == Pno]                # the entire text\n",
    "    line = text.loc[text.id_line == lineno] #  the line with the PN\n",
    "    mnw = line.index[0]                 # index no of firstword in line\n",
    "    mxw = line.index[-1]                   # index no. of last word in line\n",
    "    mx = text.index[-1]                    # index no. of last word in text\n",
    "    mxl = text.loc[mx]['id_line']          # highest line number in text\n",
    "    if lineno < mxl:                       # check that nextline is still in the same text\n",
    "        nextlno = words.loc[mxw+1]['id_line']\n",
    "        nextline = text.loc[text.id_line == nextlno] # words in nextline\n",
    "    else:\n",
    "        nextline = line\n",
    "    attr = []\n",
    "    r = ''\n",
    "    if i < mxw:\n",
    "        attr = list(line.loc[i+1:]['lemma'])\n",
    "    if mnw == i:               # PN in first position\n",
    "        r = key_post.get(line.loc[mxw]['lemma'], r)    # keyword appears in same line in last position\n",
    "        r = key_post.get(words.loc[mxw+1]['lemma'], r) # or in the next line in first position\n",
    "        if len(line) > 1:                   # additional words are attributes of the name\n",
    "            lastwords = ' '.join(list(line[-2:]['lemma'])) # two-word keywords (as in šu ba-ti)\n",
    "            r = key_post.get(lastwords, r)\n",
    "        if len(nextline) > 1:               # two-word keyword appearing in first position in next line\n",
    "            firstwords = ' '.join(list(nextline[:2]['lemma']))\n",
    "            r = key_post.get(firstwords, r)\n",
    "            \n",
    "    elif i > mnw:             # PN appears further in line with keyword(s) preceding\n",
    "        firstwords = ' '.join(list(line.loc[mnw:i-1]['lemma']))  # join all words before PN\n",
    "        r = key_pre.get(firstwords, r)        # ŋiri₃ PN; ki PN-ta; kin ak PN; etc\n",
    "        if line.loc[mnw]['lemma'] == 'ki[place]N' and line.loc[mxw]['form'].endswith('-še₃'): \n",
    "            r = 'destination'              # special case: ki PN-še₃         \n",
    "        if r == '':                # role has not been filled yet\n",
    "            PN = [w for w in line.loc[mnw:i-1]['lemma'] if w in list(PNs['lemma'])] # is there another PN previously in the line?\n",
    "            if PN:\n",
    "                if words.loc[i-1]['lemma'] == 'u[and]CNJ':\n",
    "                    r = role[-1]               # same role as previous PN\n",
    "                else:\n",
    "                    r = 'relation'             # as in PN dumu PN\n",
    "            elif treasure.get(Pno) == 'expenditure': # commodities + PN: recipient\n",
    "                r = 'recipient'\n",
    "            else:                              # if there is no preceding PN: drop the name\n",
    "                r = 'none'                     # this may  need refinement\n",
    "        if PNs.loc[i]['lemma'] in unnamed and words.loc[i-1]['lemma'] in list(PNs['lemma']):  \n",
    "            r = 'none'       # 'unnamed' person is preceded byname\n",
    "    if r == '' :\n",
    "        if treasure.get(Pno) == 'expenditure': # default role for zi-ga/ba-zi texts\n",
    "            r = 'recipient'\n",
    "        elif treasure.get(Pno) == 'intake' or treasure.get(Pno) == 'transfer':    # default role for mu-DU texts\n",
    "            r = 'source'\n",
    "    role.append(r)\n",
    "    attribute.append(' '.join(attr))\n",
    "PNs['role'] = role\n",
    "PNs['attribute'] = attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results in a table with links for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://build-oracc.museum.upenn.edu/epsd2/admin/ur3/{}\", target=\"_blank\">{}</a>'\n",
    "PNs2 = PNs.copy()\n",
    "PNs2['id_word'] = [anchor.format(val,val) for val in PNs['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b80a2a9664477696ae5c1afece8a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=25, description='rows', max=1064, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(rows = (1, len(PNs2), 1))\n",
    "def showpns(rows = 25): \n",
    "    return PNs2.loc[PNs2.id_text == 'P103763', ['id_word', 'form', 'pos', 'lemma', 'role', 'attribute']][:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Edges\n",
    "Needs checking. Deal with roles 'offerer' and 'reason'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake = [i for i in ids if treasure.get(i) == 'intake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for i in ids:                              # one text ID at a time; muDU texts\n",
    "    people = PNs.index[PNs.id_text == i]    # indexes of all the people in that text\n",
    "    for p in people:                       # iterate over those indexes\n",
    "        source = ''\n",
    "        target = ''\n",
    "        role = PNs.loc[p]['role']\n",
    "        if role in ['intermediary', 'representative']:\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in people if n > p and PNs.loc[n]['role'] in ['recipient', 'sealer']]  # look for target after the source\n",
    "            if q:\n",
    "                target = PNs.loc[q[0]]['lemma']\n",
    "        elif role in ['sender', 'deliverer', 'source']:\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in people if n > p and PNs.loc[n]['role'] in ['recipient', 'sealer', 'intermediary']] \n",
    "            if q:\n",
    "                target = PNs.loc[q[0]]['lemma']\n",
    "        elif role == 'relation':\n",
    "            source = PNs.loc[p]['lemma']\n",
    "            q = [n for n in people if n < p]\n",
    "            if q:\n",
    "                target = PNs.loc[q[-1]]['lemma']\n",
    "        elif role in ['recipient', 'sealer']:\n",
    "            target = PNs.loc[p]['lemma']\n",
    "            q = [n for n in people if n > p and PNs.loc[n]['role'] in ['representative', 'source', 'intermediary']]\n",
    "            if q:\n",
    "                source = PNs.loc[q[0]]['lemma']\n",
    "        if source and target:\n",
    "            edges.append([source, target, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgs = pd.DataFrame(edges)\n",
    "edgs.columns = ['source', 'target', 'id_text']\n",
    "anchor = '<a href=\"http://build-oracc.museum.upenn.edu/epsd2/admin/ur3/{}\", target=\"_blank\">{}</a>'\n",
    "edgs2 = edgs.copy()\n",
    "edgs2['id_text'] = [anchor.format(val,val) for val in edgs['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb8ce5a2b15461e824f44f2d799a51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=25, description='rows', max=464, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(rows = (1, len(edgs2), 1))\n",
    "def showedges(rows = 25): \n",
    "    return edgs2.loc[edgs2.duplicated()][:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: remove duplicates; make multiple edges between the same people into weights; make into directed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e035b1b0382a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.convert_matrix.from_pandas_edgelist(edgs, 'source', 'target', ['id_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G)\n",
    "plt.figure(3,figsize=(12,12)) \n",
    "nx.draw(G,pos, node_size=30, node_color = 'b', edge_color='r', with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\veldhuis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib-3.1.3-py3.7-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\veldhuis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n",
      "Error processing line 1 of C:\\Users\\veldhuis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib-3.1.3-py3.7-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\veldhuis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n"
     ]
    }
   ],
   "source": [
    "%conda install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
