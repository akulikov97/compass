# 1. Introduction

This book project attempts to bring together two recent trends: the digital turn in Assyriology, and the rise of Data Science. Although Assyriologists had actively used databases since the nineteen sixties, such data sets were available only to a small circle - important examples include Miguel Civil's Nippur Corpus (a group of Sumerian literary texts, now largely available on [ETCSL][ETCSL]) and Simo Parpola's database of Neo-Assyrian texts, subsequently published in the State Archive of Assyria series and now online avaialble in [SAAo](http://oracc.org/saao) . With the advent of the Internet, it became possible to give public access to data in the form of transliterations, translations, photographs, and glossaries. This development began in earnest in 1996 with the appearance of the Electronic Text Corpus of Sumerian Literature ([ETCSL][ETCSL]), and continues to the present day. Data Science is an interdisciplinary field that draws primarily from Statistics and Computer Science and usually involves a third field, the domain discipline. Although "Data Science" is a relatively recent coinage, its roots go back well into the twentieth century. In recent years many universities have created institutes or departments for Data Science and/or for Digital Humanities, and have developed undergraduate or graduate programs in such fields. The eco-system for applying Data Science methods to Assyriological data, therefore, is much better today than it was even five years ago. 

This Introduction will briefly discuss the history of the digital turn in Assyriology and some relevant aspects of developments in Data Science (1.1). Next we will discuss software and installing software (1.2), with a brief discussion of differences between Windows and Mac. Section 1.3 is devoted to principles of Data Science in Assyriology: open data, open source, and reproducibility.

## 1.1 Assyriology and Data Science

### 1.1.1 Digital Assyriology

The turn towards publicly available electronic data is due in no small part to the initiative by Jeremy Black to develop the Electronic Text Corpus of Sumerian Literature ([ETCSL][ETCSL]), which started in 1996 and remained active until 2006, when it became archival. Initially [ETCSL][ETCSL] offered composite editions (transliterations) of Sumerian literary texts with translations. In version 2, the entire corpus was lemmatized, which allowed for the addition of glossaries and other tools for search and research.  

[ETCSL][ETCSL] was quickly followed by the Cuneiform Digital Library Inititative ([CDLI][CDLI]), created by Bob Englund, UCLA. This project provides access to metadata, photographs, line drawings, and transliterations (occasionally also translations) of cuneiform documents of all periods and genres. Importantly, [CDLI][CDLI] assigns unique ID numbers to cuneiform objects. Historically, [CDLI][CDLI] focused on administrative and legal documents from the fourth and third millennium BCE, but today one may a broad variety of text genres. In 2006 (after several precursors) the Open Richly Annotated Cuneiform Corpus ([ORACC][ORACC]) conglomerate of projects was built by Steve Tinney (University of Pennsylvania), who was also involved in the development of both [ETCSL][ETCSL] and [CDLI][CDLI]. In a sense [ORACC][ORACC] is an extension of [epsd][epsd], which is the electronic successor to the Pennsylvania Sumerian Dictionary. [ORACC][ORACC] works with semi-independent projects, where project directors have broad leeway in the definition of the scope of their project, but follow editorial principles in terms of transliteration and lemmatization.

These three projects together fundamentally changed research and teaching in Assyriology and many Assyriologists today depend in one way or another on these and other digital resources. Together, these three make available large amounts of searchable data and make those data freely accessible to other scholars. All three projects use explicit standards, and reuse data where possible, setting a pretty high standard for digital Assyriology. Many other larger and smaller projects were created in their wake, among the most important are the Database of Neo-Sumerian Texts ([BDTNS][BDTNS]; currently comprising almost 100,000 documents in transliteration); Sources of Early Akkadian Literature ([SEAL][SEAL]; several hundred literary texts in Akkadian from the third and second millennium BCE) and Archives Babyloniennes ([ARCHIBAB][ARCHIBAB]; a collection of thousands of Old Babylonian letters, and legal, and administrative documents)[^1].  The [BDTNS][BDTNS] data set is freely available (transliterations and metadata) and are of a high quality (often improving upon the editions available in [CDLI][CDLI]). [ARCHIBAB][ARCHIBAB] and [SEAL][SEAL] both make their data available in the form of PDFs and restrict usage to non derivatives, making the data of these projects unavailable for computational analysis.

### 1.1.2 Data Science

Data Science developed in response to the quantitative explosion in data collected and produced by cell phones, usage of online services, and web-connected utilities. Most relevant for the current project are  developments in Natural Language Processing (NLP), software tools, and institutional eco-systems. Natural Language Processing, which may be considered one branch of Data Science, has taken advantage of the huge amounts of textual data available on the web - either originally produced for the web, or in the form of scanned documents. Not less important were developments in computer language recognition (speech recognition and Optical Character Recognition). Search engines need efficient ways to determine which pages are likely to be relevant in response to a user's search entry. This led, for example, to concepts such as [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Text Frequency - Inverse Document Frequency), a family of statistical measures that indicate the importance of a word (or token) in a particular document, expressed by a number between 0 and 1. The token "constitution, " for example, will have a  value close to 1 in an article on the constitution, whereas the same word will have a value close to 0 in a culinary recipe. TF-IDF is now widely used not only in search engines, but in all kinds of research projects that deal with natural language. Similarly, research teams at Google ([GloVe](https://nlp.stanford.edu/projects/glove/)) and Facebook ([fastText](https://fasttext.cc/)) have contributed significantly to the development of so-called word-embeddings (word vectors), which allow one to express the semantic distance between two words as a vector or as a number between 0 and 1. Word embeddings, which are based on Neural Network architecture, are now routinely used in a wide variety of NLP tasks. Typically, such developments are initially driven by commercial interests, but yield tools that are useful for research purposes in a wide variety of disciplines.  

On the software side, the introduction of the [Jupyter Notebook](jupyter) (developed by Fernando Pérez, UC Berkeley) fundamentally changed research and teaching in Data Science. [Jupyter][jupyter] is an application that allows one to run Python (or one of many other programming languages) in a local page of a web browser. The [Jupyter Notebook][jupyter] mixes interactive code with narrative text, and shows the results of its computations (including visualizations) on the same page. [Jupyter Notebooks][jupyter] can easily be transformed into HTML for web publication (where they can be displayed for explanatory purposes, but cannot be run) and can be rendered by [Github][git] (a popular software development platform). All the coding for the present project is done in [Jupyter Notebooks][Jupyter] and made available on the Github pages of [CompAss][compass]. 

In terms of institutional embedding, many universities now have an institute for Data Science, a Department of Digital Humanities, or some type of technical support or training for humanities and social science scholars. Although the specifics are very different from one university to the next, such initiatives may create exciting venues for interaction between disciplines, working on very different data with similar computational tools. Where such programs exist students (graduate and undergraduate) may need real-world research projects where they can display and apply their (often very considerable) data-analytic skills.

Taken together, the developments in (digital) Assyriology and in Data Science provide an exciting opportunity and form the background against which this project develops. Initially, Assyriological web projects were used almost exclusively as a cheap and convenient alternative to book publications. With input from Data Science, they also enable the search for patterns (or latent variables, in Data Science speak), not immediately visible to the naked eye. Few projects so far have attempted to do so. Saana Svärd and her team (Helsinki University) have worked on word embeddings, using Akkadian data derived from [ORACC][ORACC]. The Machine Translation and Automated Analysis of Cuneiform Languages ([MTAAC][MTAAC]) team (including Emilie Pagé-Peron, Toronto and Bob Englund, UCLA) has received a major grant (2017) to train a neural network in order to translate the 67,000 Ur III documents now available on [CDLI][CDLI]. Both projects presented initial results in V B Jouloux , A R Gansell & A di Ludovico (eds) , [*CyberResearch on the Ancient Near East and Neighboring Regions*](https://doi.org/10.1163/9789004375086) Brill , Leiden 2018.

### 1.1.3 CompAss

The present project, [Computational Assyriology][compass],  is intended as an introduction to some of the things one can do computationally with cuneiform text data. Three projects of increasing complexity will be discussed.

Chapter 3 will ask: what is the overlap between the Sumerian vocabulary of Old Babylonian (ca. 18,00 BCE) lexical texts and the corpus of contemporary literary texts. Old Babylonian lexical texts were used to introduce scribal pupils into reading and writing Sumerian (a dead language by this time). In a more advanced stage of their education, pupils started to copy Sumerian literary texts. It stands to reason, therefore, to see the lexical corpus as sort of a dictionary or concordance, that might have helped pupils to master the literary material. It has long been known, however, that the relationship between literary and lexical vocabulary is not that straightforward - is it possible to express that computationally? And once we are at it, can we dig deeper and see which lexical texts and which literary texts contribute particularly to the overlap - or to the lack of overlap?

Chapter 4 will focus on a group of letters in Akkadian from the late eight century BCE, addressed to Sargon. These letters never explicitly mention the king by name, addressing him as "the king my lord". That this king is, in fact,  Sargon II (reigned from 722-705 BCE), and not some other Neo-Assyrian king before or after him, was decided by the modern editor of these letters (Simo Parpola) and has since been accepted by the research community. We will use a network approach to see if we can establish a first or second degree connection between the authors of these letters (who are known by name) and a brief list of contemporaries of Sargon (high officials in his kingdom). This is hardly a new idea - it is an approach that was no doubt utilized by Parpola when he classified the letters. What is new is the ease by which we can do this, pulling Proper Nouns out of our dataset, feeding them into a Social Network Analysis package and test first, second, and third degree relationships. One does not need to be a corpus specialist anymore to start approaching this question.

Chapter 5 will discuss word embeddings for Sumerian. Various types of word embeddings and various ways of representing Sumerian text will be explored. We will use word embeddings to explore the semantics of words for animals.

Chapter 6, finally, by way of conclusion will reflect on the advantages and disadvantages of a computational approach to Assyriology.

Before any of this can be done, however, we need to gain access to data. Chapter 2, therefore, deals with data acquistion, with separate discussions of the various online projects ([ORACC][ORACC], [ETCSL][ETCSL], [BDTNS][BDTNS], and[CDLI][CDLI]).

Each of the chapters will discuss and explain code. More detailed explanations of the code will be found in the [Jupyter Notebooks][Jupyter] that accompany each chapter and that are available on the [CompAss][compass] github pages. Chapter 2 (Data Acquisition) is more technical than any of the other chapters. Data acquisition data formatting and data cleaning are fundamental to any computational project and often take considerably more time than the data analysis. The goal of Chapter 2 (and the accompanying notebook) is not just to prpeare the data for the chapters 3-5, but also to provide the reader with technical means for exploring her own research questions. For an initial exploration of this book one may well skip Chapter 2 and go along with the analyses in the Chapters 3-5. In order to devise your own project, it will be necessary to gain a deeper understanding of how data is acquired and formatted and the (many) options that you have.

This study, finally, is not an introduction to Python. The chapters and notebooks contain code explanation, but for the fundamentals one would need to consult one of the many excellent introductions to Python, to scripting languages in general, or to data science that are available on the Web or in paper.

## 1.2 The Software

### 1.2.1 Anaconda, Python, and Jupyter

In order to run the scripts in the chapters 2-5 one needs to install Python and Jupyter and download the scripts from [Github][git]. The easiest way to do so is by installing the [Anaconda Distribution][anaconda], a data science platform that includes Jupyter, Python, and a host of Python libraries. It is important to choose the [Anaconda][anaconda] version with Python 3.7 or higher (if you happen to have 3.5, that should work, too). [Anaconda][anaconda] is available for Windows, Mac OS X and Linux. Once [Anaconda][anaconda] is installed one can open it and launch [Jupyter][jupyter] from within Anaconda Navigator. This will open a local web page that displays the current directory. It is often more practical to open [Jupyter][jupyter] from the console (see below). 

In order to download the scripts go to http://github.com/niekveldhuis/compass and click the green "Clone or Download" button. Now click "Download Zip" to acquire all the files that belong to the [CompAss][compass] project. Unzip the files in a convenient place.

To open the scripts, open the Command Prompt (Windows) or Terminal (OS X) and type `jupyter notebook`. This will open a local web page with the current directory. Navigate to the place where the [CompAss][compass] files are located and open a notebook (extension `.ipynb`) by clicking on it.

For more details see the instructions for [installing](http://jupyter.org/install) and [running](https://jupyter.readthedocs.io/en/latest/running.html) notebooks on the [Jupyter][jupyter] web site.

### 1.2.2 Additional Python Libraries

Python libraries (also called modules or packages) are extensions of the Python core that provide useful functionality. A library needs to be *installed* once (after which it may be occasionally updated) but must be *imported* each time a script that uses the library is run. Each script, therefore, starts with a number of import statements that look like: 

```python
import requests   # a library for communicating with servers over the internet
import pandas as pd # data analytics
from gensim.models.fasttext import FastText as FT_gensim # word embeddings
```
Installing packages can be challenging, in particular if your computer happens to have multiple instances of Python (which is not uncommon). Luckily, many important libraries belong to the [Anaconda Distribution][anaconda] and are properly installed with [Anaconda][anaconda]. Installing additional libraries can be done from within a [Jupyter][jupyter] notebook with the command

```python
! pip install [package name]
```
However, this may not work properly and when you try to `import`the package you may get an error. The notebook `install_packages.ipynb` in the directory `1_Preliminaries` of [CompAss][compass]  provides a more robust way of installing libraries, based on a [blog](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/) by Jake VanderPlas.

### 1.2.3 Windows vs. Mac OS X: Unicode and UTF-8

[Jupyter][jupyter] notebooks and Python are largely platform independent and the notebooks in this project are tested for both Windows and Mac OS X. There is one issue that one may encounter with some frequency and that is in reading and writing files.  Python 3 by default stores any string as Unicode, using the UTF-8 encoding (an encoding is a way to represent a Unicode code point as a set of bytes in memory). Internally, however, Windows uses another kind of encoding, which means that in opening and writing files the option `encoding = "utf-8"` needs to be added explicitly. The option is superfluous in Mac OS X (or in Unix) where `utf-8` is the standard, but is added in the notebooks at every place appropriate to ensure interoperability, as in the following examples:  

```python
with open("equivalencies/equivalencies.json", encoding="utf-8") as f:
    eq = json.load(f)
with open('output/alltexts.csv', 'w', encoding="utf-8") as w:
    df.to_csv(w, index=False)
```

In developing your own code, it is advisable to do so (even if things work fine without the `encoding` option on your computer), to ensure that your code will run for others, too. In writing to `.csv` files that are meant to be read in Excel, it may be advisable to encode in `utf-16`. A file encoded in `utf-8`cannot be read directly into Excel (it must be imported) or else all special characters will be scrambled.  In the present project all files are read by Python libraries and all encoding is done in `utf-8`.



## 1.3 Principles of Compotational Assyriology

Talk about reproducibility and its problems. Check  [Justin Kitzes](https://bids.berkeley.edu/people/justin-kitzes), [Daniel Turek](https://bids.berkeley.edu/people/daniel-turek), [Fatma Deniz](https://bids.berkeley.edu/people/fatma-imamoglu) (eds); *The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences* 2017. [online version](https://www.practicereproducibleresearch.org/)




[^1]: [D. Charpin, *Bibliotheca Orientalis* 71, 331-357](http://doi.org/10.2143/BIOR.71.3.3062115 ) (open access).



[Anaconda]: http://www.anaconda.com
[ARCGHIBAB]: http://www.archibab.fr
[CompAss]: http://github.com/niekveldhuis/compass
[ETCSL]: http://etcsl.orinst.ox.ac.uk
[CDLI]: http://cdli.ucla.edu
[MTAAC]: https://cdli-gh.github.io/mtaac/
[ORACC]: http://oracc.org
[BDTNS]: http://bdtns.filol.csic.es/
[SEAL]: https://www.seal.uni-leipzig.de/
[epsd]: http://psd.museum.upenn.edu/epsd1/index.html
[epsd2]: http://oracc.org/epsd2
[Jupyter]: http://jupyter.org
[git]: http://github.com
```

```