# 1. Introduction

This book project attempts to bring together two recent trends: the digital turn in Assyriology, and the rise of Data Science. Although Assyriologists had actively used databases since the nineteen sixties, such data sets were available only to a small circle - important examples include Miguel Civil's Nippur Corpus (a group of Sumerian literary texts, now largely available on [ETCSL][ETCSL]) and Simo Parpola's database of Neo-Assyrian texts, subsequently published in the State Archive of Assyria series and now online avaialble in [SAAo](http://oracc.org/saao) . With the advent of the Internet, it became possible to give public access to data in the form of transliterations, translations, photographs, and glossaries. This development began in earnest in 1996 with the appearance of the Electronic Text Corpus of Sumerian Literature ([ETCSL][ETCSL]), and continues to the present day. Data Science is an interdisciplinary field that draws primarily from Statistics and Computer Science and usually involves a third field, the domain discipline. Although "Data Science" is a relatively recent coinage, its roots go back well into the twentieth century. In recent years many universities have created institutes or departments for Data Science and/or for Digital Humanities, and have developed undergraduate or graduate programs in such fields. The eco-system for applying Data Science methods to Assyriological data, therefore, is much better today than it was even five years ago. 

This Introduction will briefly discuss the history of the digital turn in Assyriology and some relevant aspects of developments in Data Science (1.1). Next we will discuss software and installing software (1.2), with a brief discussion of differences between Windows and Mac. Section 1.3 is devoted to principles of Data Science in Assyriology: open data, open source, and reproducibility.

## 1.1 Assyriology and Data Science

The turn towards publicly available electronic data is due in no small part to the initiative by Jeremy Black to develop the Electronic Text Corpus of Sumerian Literature ([ETCSL][ETCSL]), which started in 1996 and remained active until 2006, when it became archival. Initially [ETCSL][ETCSL] offered composite editions (transliterations) of Sumerian literary texts with translations. In version 2, the entire corpus was lemmatized, which allowed for the addition of glossaries and other tools for search and research.  

[ETCSL][ETCSL] was quickly followed by the Cuneiform Digital Library Inititative ([CDLI][CDLI]), created by Bob Englund, UCLA. This project provides access to metadata, photographs, line drawings, and transliterations (occasionally also translations) of cuneiform documents of all periods and genres. Importantly, [CDLI][CDLI] assigns unique ID numbers to cuneiform objects. Historically, [CDLI][CDLI] focused on administrative and legal documents from the fourth and third millennium BCE, but today one may a broad variety of text genres. In 2006 (after several precursors) the Open Richly Annotated Cuneiform Corpus ([ORACC][ORACC]) conglomerate of projects was built by Steve Tinney (University of Pennsylvania), who was also involved in the development of both [ETCSL][ETCSL] and [CDLI][CDLI]. In a sense [ORACC][ORACC] is an extension of [epsd][epsd], which is the electronic successor to the Pennsylvania Sumerian Dictionary. [ORACC][ORACC] works with semi-independent projects, where project directors have broad leeway in the definition of the scope of their project, but follow editorial principles in terms of transliteration and lemmatization.

These three projects together fundamentally changed research and teaching in Assyriology and many Assyriologists today depend in one way or another on these and other digital resources. Together, these three make available large amounts of searchable data and make those data freely accessible to other scholars. All three projects use explicit standards, and reuse data where possible, setting a pretty high standard for digital Assyriology. Many other larger and smaller projects were created in their wake, among the most important are the Database of Neo-Sumerian Texts ([BDTNS][BDTNS]; currently comprising almost 100,000 documents in transliteration); Sources of Early Akkadian Literature ([SEAL][SEAL]; several hundred literary texts in Akkadian from the third and second millennium BCE) and Archives Babyloniennes ([ARCHIBAB][ARCHIBAB]; a collection of thousands of Old Babylonian letters, and legal, and administrative documents)[^1].  The [BDTNS][BDTNS] data set is freely available (transliterations and metadata) and are of a high quality (often improving upon the editions available in [CDLI][CDLI]). [ARCHIBAB][ARCHIBAB] and [SEAL][SEAL] both make their data available in the form of PDFs and restrict usage to non derivatives, making the data of these projects unavailable for computational analysis.

Data Science developed in response to the quantitative explosion in data collected and produced by cell phones, usage of online services, and web-connected utilities. Most relevant for the current project are  developments in Natural Language Processing (NLP), software tools, and institutional eco-systems. Natural Language Processing, which may be considered one branch of Data Science, has taken advantage of the huge amounts of textual data available on the web - either originally produced for the web, or in the form of scanned documents. Not less important were developments in computer language recognition (speech recognition and Optical Character Recognition). Search engines need efficient ways to determine which pages are likely to be relevant in response to a user's search entry. This led, for example, to concepts such as [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Text Frequency - Inverse Document Frequency), a family of statistical measures that indicate the importance of a word (or token) in a particular document, expressed by a number between 0 and 1. The token "constitution, " for example, will have a  value close to 1 in an article on the constitution, whereas the same word will have a value close to 0 in a culinary recipe. TF-IDF is now widely used not only in search engines, but in all kinds of research projects that deal with natural language. Similarly, research teams at Google ([GloVe](https://nlp.stanford.edu/projects/glove/)) and Facebook ([fastText](https://fasttext.cc/)) have contributed significantly to the development of so-called word-embeddings (word vectors), which allow one to express the semantic distance between two words as a vector or as a number between 0 and 1. Word embeddings, which are based on Neural Network architecture, are now routinely used in a wide variety of NLP tasks. Typically, such developments are initially driven by commercial interests, but yield tools that are useful for research purposes in a wide variety of disciplines.  

On the software side, the introduction of the [Jupyter Notebook](jupyter) (developed by Fernando Pérez, UC Berkeley) fundamentally changed research and teaching in Data Science. [Jupyter][jupyter] is an application that allows one to run Python (or one of many other programming languages) in a local page of a web browser. The [Jupyter Notebook][jupyter] mixes interactive code with narrative text, and shows the results of its computations (including visualizations) on the same page. [Jupyter Notebooks][jupyter] can easily be transformed into HTML for web publication (where they can be displayed for explanatory purposes, but cannot be run) and can be rendered by [Github][git] (a popular software development platform). All the coding for the present project is done in [Jupyter Notebooks][Jupyter] and made available on the Github pages of [CompAss][compass]. 

In terms of institutional embedding, many universities now have an institute for Data Science, a Department of Digital Humanities, or some type of technical support or training for humanities and social science scholars. Although the specifics are very different from one university to the next, such initiatives may create exciting venues for interaction between disciplines, working on very different data with similar computational tools. Where such programs exist students (graduate and undergraduate) may need real-world research projects where they can display and apply their (often very considerable) data-analytic skills.

Taken together, the developments in (digital) Assyriology and in Data Science provide an exciting opportunity and form the background against which this project develops. Initially, Assyriological web projects were used almost exclusively as a cheap and convenient alternative to book publications. With input from Data Science, they also enable the search for patterns (or latent variables, in Data Science speak), not immediately visible to the naked eye. Few projects so far have attempted to do so. Saana Svärd and her team (Helsinki University) have worked on word embeddings, using Akkadian data derived from [ORACC][ORACC]. The Machine Translation and Automated Analysis of Cuneiform Languages ([MTAAC][MTAAC]) team (including Emilie Pagé-Peron, Toronto and Bob Englund, UCLA) has received a major grant (2017) to train a neural network in order to translate the 67,000 Ur III documents now available on [CDLI][CDLI]. Both projects presented initial results in V B Jouloux , A R Gansell & A di Ludovico (eds) , [*CyberResearch on the Ancient Near East and Neighboring Regions*](https://doi.org/10.1163/9789004375086) Brill , Leiden 2018.

The present project is intended as an introduction to some of the things one can do computationally with cuneiform text data. Three projects of increasing complexity will be discussed. Chapter 3 will ask: what is the overlap between the Sumerian vocabulary of Old Babylonian lexical texts and the corpus of contemporary literary texts. We know that lexical texts were used to introduce scribal pupils into reading and writing Sumerian (a dead language by this time, ca 1,800 BCE). In a secondary stage, these pupils started to copy (Sumerian) literary texts. It stands to reason, therefore, to see the lexical corpus as  sort of a dictionary or concordance, that might help pupils to master the literary material. It has long been known, however, that the relationship between literary and lexical vocabulary is not that straightforward - is it possible to express that computationally? And once we are at it, can we dig deeper and see which lexical texts and which literary texts contribute particularly to the overlap - or the lack of overlap.

Chapter 4 will focus on a group of letters in Akkadian from the late eight century BCE, addressed to Sargon. The letters, however, never explicitly mention the king by name - they will say "To the king my lord". That this king is, in fact,  Sargon II (reigned from 722-705 BCE), and some other Neo-Assyrian king before or after him, was decided by the editor of these letters and has since been accepted by the research community. We will use a network approach to see if we can establish a first or second degree connection between the authors of these letters (who are known by name) and a brief list of contemporaries of Sargon (high officials in his kingdom). This is hardly a new idea - it is an approach that was no doubt utilized by the corpus specialist who published the letters. What is new is the ease by which we can do this, pulling Proper Nouns out of our dataset, feeding them into a Social Network Analysis package and test first, second, and third degree relationships.

[^1]: [D. Charpin, *Bibliotheca Orientalis* 71, 331-357](http://doi.org/10.2143/BIOR.71.3.3062115 ) (open access).



[ARCGHIBAB]: http://www.archibab.fr
[CompAss]: http://github.com/niekveldhuis/compass
[ETCSL]: http://etcsl.orinst.ox.ac.uk
[CDLI]: http://cdli.ucla.edu
[MTAAC]: https://cdli-gh.github.io/mtaac/
[ORACC]: http://oracc.org
[BDTNS]: http://bdtns.filol.csic.es/
[SEAL]: https://www.seal.uni-leipzig.de/
[epsd]: http://psd.museum.upenn.edu/epsd1/index.html
[epsd2]: http://oracc.org/epsd2
[Jupyter]: http://jupyter.org
[git]: http://github.com