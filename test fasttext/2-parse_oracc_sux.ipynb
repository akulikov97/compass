{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Represent ORACC Texts in UTF-8 Cuneiform\n",
    "The code in this notebook will parse [ORACC](http://oracc.org) `JSON` files to extract signs from the Sumerian texts of one or more projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Create Directories, if Necessary\n",
    "The two directories needed for this script are `jsonzip` and `output`. If they do not exist they are created, else: do nothing.\n",
    "\n",
    "For the code, see [Stack Overflow](http://stackoverflow.com/questions/18973418/os-mkdirpath-returns-oserror-when-directory-does-not-exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['jsonzip', 'output', 'corpus']\n",
    "make_dirs(directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Input Project Names\n",
    "Provide a list of one or more project names, separated by commas. Note that subprojects must be listed separately, they are not included in the main project. For instance:\n",
    "\n",
    "`epsd2/admin/ed3a, epsd2/admin/ed3b, epsd2/admin/ebla, epsd2/admin/oakk, epsd2/admin/lagash2, epsd2/admin/u3adm, epsd2/admin/u3let, epsd2/admin/u3leg, epsd2/admin/oldbab, epsd2/literary, epsd2/emesal, epsd2/earlylit, epsd2/royal, epsd2/praxis, dcclt, dcclt/nineveh, dcclt/signlists, obmc, ckst, blms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = input('Project(s): ').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Split the List of Projects and Download the ZIP files.\n",
    "Split the list of projects and create a list of project names, using the `format_project_list()` and `oracc_download()` functions in the `utils` module. The code of these functions is discussed in more detail in 2.1.0. Download ORACC JSON Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = format_project_list(projects)\n",
    "oracc_download(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"head21\"></a>2.1 The `parsejson()` function\n",
    "The `parsejson()` function will \"dig into\" the `json` file (transformed into a dictionary) until it finds the relevant data. The `json` file consists of a hierarchy of `cdl` nodes; only the lowest nodes contain lemmatization data. The function goes down this hierarchy by calling itself when another `cdl` node is encountered. For nore information about the data hierarchy in the [ORACC](http://oracc.org) `json` files, see [ORACC Open Data](http://oracc.museum.upenn.edu/doc/opendata/index.html).\n",
    "\n",
    "The argument of the `parsejson()` function is a `JSON` object, a dictionary that initially contains the entire contents of the original JSON file. The code takes the key `cdl` which itself contains an array (a list) of `JSON` objects. Iterating through these objects, if an object contains another `cdl` node, the function calls itself with this object as first argument. This way the function digs deeper and deeper into the `JSON` tree, until it does not encounter a `cdl` key anymore. Here we are at the level of individual words. The code checks for a key `f`, if it exists the signs are in the node `gdl` within the `f` node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson_signs(text):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        field = ''\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson_signs(JSONobject)\n",
    "        if \"type\" in JSONobject and JSONobject[\"type\"] == \"field-start\":\n",
    "            field = JSONobject[\"subtype\"]\n",
    "        if \"f\" in JSONobject and not field in ['sg', 'pr']: # skip the fields \"sign\" and \"pronunciation\"\n",
    "                                # in lexical texts\n",
    "            if JSONobject[\"f\"][\"lang\"][:3] == \"sux\": #only Sumerian and Emesal\n",
    "                word = JSONobject[\"f\"]\n",
    "                f = word[\"form\"]\n",
    "                if \"sexified\" in word[\"gdl\"][0]:\n",
    "                    f = word[\"gdl\"][0][\"sexified\"]\n",
    "                if \"cf\" in word:\n",
    "                    if 'pos' in word:  #for some reason some words appear without pos. Provisionally treated as Noun\n",
    "                        lemm = word[\"cf\"] + '[' + word[\"gw\"] + \"]\" + word[\"pos\"]\n",
    "                    else:\n",
    "                        lemm = word[\"cf\"] + '[' + word[\"gw\"] + \"]N\"\n",
    "                    lemm = lemm.replace(' ', '-') # remove commas and spaces from lemm\n",
    "                    lemm = lemm.replace(',', '')\n",
    "                else:\n",
    "                    lemm = word[\"form\"] # if word is unlemmatized\n",
    "                all_.append(f)\n",
    "                lemm_.append(lemm)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Call the `parsejson()` function for every `JSON` file\n",
    "The code in this cell will iterate through the list of projects entered above (1.1). For each project the `JSON` zip file is located in the directory `jsonzip`, named PROJECT.zip. \n",
    "\n",
    "Each of these files is extracted from the `zip` file and read with the command `json.loads()`, which reads the json data and transforms it into a Python dictionary (a sequence of keys and values).\n",
    "\n",
    "This dictionary, which is called `text` is now sent to the `parsejson()` function. The function adds signs to the `sign_l` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ = []\n",
    "lemm_ = []\n",
    "ids_ = []\n",
    "for project in p:\n",
    "    file = \"jsonzip/\" + project.replace(\"/\", \"-\") + \".zip\"\n",
    "    try:\n",
    "        z = zipfile.ZipFile(file)       # create a Zipfile object\n",
    "    except:\n",
    "        print(file + \" does not exist or is not a proper ZIP file\")\n",
    "        continue\n",
    "    files = z.namelist()     # list of all the files in the ZIP\n",
    "    files = [name for name in files if \"corpusjson\" in name and name[-5:] == '.json']                                                                                                  #that holds all the P, Q, and X numbers.\n",
    "    for filename in tqdm.tqdm(files):                            #iterate over the file names\n",
    "        id_no = filename[-13:-5]\n",
    "        if id_no in ids_ and not \"X\" in id_no: # Check if P/Q number is already in there\n",
    "            continue        # a text may appear in multiple projects\n",
    "        id_text = project + id_no # id_text is, for instance, blms/P414332\n",
    "        try:\n",
    "            text = z.read(filename).decode('utf-8')         #read and decode the json file of one particular text\n",
    "            data_json = json.loads(text)                # make it into a json object (essentially a dictionary)\n",
    "            all_.append('Start'+id_text)\n",
    "            lemm_.append('Start'+id_text)   # to keep all_ and lemm_ same length\n",
    "            parsejson_signs(data_json)\n",
    "            ids_.append(id_no)\n",
    "            #print(filename)\n",
    "        except:\n",
    "            print(id_text + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Structuring\n",
    "### 3.1 Transform the Data into a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_l = []\n",
    "separators = ['{', '}', '-']\n",
    "separators2 = ['.', '+', '|']\n",
    "operators = ['&', '%', '@', '×']\n",
    "for e in tqdm.tqdm(all_):\n",
    "    word = []\n",
    "    if '1(šar₂{gal})' in e: # this cheating but it seems to work (appears in SKL 38)\n",
    "            e = e.replace('1(šar₂{gal})', '1(šar₂)-gal')\n",
    "    for s in separators: # first split word into signs   \n",
    "        e = e.replace(s, ' ').strip()\n",
    "    s_l = e.split()\n",
    "    for sign in s_l:\n",
    "        if sign[0].isdigit(): # 1(geš₂), 2(DIŠ), etc.\n",
    "            sign = sign.lower()\n",
    "        elif sign[-1] == ')': # qualified sign - get only the qualifier\n",
    "            stack = []  # |GIŠ×(GIŠ%GIŠ)|(LAK277) becomes LAK277\n",
    "            ind = {}    # LAK277(|GIŠ×(GIŠ%GIŠ)|) becomes |GIŠ×(GIŠ%GIŠ)|\n",
    "            for i, c in reversed(list(enumerate(sign))):\n",
    "                if c == ')':\n",
    "                    stack.append(i)\n",
    "                if c == '(':\n",
    "                    ind[stack.pop()] = i   # find the opening parens that belongs to the closing parens at position -1    \n",
    "            start = ind[len(sign)-1]   # this line fails on 1(šar₂{gal}) in SKL.\n",
    "            t = sign[start+1:-1]\n",
    "            if t.isupper(): #leave 1(diš) etc. alone\n",
    "                sign = t\n",
    "            \n",
    "        if '|' in sign:  # separate |DU.DU| and |DU+DU| into its components but not |DU&DU|\n",
    "                        # and also not |DU.DU&DU|\n",
    "            flag = False\n",
    "            for o in operators:\n",
    "                if o in sign:\n",
    "                    flag = True\n",
    "            if not flag:\n",
    "                for s in separators2:\n",
    "                    sign = sign.replace(s, ' ').strip() \n",
    "                sign_l = sign.split()\n",
    "                word.extend(sign_l)\n",
    "                continue\n",
    "        elif \"+\" in sign:  # + as marker of gloss\n",
    "            sign = sign.replace('+', ' ').strip()\n",
    "            sign_l = sign.split()\n",
    "            word.extend(sign_l)\n",
    "            continue\n",
    "        word.append(sign)\n",
    "    words_l.append(word)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/ogsl.p\", \"rb\") as f:\n",
    "    o = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = list(o[\"value\"])\n",
    "utf = list(o[\"utf8\"])\n",
    "names = list(o[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(zip(names, utf))\n",
    "d2 = dict(zip(val,names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_l = []\n",
    "utf8_l = []\n",
    "for w in tqdm.tqdm(words_l):\n",
    "    seq = [d2[s.lower()] if s.lower() in d2 else s for s in w]\n",
    "    names_l.append(seq)\n",
    "    utf8 = [d[n] if n in d else n for n in seq]\n",
    "    utf8_l.append(''.join(utf8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"transliteration\":all_, \"words\":words_l, \"names\":names_l, \"utf-8\":utf8_l, \"lemm\" : lemm_})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus/sux_df.p\", \"wb\") as w:\n",
    "    pickle.dump(df, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as Text\n",
    "Save three different representations of the Sumerian text. Each representation is saved in a separate text file:\n",
    "- in transliteration        ===> sux_tl.txt\n",
    "- in lemmatized format   ===> sux_lemm.txt\n",
    "- in cuneiform (utf-8)    ===> sux_utf8.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_d = {\"sux_utf8\": \"utf-8\", \"sux_lemm\": \"lemm\", \"sux_tl\" : \"transliteration\"}\n",
    "for rep in rep_d:\n",
    "    text = ' '.join(df[rep_d[rep]]).strip()\n",
    "    text = text.replace(' Start', '\\n').strip()\n",
    "    text = text.replace('Start', '')\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    file = \"corpus/\" + rep + \".txt\"\n",
    "    with open(file, 'w', encoding=\"utf-8\") as w:\n",
    "        w.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
