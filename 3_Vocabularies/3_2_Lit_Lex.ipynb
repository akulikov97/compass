{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Overlap in Lexical and Literary Vocabulary: Digging Deeper\n",
    "\n",
    "In order to research the relationship between lexical and literary material in more detail we will look at individual literary texts. Which texts have more and which have less overlap with the lexical vocabulary?\n",
    "\n",
    "As it turns out, this question is more complicated as it may seem. Longer texts will have more vocabulary items (and Multiple Word Expressions) in common with the lexical corpus than shorter texts, but that does not tell us anything. A simple count, therefore, is no longer sufficient. We will look at various ways to find a measure that will rank literary texts in a meaning fukl way.\n",
    "\n",
    "### 3.2.0 Preparation\n",
    "\n",
    "First import the necessary libraries. If you are running this notebook in Jupyter Lab you will need to install the Jupyter Lab ipywidgets extension (see [Introduction](../1_Preliminaries/1_Introduction.md), section 1.2.2.1). \n",
    "\n",
    "> The [LexicalRichness](https://pypi.org/project/lexicalrichness/) package by Lucas Shen has been slightly adapted for the present purposes. The package expects a data set in the English language in a raw text format that must be pre-processed (removal of interpunction, digits, etc.) and tokenized (cut up into individual words). These steps do not work well for the present data set. The adapted version, named `lexicalrichness_v` is imported from the `utils` directory. The usage information in the [LexicalRichness](https://pypi.org/project/lexicalrichness/) website is valid for `lexicalrichness_v` with the following exceptions:\n",
    "> - the option use_TextBlob in LexicalRichness() is removed\n",
    "> - the option use_tokenizer in LexicalRichness is added; default is use_tokenizer = False.\n",
    "\n",
    "> If `use_tokenizer = False` (default) the main function expects a list as input; no tokenizing or preprocessing is performed. If `use_tokenizer = True` the function expects a string, which is preprocessed and tokenized (default behaviour in the original package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "import os\n",
    "import sys\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from lexicalrichness_v import LexicalRichness as lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the files `litlines.p` `lexlines.p`,, and `lex_vocab.txt` which were produced in [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb). The first two files contain the pickled versions of the DataFrames `lit_lines` and `lex_lines` in which the literary ([epsd2/literary](http://oracc.org/epsd2/literary)) and lexical corpora ([DCCLT](http://oracc.org/dcclt)) are represented in line-by-line format. The file `lex_vocab.txt` contains a list of the full lexical vocabulary, including Multiple Word Expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lines = pd.read_pickle('output/litlines.p')\n",
    "lex_lines = pd.read_pickle('output/lexlines.p')\n",
    "with open('output/lex_vocab.txt', 'r', encoding = 'utf8') as r:\n",
    "    lex_vocab = r.read().splitlines()\n",
    "lex_vocab.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.0.1 Literary: By Composition\n",
    "For the literary corpus we can take the line-by-line representation that was prepared in the previous notebook and transform that into a composition-by-composition representation. The DataFrame `lit_lines` includes the column `lemma_mwe` in which each line is represented as a sequence of lemmas and/or Multiple Word Expressions (lemmas connected by underscores). The `pandas` `groupby()` function is used to group on `id_text` and `text_name`. The aggregate function for the `lemma_mwe` column in this case is simply `' '.join`: all the entries (representing lines) are concatenated to form one long sequence of lemmas representing one composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>lemma_mwe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>P254863</td>\n",
       "      <td>iri[city]n silim[healthy]v/i tag[touch]v/t lul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>P254864</td>\n",
       "      <td>kugzu[wise]aj namkugzu[wisdom]n na-an-ak-x[na]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P254865</td>\n",
       "      <td>lu[person]n_niŋgina[truth]n zi[life]n utud[bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P254866</td>\n",
       "      <td>niŋ[thing]n_gu[neck]n_ŋar[place]v/t niŋ[thing]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>P254867</td>\n",
       "      <td>dubsar[scribe]n mu[name]n ni₂-x[na]na igi[eye]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P254868</td>\n",
       "      <td>x-x[na]na mu[name]n diš[one]nu zu[know]v/t x-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>P254869</td>\n",
       "      <td>dubsar[scribe]n emegir[sumerian]n nu-un-zu-x[n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>P254870</td>\n",
       "      <td>dubsar[scribe]n_tur[small]v/i bar[outside]n ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>P254871</td>\n",
       "      <td>šah[pig]n šu[hand]n kar[flee]v/i iginzu[as-if]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>P254872</td>\n",
       "      <td>amaʾatud[slave]n sulum[contempt]n in-na-x[na]n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_text                                          lemma_mwe\n",
       "25  P254863  iri[city]n silim[healthy]v/i tag[touch]v/t lul...\n",
       "26  P254864  kugzu[wise]aj namkugzu[wisdom]n na-an-ak-x[na]...\n",
       "27  P254865  lu[person]n_niŋgina[truth]n zi[life]n utud[bea...\n",
       "28  P254866  niŋ[thing]n_gu[neck]n_ŋar[place]v/t niŋ[thing]...\n",
       "29  P254867  dubsar[scribe]n mu[name]n ni₂-x[na]na igi[eye]...\n",
       "30  P254868  x-x[na]na mu[name]n diš[one]nu zu[know]v/t x-n...\n",
       "31  P254869  dubsar[scribe]n emegir[sumerian]n nu-un-zu-x[n...\n",
       "32  P254870  dubsar[scribe]n_tur[small]v/i bar[outside]n ni...\n",
       "33  P254871  šah[pig]n šu[hand]n kar[flee]v/i iginzu[as-if]...\n",
       "34  P254872  amaʾatud[slave]n sulum[contempt]n in-na-x[na]n..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_comp = lit_lines.groupby(\n",
    "    [lit_lines[\"id_text\"]]).aggregate(\n",
    "    {\"lemma_mwe\": ' '.join}).reset_index()\n",
    "lit_comp['id_text'] = [id[-7:] for id in lit_comp[\"id_text\"]]\n",
    "lit_comp[25:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a DataFrame with two columns: `id_text`, and `lemma_mwe`. Each row represents a literary composition from the [epsd2/literary](http://oracc.org/epsd2/literary) corpus. Each cell in the column `lemma_mwe` contains a sequence of lemmas of one composition (with MWEs connected by underscores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.0.2 Some Statistics for the Literary Corpus\n",
    "\n",
    "In the following we will compute the number of vocabulary matches between each text in the literary corpus and the full lexical corpus. In order to interpret the number of matches propoerly, we will first compute a number of basic text measures, such as text length, type-token ratio, etc. In all the measures discussed below only the words that are properly lemmatized are counted.\n",
    "\n",
    "| Measurement       |                                             |\n",
    "|-------------------|---------------------------------------------|\n",
    "| Text Length       | Number of lemmatized words and MWEs         |\n",
    "| Lexical Variation | Number of unique lemmas and MWEs            |\n",
    "| Type Token Ration | Lexical Variation divided by Text Length    |\n",
    "| MTLD              | (see below)                                 |\n",
    "\n",
    "Lexical richness measures the variation in vocabulary usage. Texts that use a relatively low number of unique lexemes (repeat the same words all over the place) receive a low lexical richness score. Texts that use the lexicon more ingenuously, using synonyms or circumscriptions to refer to the same concept, receive a higher lexical richness score. Lexical richness is used, among other things, to identify texts written by langauge learners, or to assess the difficulty of a text. For Sumerian literature, we may expect that compositions with high levels of repetition, such as certain hymns and narratives, end up with a low lexical richness score, whereas disputation texts may have a higher score.\n",
    "\n",
    "The most straightforward lexical richness score is the Type Token Ratio (or TTR), which simply divides the number of unique lexemes by the total number of lexemes. This is a fine measurement to compare texts of (approximately) equal length, but does not work well for a corpus with texts of very different length, as is the case here. Short texts have higher TTR values than long texts, because longer texts will by necessity use the same words over and over again and function words such as \"the\" or \"in\" will be repeated many times whatever the lexical ingenuity of the author. A better measurement is called MTLD or Measure of Textual Lexical Diversity (McCarthy and Jarvis 2010). The MTLD value is calculated as the mean number of words in a text that will bring TTR from 1 (at the first word in the text) down to a threshold value (default is 0.720). In practice that means that a text is cut in many small units, each with approximately the same TTR - eliminating the effect of text length. This is a promising approach that may well work for Sumerian and a Python module that includes MTLD is available (lexicalrichness). Its usage here, however, is experimental and preliminary. The threshold value is based on the observation that when going through a text sequentially the TTR in any text will drop drastically as soon as the first repeated word is encountered. At some place in the text the TTR will stabilize and drop only very gradually later on. That place is approximated by the default threshold value of 0.720. It seems likely, however, that a valid threshold value is language dependent and that a language with very few function words, such as the literary register of Sumerian, might need a lower value. On the other hand, a corpus of texts with very substantial repetition (occasionaly repetition of entire lengthy passages) may well require a higher threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lit_stats(lemmas):\n",
    "    lemmas = lemmas.split()\n",
    "    lemmas = [lemma for lemma in lemmas if not '[na]na' in lemma]\n",
    "    lex = lr(lemmas)\n",
    "    words = lex.words\n",
    "    terms = lex.terms\n",
    "    if words > 0:\n",
    "        ttr = lex.ttr\n",
    "        mtld = lex.mtld()\n",
    "    else:\n",
    "        ttr = 0\n",
    "        mtld = 0\n",
    "    return ' '.join(lemmas), words, terms, ttr, mtld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932671b6821144cc8f326b941bbb661d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=911.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lit_comp['lemma_mwe'], lit_comp['length'], lit_comp['lex_var'], lit_comp['ttr'], lit_comp['mtld'] = \\\n",
    "    zip(*lit_comp['lemma_mwe'].progress_map(lit_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a first glimpse of the results by inspecting the basic descriptive statistics. For this, we ignore texts shorter than 200 lemmas, because measures like TTR and MTLD become rather meaningless for such short compositions. It appears that MTLD varies from 10.03 all the way up to 271.08, with a mean value of 79.53. That means that there is a text that, on average, needs only 10 words (two or three lines) to push the TTR under 0.720 - meaning a lot of repeated words (or repeated phrases) all over the place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>ttr</th>\n",
       "      <th>mtld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>203.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>203.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>577.605911</td>\n",
       "      <td>230.093596</td>\n",
       "      <td>0.475719</td>\n",
       "      <td>79.529468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>527.749108</td>\n",
       "      <td>136.288701</td>\n",
       "      <td>0.128182</td>\n",
       "      <td>52.289918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>201.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.164575</td>\n",
       "      <td>10.032316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>257.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.380827</td>\n",
       "      <td>38.803385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>352.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>0.481879</td>\n",
       "      <td>66.734320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>712.500000</td>\n",
       "      <td>280.500000</td>\n",
       "      <td>0.581742</td>\n",
       "      <td>106.224661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3139.000000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>0.753138</td>\n",
       "      <td>271.082712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length     lex_var         ttr        mtld\n",
       "count   203.000000  203.000000  203.000000  203.000000\n",
       "mean    577.605911  230.093596    0.475719   79.529468\n",
       "std     527.749108  136.288701    0.128182   52.289918\n",
       "min     201.000000   59.000000    0.164575   10.032316\n",
       "25%     257.000000  138.000000    0.380827   38.803385\n",
       "50%     352.000000  180.000000    0.481879   66.734320\n",
       "75%     712.500000  280.500000    0.581742  106.224661\n",
       "max    3139.000000  824.000000    0.753138  271.082712"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_comp.loc[lit_comp.length > 200].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Document Term Matrix\n",
    "\n",
    "The literary corpus is transformed into a Document Term Matrix (or DTM), a table in which each column represents a lemma (or Multiple Word Expression)  and each row represents a Sumerian composition. Each cell is a number, 0 or 1, indicating whether or not that word appears  in a particular composition. This is a binary DTM, non-binary DTMs give the number of times a word appears in a composition.\n",
    "\n",
    "The function `CountVectorizer()` (from the `Sklearn` package) is a very flexible tool with many possible parameters. The most common use case is a corpus of raw documents (probably in English), each of them consisting of a text string that needs to be pre-processed and tokenized (turned into a list of words or lemmas) before anything else can be done. Default pre-processing includes, for instance, lowercasing the entire text (so that thursday, Thursday, and THURSDAY will all be recognized as the same lemma). Default tokenizers assume that the text is in a modern (western) language and take spaces and punctuation marks as word dividers. The structure of our data is much simpler than that. Pre-processing is unnecessary, and tokenization should split the string *only* at blank spaces.\n",
    "\n",
    "This can be achieved by defining custom tokenizer/preprocessor functions, and tell `Countvectorizer()` to use these. The custom tokenizer consists of the standard Python function `split()`; the preprocessor function does nothing at all. \n",
    "\n",
    "The parameter `vocabulary` is set to the variable `lex_vocab`, which includes all lemmas and MWEs in the lexical corpus. Without this parameter, `Countvectorizer()` will simply take each unique lemma in the literary corpus and make that into a column of the DTM. The `vocabulary` option allows us to compare the lexical vocabulary (the column names) with the usage of that vocabulary in each literary text (the rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a[arm]n</th>\n",
       "      <th>a[arm]n_ak[do]v/t</th>\n",
       "      <th>a[arm]n_apin[plow]n</th>\n",
       "      <th>a[arm]n_bad[open]v/t</th>\n",
       "      <th>a[arm]n_bad[wall]n</th>\n",
       "      <th>a[arm]n_badsi[parapet]n</th>\n",
       "      <th>a[arm]n_be[diminish]v/t</th>\n",
       "      <th>a[arm]n_da[line]n</th>\n",
       "      <th>a[arm]n_dabašin[object]n</th>\n",
       "      <th>a[arm]n_daluš[sling]n</th>\n",
       "      <th>...</th>\n",
       "      <th>šuʾabdu[1]wn</th>\n",
       "      <th>šuʾi[barber]n</th>\n",
       "      <th>šuʾi[barber]n_egir[back]n</th>\n",
       "      <th>šuʾi[barber]n_gin[firm]v/i</th>\n",
       "      <th>šuʾi[barber]n_gina[offering]n</th>\n",
       "      <th>šuʾi[barber]n_gu[neck]n</th>\n",
       "      <th>šuʾi[barber]n_lugal[king]n</th>\n",
       "      <th>šuʾi[barber]n_saŋ[head]n</th>\n",
       "      <th>šuʾu[stone]n</th>\n",
       "      <th>šuʾura[goose]n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P209784</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251427</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251728</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P252215</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000823</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000825</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q002338</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X010001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>911 rows × 10146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         a[arm]n  a[arm]n_ak[do]v/t  a[arm]n_apin[plow]n  \\\n",
       "id_text                                                    \n",
       "P209784        0                  0                    0   \n",
       "P251427        0                  0                    0   \n",
       "P251713        0                  0                    0   \n",
       "P251728        0                  0                    0   \n",
       "P252215        1                  0                    0   \n",
       "...          ...                ...                  ...   \n",
       "Q000823        1                  0                    0   \n",
       "Q000824        0                  0                    0   \n",
       "Q000825        1                  0                    0   \n",
       "Q002338        0                  0                    0   \n",
       "X010001        0                  0                    0   \n",
       "\n",
       "         a[arm]n_bad[open]v/t  a[arm]n_bad[wall]n  a[arm]n_badsi[parapet]n  \\\n",
       "id_text                                                                      \n",
       "P209784                     0                   0                        0   \n",
       "P251427                     0                   0                        0   \n",
       "P251713                     0                   0                        0   \n",
       "P251728                     0                   0                        0   \n",
       "P252215                     0                   0                        0   \n",
       "...                       ...                 ...                      ...   \n",
       "Q000823                     0                   0                        0   \n",
       "Q000824                     0                   0                        0   \n",
       "Q000825                     0                   0                        0   \n",
       "Q002338                     0                   0                        0   \n",
       "X010001                     0                   0                        0   \n",
       "\n",
       "         a[arm]n_be[diminish]v/t  a[arm]n_da[line]n  a[arm]n_dabašin[object]n  \\\n",
       "id_text                                                                         \n",
       "P209784                        0                  0                         0   \n",
       "P251427                        0                  0                         0   \n",
       "P251713                        0                  0                         0   \n",
       "P251728                        0                  0                         0   \n",
       "P252215                        0                  0                         0   \n",
       "...                          ...                ...                       ...   \n",
       "Q000823                        0                  0                         0   \n",
       "Q000824                        0                  0                         0   \n",
       "Q000825                        0                  0                         0   \n",
       "Q002338                        0                  0                         0   \n",
       "X010001                        0                  0                         0   \n",
       "\n",
       "         a[arm]n_daluš[sling]n  ...  šuʾabdu[1]wn  šuʾi[barber]n  \\\n",
       "id_text                         ...                                \n",
       "P209784                      0  ...             0              0   \n",
       "P251427                      0  ...             0              0   \n",
       "P251713                      0  ...             0              0   \n",
       "P251728                      0  ...             0              0   \n",
       "P252215                      0  ...             0              0   \n",
       "...                        ...  ...           ...            ...   \n",
       "Q000823                      0  ...             0              0   \n",
       "Q000824                      0  ...             0              0   \n",
       "Q000825                      0  ...             0              0   \n",
       "Q002338                      0  ...             0              0   \n",
       "X010001                      0  ...             0              0   \n",
       "\n",
       "         šuʾi[barber]n_egir[back]n  šuʾi[barber]n_gin[firm]v/i  \\\n",
       "id_text                                                          \n",
       "P209784                          0                           0   \n",
       "P251427                          0                           0   \n",
       "P251713                          0                           0   \n",
       "P251728                          0                           0   \n",
       "P252215                          0                           0   \n",
       "...                            ...                         ...   \n",
       "Q000823                          0                           0   \n",
       "Q000824                          0                           0   \n",
       "Q000825                          0                           0   \n",
       "Q002338                          0                           0   \n",
       "X010001                          0                           0   \n",
       "\n",
       "         šuʾi[barber]n_gina[offering]n  šuʾi[barber]n_gu[neck]n  \\\n",
       "id_text                                                           \n",
       "P209784                              0                        0   \n",
       "P251427                              0                        0   \n",
       "P251713                              0                        0   \n",
       "P251728                              0                        0   \n",
       "P252215                              0                        0   \n",
       "...                                ...                      ...   \n",
       "Q000823                              0                        0   \n",
       "Q000824                              0                        0   \n",
       "Q000825                              0                        0   \n",
       "Q002338                              0                        0   \n",
       "X010001                              0                        0   \n",
       "\n",
       "         šuʾi[barber]n_lugal[king]n  šuʾi[barber]n_saŋ[head]n  šuʾu[stone]n  \\\n",
       "id_text                                                                       \n",
       "P209784                           0                         0             0   \n",
       "P251427                           0                         0             0   \n",
       "P251713                           0                         0             0   \n",
       "P251728                           0                         0             0   \n",
       "P252215                           0                         0             0   \n",
       "...                             ...                       ...           ...   \n",
       "Q000823                           0                         0             0   \n",
       "Q000824                           0                         0             0   \n",
       "Q000825                           0                         0             0   \n",
       "Q002338                           0                         0             0   \n",
       "X010001                           0                         0             0   \n",
       "\n",
       "         šuʾura[goose]n  \n",
       "id_text                  \n",
       "P209784               0  \n",
       "P251427               0  \n",
       "P251713               0  \n",
       "P251728               0  \n",
       "P252215               0  \n",
       "...                 ...  \n",
       "Q000823               0  \n",
       "Q000824               0  \n",
       "Q000825               0  \n",
       "Q002338               0  \n",
       "X010001               0  \n",
       "\n",
       "[911 rows x 10146 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=lambda x: x.split(), preprocessor=lambda x: x, vocabulary=lex_vocab, binary=True)\n",
    "#Alternative way to do the same thing:\n",
    "#cv = CountVectorizer(token_pattern = r'[^ ]+', vocabulary=lex_vocab, binary=True)\n",
    "dtm = cv.fit_transform(lit_comp['lemma_mwe'])\n",
    "lit_df = pd.DataFrame(dtm.toarray(), columns= cv.get_feature_names(), index=lit_comp[\"id_text\"])\n",
    "lit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame lit_df has a row for each *literary* composition and it has a column for every lemma/expression in the *lexical* corpus. The number of columns, therefore, should correspond to the size of the lexical vocabulary in the Venn diagram produced in the previous notebook:\n",
    "\n",
    "![venn diagram 3](viz/venn_3.png)\n",
    "\n",
    "As we have seen in the previous notebook, many of these words/expressions do not appear in the [epsd2/literary](http://oracc.org/epsd2/literary) corpus, and thus all cells in those columns are 0. If we remove those columns, we get the vocabulary that is shared between the lexical corpus and the literary corpus (the intersection of the circles in the Venn diagram). The left side of the Venn diagram (the literary vocabulary that does not appear in lexical texts) is not represented in the DTM. This DTM, therefore, should only be used to research *intersection* between the two (literary and lexical) vocabularies.\n",
    "\n",
    "> The number of non-zero columns does not *exactly* correspond to the size of the intersection in the Venn diagram. The reason is that a word like **ašrinna\\[object\\]n**, a word that in the literary corpus only appears in the MWE **kid\\[mat\\]n_ašrinna\\[object\\]n**, is counted as a match in the Venn diagram, but only appears in the column **kid\\[mat\\]n_ašrinna\\[object\\]n** in the DTM. See the previous notebook section 3.1.3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a[arm]n</th>\n",
       "      <th>a[arm]n_ak[do]v/t</th>\n",
       "      <th>a[arm]n_bad[open]v/t</th>\n",
       "      <th>a[arm]n_dar[split]v/t</th>\n",
       "      <th>a[arm]n_daŋal[wide]v/i</th>\n",
       "      <th>a[arm]n_durah[goat]n</th>\n",
       "      <th>a[arm]n_e[leave]v/i</th>\n",
       "      <th>a[arm]n_gab[left]n</th>\n",
       "      <th>a[arm]n_gal[big]v/i</th>\n",
       "      <th>a[arm]n_gud[ox]n</th>\n",
       "      <th>...</th>\n",
       "      <th>šutum[storehouse]n</th>\n",
       "      <th>šutur[garment]n</th>\n",
       "      <th>šuziʾana[1]dn</th>\n",
       "      <th>šuš[cover]v/t</th>\n",
       "      <th>šušana[one-third]nu</th>\n",
       "      <th>šuši[sixty]nu</th>\n",
       "      <th>šušin[1]sn</th>\n",
       "      <th>šušru[distressed]v/i</th>\n",
       "      <th>šuʾi[barber]n</th>\n",
       "      <th>šuʾura[goose]n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P209784</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251427</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251728</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P252215</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000823</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000825</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q002338</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X010001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>911 rows × 3508 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         a[arm]n  a[arm]n_ak[do]v/t  a[arm]n_bad[open]v/t  \\\n",
       "id_text                                                     \n",
       "P209784        0                  0                     0   \n",
       "P251427        0                  0                     0   \n",
       "P251713        0                  0                     0   \n",
       "P251728        0                  0                     0   \n",
       "P252215        1                  0                     0   \n",
       "...          ...                ...                   ...   \n",
       "Q000823        1                  0                     0   \n",
       "Q000824        0                  0                     0   \n",
       "Q000825        1                  0                     0   \n",
       "Q002338        0                  0                     0   \n",
       "X010001        0                  0                     0   \n",
       "\n",
       "         a[arm]n_dar[split]v/t  a[arm]n_daŋal[wide]v/i  a[arm]n_durah[goat]n  \\\n",
       "id_text                                                                        \n",
       "P209784                      0                       0                     0   \n",
       "P251427                      0                       0                     0   \n",
       "P251713                      0                       0                     0   \n",
       "P251728                      0                       0                     0   \n",
       "P252215                      0                       0                     0   \n",
       "...                        ...                     ...                   ...   \n",
       "Q000823                      0                       0                     0   \n",
       "Q000824                      0                       0                     0   \n",
       "Q000825                      0                       0                     0   \n",
       "Q002338                      0                       0                     0   \n",
       "X010001                      0                       0                     0   \n",
       "\n",
       "         a[arm]n_e[leave]v/i  a[arm]n_gab[left]n  a[arm]n_gal[big]v/i  \\\n",
       "id_text                                                                 \n",
       "P209784                    0                   0                    0   \n",
       "P251427                    0                   0                    0   \n",
       "P251713                    0                   0                    0   \n",
       "P251728                    0                   0                    0   \n",
       "P252215                    0                   0                    0   \n",
       "...                      ...                 ...                  ...   \n",
       "Q000823                    0                   0                    0   \n",
       "Q000824                    0                   0                    0   \n",
       "Q000825                    0                   0                    0   \n",
       "Q002338                    0                   0                    0   \n",
       "X010001                    0                   0                    0   \n",
       "\n",
       "         a[arm]n_gud[ox]n  ...  šutum[storehouse]n  šutur[garment]n  \\\n",
       "id_text                    ...                                        \n",
       "P209784                 0  ...                   0                0   \n",
       "P251427                 0  ...                   0                0   \n",
       "P251713                 0  ...                   0                0   \n",
       "P251728                 0  ...                   0                0   \n",
       "P252215                 0  ...                   0                0   \n",
       "...                   ...  ...                 ...              ...   \n",
       "Q000823                 0  ...                   0                0   \n",
       "Q000824                 0  ...                   0                0   \n",
       "Q000825                 0  ...                   0                0   \n",
       "Q002338                 0  ...                   0                0   \n",
       "X010001                 0  ...                   0                0   \n",
       "\n",
       "         šuziʾana[1]dn  šuš[cover]v/t  šušana[one-third]nu  šuši[sixty]nu  \\\n",
       "id_text                                                                     \n",
       "P209784              0              0                    0              0   \n",
       "P251427              0              0                    0              0   \n",
       "P251713              0              0                    0              0   \n",
       "P251728              0              0                    0              0   \n",
       "P252215              0              0                    0              0   \n",
       "...                ...            ...                  ...            ...   \n",
       "Q000823              0              0                    0              0   \n",
       "Q000824              0              0                    0              0   \n",
       "Q000825              0              0                    0              0   \n",
       "Q002338              0              0                    0              0   \n",
       "X010001              0              0                    0              0   \n",
       "\n",
       "         šušin[1]sn  šušru[distressed]v/i  šuʾi[barber]n  šuʾura[goose]n  \n",
       "id_text                                                                   \n",
       "P209784           0                     0              0               0  \n",
       "P251427           0                     0              0               0  \n",
       "P251713           0                     0              0               0  \n",
       "P251728           0                     0              0               0  \n",
       "P252215           0                     0              0               0  \n",
       "...             ...                   ...            ...             ...  \n",
       "Q000823           0                     0              0               0  \n",
       "Q000824           0                     0              0               0  \n",
       "Q000825           0                     0              0               0  \n",
       "Q002338           0                     0              0               0  \n",
       "X010001           0                     0              0               0  \n",
       "\n",
       "[911 rows x 3508 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df = lit_df.loc[: , lit_df.sum(axis=0) != 0].copy()\n",
    "vocab = lit_df.columns # `vocab` is a list with all the vocabulary items currently in `lit_df`\n",
    "lit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Lexical/Literary Matches per Literary Composition. \n",
    "Since the DTM was built with the option `binary = True` the sum of each row equals the number of unique words/expressions that the composition shares with the lexical corpus. The code in the cell below may be simplified as:\n",
    "```python\n",
    "lit_df[\"n_matches\"] = lit_df.sum(axis=1)\n",
    "```\n",
    "which will yield exactly the same result. The extra elements in the code are added for two reasons. First, if we add additional columns to the DataFrame, for instance composition names, the code will fail unless we add the option `numeric_only = True`. Second, if the (simplified) code is run twice, even with the option `numeric_only=True` the column `n_matches` will become part of the summation and the result in the new `n_matches` column will be twice the correct outcome. By explicitly stating that only the columns named after the lemmas in `vocab` should be used such accidents are avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df[\"n_matches\"] = lit_df[vocab].sum(axis=1, numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Metadata and Lexical Richness Statistics\n",
    "Above, we computed various statistics for each of the literary compositions. The catalog file for epsd2/literary contains further information (such as the composition name). We can merge all of this information into a single DataFrame. The merge method is \"inner,\" which means that only those rows that exist in both DataFrames will end up in the new DataFrame. Reduce the DataFrame by only admitting texts that include more than 200 lemmatized words. This last step removes a large number of documents - documents that are not lemmatized, fragments of literary texts that only include a few words, and texts such as lentils (school text) that contain a single proverb or a single line from a royal hymn. For these short texts, the quantitative comparison in the intersection with lexical vocabulary makes little sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the metadata. \n",
    "file = \"jsonzip/epsd2-literary.zip\"\n",
    "z = zipfile.ZipFile(file) \n",
    "st = z.read(\"epsd2/literary/catalogue.json\").decode(\"utf-8\")\n",
    "j = json.loads(st)\n",
    "cat_df = pd.DataFrame(j[\"members\"]).T\n",
    "#The important information, giving the tile of the literary text is sometimes found in \n",
    "# `designation` and sometimes in `subgenre`. Merge those two fields.\n",
    "cat_df.loc[cat_df.designation.str[:13] == \"CDLI Literary\", \"designation\"] = cat_df.subgenre\n",
    "# Exemplars have a P number (`id_text`), composite texts have a Q number (`id_composite`).\n",
    "# Merge those two in `id_text`.\n",
    "cat_df[\"id_text\"] = cat_df[\"id_text\"].fillna(cat_df[\"id_composite\"])\n",
    "# # Keep only `id_text` and `designation`.\n",
    "cat_df = cat_df[[\"id_text\", \"designation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the Lexical Richness statistics (`lit_comp`) with the number of lexical matches from the DTM (`lit_df`) by the shared field `id_text`. Merge the resulting DataFrame (`lit_df2`) with the metadata (`cat_df`) and restrict the data set to compositions with more than 200 lemmatized words or MWEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>designation</th>\n",
       "      <th>length</th>\n",
       "      <th>mtld</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>n_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P251713</td>\n",
       "      <td>CUSAS 38, 05</td>\n",
       "      <td>268</td>\n",
       "      <td>89.333333</td>\n",
       "      <td>0.555970</td>\n",
       "      <td>149</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P252270</td>\n",
       "      <td>ETCSL 4.02.01 Baba A (witness)</td>\n",
       "      <td>211</td>\n",
       "      <td>183.236842</td>\n",
       "      <td>0.691943</td>\n",
       "      <td>146</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>P252333</td>\n",
       "      <td>ETCSL nn szir3-nam-szub-ba {d}en-ki-ka3-kam (w...</td>\n",
       "      <td>396</td>\n",
       "      <td>13.979005</td>\n",
       "      <td>0.371212</td>\n",
       "      <td>147</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>P346086</td>\n",
       "      <td>ETCSL 1.01.01 Enki and Ninhursaga (witness)</td>\n",
       "      <td>201</td>\n",
       "      <td>23.366089</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>117</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>P346087</td>\n",
       "      <td>ETCSL 1.06.03 Ninurta and Turtle (witness)</td>\n",
       "      <td>272</td>\n",
       "      <td>63.826897</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>148</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>Q000818</td>\n",
       "      <td>Proverbs: collection 26</td>\n",
       "      <td>242</td>\n",
       "      <td>99.130068</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>156</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Q000821</td>\n",
       "      <td>Proverbs: from Nippur</td>\n",
       "      <td>413</td>\n",
       "      <td>178.529830</td>\n",
       "      <td>0.607748</td>\n",
       "      <td>251</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>Q000823</td>\n",
       "      <td>Proverbs: from Ur</td>\n",
       "      <td>1399</td>\n",
       "      <td>117.398601</td>\n",
       "      <td>0.380272</td>\n",
       "      <td>532</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Q000825</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>539</td>\n",
       "      <td>87.972272</td>\n",
       "      <td>0.556586</td>\n",
       "      <td>300</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>X010001</td>\n",
       "      <td>Saeedi 0212</td>\n",
       "      <td>254</td>\n",
       "      <td>134.760218</td>\n",
       "      <td>0.610236</td>\n",
       "      <td>155</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text                                        designation  length  \\\n",
       "2    P251713                                       CUSAS 38, 05     268   \n",
       "8    P252270                     ETCSL 4.02.01 Baba A (witness)     211   \n",
       "14   P252333  ETCSL nn szir3-nam-szub-ba {d}en-ki-ka3-kam (w...     396   \n",
       "61   P346086        ETCSL 1.01.01 Enki and Ninhursaga (witness)     201   \n",
       "62   P346087         ETCSL 1.06.03 Ninurta and Turtle (witness)     272   \n",
       "..       ...                                                ...     ...   \n",
       "901  Q000818                            Proverbs: collection 26     242   \n",
       "904  Q000821                              Proverbs: from Nippur     413   \n",
       "906  Q000823                                  Proverbs: from Ur    1399   \n",
       "908  Q000825                    Proverbs: of unknown provenance     539   \n",
       "910  X010001                                        Saeedi 0212     254   \n",
       "\n",
       "           mtld       ttr  lex_var  n_matches  \n",
       "2     89.333333  0.555970      149        129  \n",
       "8    183.236842  0.691943      146        138  \n",
       "14    13.979005  0.371212      147        126  \n",
       "61    23.366089  0.582090      117        107  \n",
       "62    63.826897  0.544118      148        139  \n",
       "..          ...       ...      ...        ...  \n",
       "901   99.130068  0.644628      156        151  \n",
       "904  178.529830  0.607748      251        226  \n",
       "906  117.398601  0.380272      532        482  \n",
       "908   87.972272  0.556586      300        280  \n",
       "910  134.760218  0.610236      155        141  \n",
       "\n",
       "[203 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df2 = pd.merge(lit_comp[[\"id_text\", \"length\", \"mtld\", \"ttr\", \"lex_var\"]], lit_df[\"n_matches\"], on=\"id_text\", how=\"inner\")\n",
    "lit_df2 = pd.merge(cat_df, lit_df2, on = 'id_text', how = 'inner')\n",
    "lit_df2 = lit_df2.loc[lit_df2.length > 200]\n",
    "lit_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by the number of lexical matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>designation</th>\n",
       "      <th>length</th>\n",
       "      <th>mtld</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>n_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Q000351</td>\n",
       "      <td>Ninurta's exploits: a šir-sud (?) to Ninurta</td>\n",
       "      <td>3139</td>\n",
       "      <td>149.173640</td>\n",
       "      <td>0.262504</td>\n",
       "      <td>824</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>Q000380</td>\n",
       "      <td>The lament for Sumer and Ur</td>\n",
       "      <td>2668</td>\n",
       "      <td>57.629651</td>\n",
       "      <td>0.273988</td>\n",
       "      <td>731</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>Q000367</td>\n",
       "      <td>Lugalbanda in the mountain cave</td>\n",
       "      <td>1941</td>\n",
       "      <td>79.044336</td>\n",
       "      <td>0.326636</td>\n",
       "      <td>634</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>Q000750</td>\n",
       "      <td>The temple hymns</td>\n",
       "      <td>2498</td>\n",
       "      <td>86.438542</td>\n",
       "      <td>0.279023</td>\n",
       "      <td>697</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>Q000334</td>\n",
       "      <td>Enki and the world order</td>\n",
       "      <td>1937</td>\n",
       "      <td>89.129452</td>\n",
       "      <td>0.319050</td>\n",
       "      <td>618</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text                                   designation  length  \\\n",
       "547  Q000351  Ninurta's exploits: a šir-sud (?) to Ninurta    3139   \n",
       "572  Q000380                   The lament for Sumer and Ur    2668   \n",
       "561  Q000367               Lugalbanda in the mountain cave    1941   \n",
       "849  Q000750                              The temple hymns    2498   \n",
       "531  Q000334                      Enki and the world order    1937   \n",
       "\n",
       "           mtld       ttr  lex_var  n_matches  \n",
       "547  149.173640  0.262504      824        713  \n",
       "572   57.629651  0.273988      731        616  \n",
       "561   79.044336  0.326636      634        578  \n",
       "849   86.438542  0.279023      697        570  \n",
       "531   89.129452  0.319050      618        558  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df2 = lit_df2.sort_values(by = \"n_matches\", na_position=\"first\", ascending=False)\n",
    "lit_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "Lugal-e (or [Ninurta's Exploits](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.6.2&display=Crit&charenc=gcirc#) has the highest number of matches (more than 700) with the Old Babylonian lexical corpus in [DCCLT](http://oracc.org/dcclt). But this is also the longest composition in the corpus. We can normalize by dividing the total number of matches (`n_matches`) by the number of unique lemmas (`lex_var`) in the text (`norm`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>designation</th>\n",
       "      <th>length</th>\n",
       "      <th>mtld</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>n_matches</th>\n",
       "      <th>norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>Q000626</td>\n",
       "      <td>A tigi to Inana (Inana E)</td>\n",
       "      <td>294</td>\n",
       "      <td>24.177292</td>\n",
       "      <td>0.329932</td>\n",
       "      <td>97</td>\n",
       "      <td>95</td>\n",
       "      <td>0.979381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>Q000695</td>\n",
       "      <td>A tigi to Nergal (Nergal C)</td>\n",
       "      <td>213</td>\n",
       "      <td>60.664253</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>97</td>\n",
       "      <td>95</td>\n",
       "      <td>0.979381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>P346203</td>\n",
       "      <td>ETCSL 2.05.04.23 Ishme-Dagan W (witness)</td>\n",
       "      <td>215</td>\n",
       "      <td>84.493742</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>125</td>\n",
       "      <td>121</td>\n",
       "      <td>0.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>Q000818</td>\n",
       "      <td>Proverbs: collection 26</td>\n",
       "      <td>242</td>\n",
       "      <td>99.130068</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>156</td>\n",
       "      <td>151</td>\n",
       "      <td>0.967949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>Q000785</td>\n",
       "      <td>The three ox-drivers from Adab</td>\n",
       "      <td>268</td>\n",
       "      <td>21.743129</td>\n",
       "      <td>0.339552</td>\n",
       "      <td>91</td>\n",
       "      <td>88</td>\n",
       "      <td>0.967033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>Q000632</td>\n",
       "      <td>A balbale to Inana (Dumuzid-Inana A)</td>\n",
       "      <td>212</td>\n",
       "      <td>10.758310</td>\n",
       "      <td>0.278302</td>\n",
       "      <td>59</td>\n",
       "      <td>48</td>\n",
       "      <td>0.813559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>Q000752</td>\n",
       "      <td>A hymn to the E-kur</td>\n",
       "      <td>244</td>\n",
       "      <td>10.425455</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>61</td>\n",
       "      <td>46</td>\n",
       "      <td>0.754098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>Q000559</td>\n",
       "      <td>Letter from Puzur-Šulgi to Ibbi-Suen about Išb...</td>\n",
       "      <td>205</td>\n",
       "      <td>46.402360</td>\n",
       "      <td>0.531707</td>\n",
       "      <td>109</td>\n",
       "      <td>82</td>\n",
       "      <td>0.752294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Q000372</td>\n",
       "      <td>The rulers of Lagaš</td>\n",
       "      <td>405</td>\n",
       "      <td>25.857924</td>\n",
       "      <td>0.441975</td>\n",
       "      <td>179</td>\n",
       "      <td>129</td>\n",
       "      <td>0.720670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Q000371</td>\n",
       "      <td>The Sumerian king list</td>\n",
       "      <td>1049</td>\n",
       "      <td>10.893547</td>\n",
       "      <td>0.227836</td>\n",
       "      <td>239</td>\n",
       "      <td>86</td>\n",
       "      <td>0.359833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text                                        designation  length  \\\n",
       "754  Q000626                          A tigi to Inana (Inana E)     294   \n",
       "812  Q000695                        A tigi to Nergal (Nergal C)     213   \n",
       "137  P346203           ETCSL 2.05.04.23 Ishme-Dagan W (witness)     215   \n",
       "901  Q000818                            Proverbs: collection 26     242   \n",
       "870  Q000785                     The three ox-drivers from Adab     268   \n",
       "..       ...                                                ...     ...   \n",
       "760  Q000632               A balbale to Inana (Dumuzid-Inana A)     212   \n",
       "851  Q000752                                A hymn to the E-kur     244   \n",
       "720  Q000559  Letter from Puzur-Šulgi to Ibbi-Suen about Išb...     205   \n",
       "566  Q000372                                The rulers of Lagaš     405   \n",
       "565  Q000371                             The Sumerian king list    1049   \n",
       "\n",
       "          mtld       ttr  lex_var  n_matches      norm  \n",
       "754  24.177292  0.329932       97         95  0.979381  \n",
       "812  60.664253  0.455399       97         95  0.979381  \n",
       "137  84.493742  0.581395      125        121  0.968000  \n",
       "901  99.130068  0.644628      156        151  0.967949  \n",
       "870  21.743129  0.339552       91         88  0.967033  \n",
       "..         ...       ...      ...        ...       ...  \n",
       "760  10.758310  0.278302       59         48  0.813559  \n",
       "851  10.425455  0.250000       61         46  0.754098  \n",
       "720  46.402360  0.531707      109         82  0.752294  \n",
       "566  25.857924  0.441975      179        129  0.720670  \n",
       "565  10.893547  0.227836      239         86  0.359833  \n",
       "\n",
       "[203 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df2[\"norm\"] = lit_df2[\"n_matches\"] / lit_df2[\"lex_var\"]\n",
    "lit_df2.sort_values(by = \"norm\", na_position=\"first\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Results\n",
    "The following code displays the result in an interactive table that may be sorted (ascending or descending) in different ways for further exploration. The column `id_text` provides links to the editions in [epsd2/literary](http://oracc.org/epsd2/literary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.org/epsd2/literary/{}\", target=\"_blank\">{}</a>'\n",
    "lit = lit_df2.copy()\n",
    "lit['id_text'] = [anchor.format(val,val) for val in lit['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95763a95cdba439880ad87c489ed07c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='sort_by', index=7, options=('id_text', 'designation', 'length', 'm…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(sort_by = lit.columns, rows = (1, len(lit), 1))\n",
    "def sort_df(sort_by = \"norm\", ascending = False, rows = 10):\n",
    "    return lit.sort_values(by = sort_by, ascending = ascending).reset_index(drop=True)[:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide descriptive statistics of the `norm` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    203.000000\n",
       "mean       0.906160\n",
       "std        0.053349\n",
       "min        0.359833\n",
       "25%        0.895003\n",
       "50%        0.913462\n",
       "75%        0.930233\n",
       "max        0.979381\n",
       "Name: norm, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit['norm'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Some Viz\n",
    "Provisional. Mainly as example. Save the figures by opening an Output View (right click on output) and then right click on that Output View, select Save As."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbins = 5\n",
    "column = 'norm'\n",
    "fig, ax = plt.subplots()\n",
    "counts, bins, patches = ax.hist(lit[column], bins = nbins, edgecolor='k', color = \"#5d92dd\")\n",
    "ax.set_xticks(bins)\n",
    "for i in range(len(bins) - 1):\n",
    "    plt.text(bins[i],counts[i]/2,str(counts[i]), fontsize = 16)\n",
    "plt.ylabel('No. of Compositions')\n",
    "plt.xlabel(column)\n",
    "plt.savefig('viz/hist_norm.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative. Much simpler - but does not return the bins and the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit.mtld.hist(bins = 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit.plot.scatter(x = 'length', y = 'ttr', figsize = (10, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Creating Output Only\n",
    "The following code is used to create MarkDown tables from Pandas DataFrames. The tables can be included in the Compass Markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-5df088049afd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtabulate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_tab = lit_df2.copy()\n",
    "markdown = \"[{}](http://oracc.org/epsd2/literary{}\"\n",
    "lit_tab['id_text'] = [markdown.format(val,val) for val in lit_df2['id_text']]\n",
    "lit_tab = lit_tab.round({'ttr' : 3, 'norm': 3, 'mtld' : 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 10 # number of rows to be exported\n",
    "col = 'norm' # column by which to sort\n",
    "asc = True\n",
    "tab = tabulate(etcsl_tab.sort_values(by=col, ascending=asc)[:rows],\n",
    "         headers= etcsl_tab.columns , tablefmt=\"github\", showindex=False)\n",
    "with open('output/lit_tab.txt', 'w', encoding='utf8') as w:\n",
    "    w.write(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Add one word at a time and see how that influences ttr. Does ttr arrive at a plateau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for id in lit_comp['id_text']:\n",
    "    c = lit_comp.loc[lit_comp['id_text'] == id, 'lemma_mwe']\n",
    "    c = c.iloc[0]\n",
    "\n",
    "    ttr_l = []\n",
    "    enum = range(1, len(c))\n",
    "    for ind in enum:\n",
    "        t = c[:ind]\n",
    "        ttr = lr(t).ttr\n",
    "        ttr_l.append(ttr)\n",
    "    plt.plot(enum, ttr_l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df2.loc[(86 < lit_df2.mtld) & (lit_df2.mtld < 162.6)].sort_values(by = 'mtld')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following needs to be redone with Q numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetrad = {'c.2.5.8.1' : 1, 'c.2.5.3.2' : 1, 'c.2.5.5.2' : 1, 'c.4.16.1': 1}\n",
    "decad = {'c.2.4.2.01' : 2, 'c.2.5.5.1' : 2, 'c.5.5.4' : 2, 'c.4.07.2' : 2, 'c.4.05.1' : 2,\n",
    "         'c.4.80.2' : 2, 'c.1.1.4' : 2, 'c.1.3.2' : 2, 'c.4.28.1' : 2, 'c.1.8.1.5' : 2}\n",
    "houseF = {'c.5.1.2' : 3,'c.5.1.3' : 3, 'c.1.8.1.4' : 3, 'c.1.6.2' : 3, 'c.2.1.5' : 3,\n",
    "          'c.2.4.2.02' : 3, 'c.2.2.2' : 3, 'c.5.6.1' : 3, 'c.5.1.1' : 3, 'c.5.3.2' : 3,\n",
    "          'c.1.4.3' : 3, 'c.5.6.3' : 3, 'c.5.4.1' : 3, 'c.5.3.1' : 3}\n",
    "proverbs = {'c.6.1.01' : 4, 'c.6.1.02' : 4, 'c.6.1.03' : 4, 'c.6.1.04' : 4, 'c.6.1.05' : 4,\n",
    "            'c.6.1.06' : 4, 'c.6.1.07' : 4,'c.6.1.08' : 4, 'c.6.1.09' : 4, 'c.6.1.10' : 4,\n",
    "            'c.6.1.11' : 4, 'c.6.1.12' : 4, 'c.6.1.13' : 4, 'c.6.1.14' : 4,'c.6.1.15' : 4,\n",
    "            'c.6.1.16' : 4, 'c.6.1.17' : 4, 'c.6.1.18' : 4, 'c.6.1.19' : 4, 'c.6.1.20' : 4,\n",
    "            'c.6.1.21' : 4, 'c.6.1.22' : 4, 'c.6.1.23' : 4, 'c.6.1.24' : 4, 'c.6.1.25' : 4,\n",
    "            'c.6.1.26' : 4, 'c.6.1.27' : 4, 'c.6.1.28' : 4, 'c.6.2.1' : 4, 'c.6.2.2' : 4,\n",
    "            'c.6.2.3' : 4,'c.6.2.4' : 4,'c.6.2.5' : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educL = {}\n",
    "educL.update(tetrad)\n",
    "educL.update(decad)\n",
    "educL.update(houseF)\n",
    "educL.update(proverbs)\n",
    "educ = lit_df2.loc[lit_df2.id_text.isin(educL)].sort_values(by = 'norm')\n",
    "educ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educ['category'] = [educL[id] for id in educ.id_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educ.sort_values(by = 'mtld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl.norm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl.loc[round(etcsl.norm, 3) == 0.874].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#colors = {1 :'red', 2:'blue', 3:'green', 4:'black'}\n",
    "#plt.scatter(educ.norm, educ.mtld, s =75, c=educ['category'].apply(lambda x: colors[x]), alpha = 1)\n",
    "sns.scatterplot('norm', 'mtld', data=educ, hue='category', size = 'length', sizes = (50, 200), alpha = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educL['c.2.5.5.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hymns = etcsl_df2.loc[etcsl_df2.id_text.str.startswith('c.2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hymns.sort_values(by = 'id_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = etcsl_df2.groupby(etcsl_df2.id_text.str[:5]).aggregate({'norm' : 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl[['mtld', 'length', 'norm', 'lex_var', 'ttr', 'n_matches']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD = set(etcsl_comp.lemma_mwe.iloc[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DD - set(lex_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl_comp.iloc[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl_df2.loc[etcsl_df2.id_text == 'c.1.4.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl.norm.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Some thoughts\n",
    "\n",
    "> * Step 1. Measure length of lemma_mwe in etcsl_comp and remove rows with len < 200.\n",
    "> * Step 2. Create DTM (see below) of etcsl_comp, binary = True and vocabulary = lemma_mwe from lex (use lex_lines)\n",
    "> * Step 3. Order compositions by highest match\n",
    "> * Step 4. Normalize for text length (from Step 1)\n",
    "> * Step 5. Same process for individual lex texts (which has highest match for Ura 4?)\n",
    "> * Step 6. TF-IDF\n",
    "\n",
    "> In future iteration: do *not* select among lexical texts - let the script figure out which lex compositions are most relevant.\n",
    "\n",
    "> Perhaps: make DTM first - show that DTM.shape gives same numbers for lex vocabulary as second Venn diagram above. Remove all columns where sum == 0. Show that DTM.shape now gives total of overlap as in Venn diagram above. Then remove rows <= minimum. Tricky!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
