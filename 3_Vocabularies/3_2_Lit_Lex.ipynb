{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Overlap in Lexical and Literary Vocabulary: Digging Deeper\n",
    "\n",
    "In order to research the relationship between lexical and literary material in more detail we first organize the [epsd2/literary](http://oracc.org/epsd2/literary) corpus in a Document Term Matrix. A Document Term Matrix is a table in which each row is a document (in our case: a literary composition) and each column represents a lemma. Each cell indicates how many times the lemma appears in this particular document.\n",
    "\n",
    "### 3.2.0 Preparation\n",
    "\n",
    "First import the necessary libraries. If you are running this notebook in Jupyter Lab you will need to install the Jupyter Lab ipywidgets extension (see [Introduction](../1_Preliminaries/1_Introduction.md), section 1.2.2.1). \n",
    "\n",
    "The [LexicalRichness](https://pypi.org/project/lexicalrichness/) package by Lucas Shen has been adapted for the present purposes in order to circumvent preprocessing and tokenization. The adapted version, named `lexicalrichness_v` can be imported from the `utils` directory. The usage information in the [LexicalRichness](https://pypi.org/project/lexicalrichness/) website is valid for `lexicalrichness_v` with the following exceptions:\n",
    "- the option use_TextBlob in LexicalRichness() is removed\n",
    "- the option use_tokenizer in LexicalRichness is added; default is use_tokenizer = False.\n",
    "\n",
    "If `use_tokenizer = False` (default) the function expects a list as input; no tokenizing or preprocessing is performed. If `use_tokenizer = True` the function expects a string, which is preprocessed and tokenized (default behaviour in the original package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "import os\n",
    "import sys\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from lexicalrichness_v import LexicalRichness as lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the files `litlines.p` and `lexlines.p` which were produced in [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb). These files contain the pickled versions of the DataFrames `lit_lines` and `lex_lines` in which the literary ([epsd2/literary](http://oracc.org/epsd2/literary)) and lexical corpora ([DCCLT](http://oracc.org/dcclt)) are represented in line-by-line format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lines = pd.read_pickle('output/litlines.p')\n",
    "lex_lines = pd.read_pickle('output/lexlines.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unlemmatized words from the column `lemma_mwe`. Each entry in `lemma_mwe` is a list of lemmatized words and expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74620982487422bb10f75b0c536b7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38785), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lit_lines['lemma_mwe'] = lit_lines.progress_apply(lambda x: [lemma for lemma in x['lemma_mwe'] \n",
    "                                if not '[na]' in lemma],\n",
    "                                axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.0.1 Literary: By Composition\n",
    "For the literary corpus we can take the line-by-line representation that was prepared in the previous notebook and transform that into a composition-by-composition representation. The DataFrame `etcsl_lines` includes the column `lemma_mwe` in which each line is represented as a list of lemmas and/or Multiple Word Expressions (lemmas connected by underscores). The `pandas` `groupby()` function is used to group on `id_text` and `text_name`. The aggregate function for the `lemma_mwe` column in this case is simply `sum`: all the lists (representing lines) are added up to form one long list of lemmas representing one composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>lemma_mwe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>P254864</td>\n",
       "      <td>[kugzu[wise]aj, namkugzu[wisdom]n, u[and]cnj, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>P254865</td>\n",
       "      <td>[lu[person]n_niŋgina[truth]n, zi[life]n, utud[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>P254866</td>\n",
       "      <td>[niŋ[thing]n_gu[neck]n_ŋar[place]v/t, niŋ[thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>P254867</td>\n",
       "      <td>[dubsar[scribe]n, mu[name]n, igi[eye]n, ni[sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>P254868</td>\n",
       "      <td>[mu[name]n, diš[one]nu, zu[know]v/t, sag[good]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>P254869</td>\n",
       "      <td>[dubsar[scribe]n, emegir[sumerian]n, inim[word...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>P254870</td>\n",
       "      <td>[dubsar[scribe]n_tur[small]v/i, bar[outside]v/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>P254871</td>\n",
       "      <td>[šah[pig]n, šu[hand]n, kar[flee]v/i, iginzu[as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>P254872</td>\n",
       "      <td>[amaʾatud[slave]n, sulum[contempt]n, er[tears]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>P254873</td>\n",
       "      <td>[dubtuku[runner]n, bar[outside]v/t, dab[seize]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_text                                          lemma_mwe\n",
       "25  P254864  [kugzu[wise]aj, namkugzu[wisdom]n, u[and]cnj, ...\n",
       "26  P254865  [lu[person]n_niŋgina[truth]n, zi[life]n, utud[...\n",
       "27  P254866  [niŋ[thing]n_gu[neck]n_ŋar[place]v/t, niŋ[thin...\n",
       "28  P254867  [dubsar[scribe]n, mu[name]n, igi[eye]n, ni[sel...\n",
       "29  P254868  [mu[name]n, diš[one]nu, zu[know]v/t, sag[good]...\n",
       "30  P254869  [dubsar[scribe]n, emegir[sumerian]n, inim[word...\n",
       "31  P254870  [dubsar[scribe]n_tur[small]v/i, bar[outside]v/...\n",
       "32  P254871  [šah[pig]n, šu[hand]n, kar[flee]v/i, iginzu[as...\n",
       "33  P254872  [amaʾatud[slave]n, sulum[contempt]n, er[tears]...\n",
       "34  P254873  [dubtuku[runner]n, bar[outside]v/t, dab[seize]..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_comp = lit_lines.groupby(\n",
    "    [lit_lines[\"id_text\"]]).aggregate(\n",
    "    {\"lemma_mwe\": sum}).reset_index()\n",
    "lit_comp['id_text'] = [id[-7:] for id in lit_comp[\"id_text\"]]\n",
    "lit_comp[25:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a DataFrame with two columns: `id_text`, and `lemma_mwe`. Each row represents a literary composition from the [epsd2/literary](http://oracc.org/epsd2/literary) corpus. Each cell in the column `lemma_mwe` contains a list with all the lemmas of one composition (with MWEs connected by underscores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.0.2 Lexical: Extract Vocabulary\n",
    "The column `lemma` in the DataFrame `lex_lines`, which was created in the last notebook represents each line in each Old Babylonian lexical text as a tuple of lemmas. Thus, the line **udu niga** in the [Old Babylonian list of Animals](http://oracc.org/dcclt/Q000001) is represented as `(udu[sheep], niga[fattened])`. In order to get the fullest representation of the lexical vocabulary, we will create the entry `udu[sheep]_niga[fattened]` as well as the entries `udu[sheep]` and `niga[fattened]`.\n",
    "\n",
    "First, a column `lemma_mwe` is added to the `lex_lines` DataFrame, connecting all lemmas in a lexical entry by an underscore and extracting all unique lexical entries in a set. Second, the list of tuples `lex_lines['lemma']` is flattened and all unique lemmas are extracted in a second set. The union of these two sets will have all individual lemmas, as well as all Multiple Word Entries. This set is turned into a list for use in `CountVectorizer()`. Entries and lemmas that contain `[na]na` are either broken or unlemmatized for some other reason and are not admitted to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a[arm]n',\n",
       " 'a[arm]n_ak[do]v/t',\n",
       " 'a[arm]n_apin[plow]n',\n",
       " 'a[arm]n_bad[open]v/t',\n",
       " 'a[arm]n_bad[wall]n',\n",
       " 'a[arm]n_badsi[parapet]n',\n",
       " 'a[arm]n_be[diminish]v/t',\n",
       " 'a[arm]n_da[line]n',\n",
       " 'a[arm]n_dabašin[object]n',\n",
       " 'a[arm]n_daluš[sling]n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_lines[\"lemma_mwe\"] = [\"_\".join(entry) for entry in lex_lines[\"lemma\"]]\n",
    "lex_vocab_a = {lemma for lemma in lex_lines[\"lemma_mwe\"] if not '[na]' in lemma}\n",
    "lex_vocab_b = {item for t in lex_lines['lemma'] for item in t if not '[na]' in item} \n",
    "lex_vocab = lex_vocab_a.union(lex_vocab_b)\n",
    "lex_vocab = list(lex_vocab) # lex_vocab is needed for Countvectorizer\n",
    "lex_vocab.sort()\n",
    "lex_vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some thoughts\n",
    "\n",
    "* Step 1. Measure length of lemma_mwe in etcsl_comp and remove rows with len < 200.\n",
    "* Step 2. Create DTM (see below) of etcsl_comp, binary = True and vocabulary = lemma_mwe from lex (use lex_lines)\n",
    "* Step 3. Order compositions by highest match\n",
    "* Step 4. Normalize for text length (from Step 1)\n",
    "* Step 5. Same process for individual lex texts (which has highest match for Ura 4?)\n",
    "* Step 6. TF-IDF\n",
    "\n",
    "In future iteration: do *not* select among lexical texts - let the script figure out which lex compositions are most relevant.\n",
    "\n",
    "Perhaps: make DTM first - show that DTM.shape gives same numbers for lex vocabulary as second Venn diagram above. Remove all columns where sum == 0. Show that DTM.shape now gives total of overlap as in Venn diagram above. Then remove rows <= minimum. Tricky!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Basic Statistics of the [epsd2/literary](http://oracc.org/epsd2/literary) Corpus\n",
    "In computing the relationship between lexical and literary vocabulary text length is playing a big role. A long text will likely have more overlap with lexical vocabulary then a very short one. The [epsd2/literary](http://oracc.org/epsd2/literary) corpus includes compositions that are known only from a fragmentary *incipit*, as well as compositions that are more than a thousand lines long.\n",
    "\n",
    "In order to meaningfully compare these compositions we will first eliminate all texts that have fewer than 200 lemmas and/or MWEs. Second, we will collect data on text length and lexical variation (how many unique lemmas are used in this text?). Dividing lexical variation by text length provides the \"Type to Token Ratio\" or TTR. \n",
    "\n",
    "TTR is generally considered to be a poor measurement for lexical richness. Short texts have higher TTR values than long texts, because longer texts will by necessity use the same words over and over again and function words such as \"the\" or \"in\" will be repeated many times whatever the lexical ingenuity of the author. A better measurement is called MTLD or Measure of Textual Lexical Diversity ([McCarthy and Jarvis 2010](https://link.springer.com/article/10.3758/BRM.42.2.381)). The MTLD value is calculated as the mean number of words in a text that will bring TTR from 1 (at the first word in the text) down to a threshold value (default is 0.720). In practice that means that a text is cut in many small units, each with approximately the same TTR - eliminating the effect of text length. This is a promising approach that may well work for Sumerian and a Python module that includes MTLD is available ([lexicalrichness](https://pypi.org/project/lexicalrichness/)). Its usage here, however, is experimental and preliminary. The threshold value is based on the observation that when going through a text sequentially the TTR in any text will drop drastically as soon as the first repeated word is encountered. At some place in the text the TTR will stabilize and drop only very gradually later on. That place is approximated by the default threshold value of 0.720. It seems likely, however, that a valid threshold value is language dependent and that a language with very few function words, such as the literary register of Sumerian, might need a lower value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4616e9ac3d40c581a1e457769de902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=179), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>lemma_mwe</th>\n",
       "      <th>length</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>ttr</th>\n",
       "      <th>mtld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>P346235</td>\n",
       "      <td>[dumu[child]n, edubbaʾa[scribal-school]n, uda[...</td>\n",
       "      <td>247</td>\n",
       "      <td>148</td>\n",
       "      <td>0.599190</td>\n",
       "      <td>106.419366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>P346257</td>\n",
       "      <td>[ud[sun]n, ul[distant]v/i, uru[sow]v/t, dumu[c...</td>\n",
       "      <td>349</td>\n",
       "      <td>188</td>\n",
       "      <td>0.538682</td>\n",
       "      <td>38.943513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>P355693</td>\n",
       "      <td>[aya[cry]n, šeš[brother]n, aya[cry]n, šeš[brot...</td>\n",
       "      <td>256</td>\n",
       "      <td>104</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>14.775079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>P357170</td>\n",
       "      <td>[nin[lady]n, nun[prince]n, gal[big]v/i, saŋ[he...</td>\n",
       "      <td>505</td>\n",
       "      <td>307</td>\n",
       "      <td>0.607921</td>\n",
       "      <td>174.090880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>P464127</td>\n",
       "      <td>[e[leave]v/i, šu[hand]n_bala[turn]v/t, ed[asce...</td>\n",
       "      <td>382</td>\n",
       "      <td>218</td>\n",
       "      <td>0.570681</td>\n",
       "      <td>78.609157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>Q000332</td>\n",
       "      <td>[iri[city]n, kug[pure]v/i, ane[he]ip, ba[allot...</td>\n",
       "      <td>1067</td>\n",
       "      <td>297</td>\n",
       "      <td>0.278351</td>\n",
       "      <td>16.450966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>Q000333</td>\n",
       "      <td>[ud[sun]n, re[that]dp, ud[sun]n, an[sky]n, ki[...</td>\n",
       "      <td>725</td>\n",
       "      <td>249</td>\n",
       "      <td>0.343448</td>\n",
       "      <td>38.251998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>Q000334</td>\n",
       "      <td>[en[lord]n, mahdi[exalted]aj, an[sky]n, ki[pla...</td>\n",
       "      <td>1941</td>\n",
       "      <td>616</td>\n",
       "      <td>0.317362</td>\n",
       "      <td>89.335280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>Q000337</td>\n",
       "      <td>[zid[right]v/i, ŋar[place]v/t, u[admiration]n_...</td>\n",
       "      <td>971</td>\n",
       "      <td>382</td>\n",
       "      <td>0.393409</td>\n",
       "      <td>143.963128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>Q000338</td>\n",
       "      <td>[eden[back]n, šugura[turban]n, men[tiara]n, ed...</td>\n",
       "      <td>2070</td>\n",
       "      <td>340</td>\n",
       "      <td>0.164251</td>\n",
       "      <td>21.064259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text                                          lemma_mwe  length  \\\n",
       "161  P346235  [dumu[child]n, edubbaʾa[scribal-school]n, uda[...     247   \n",
       "176  P346257  [ud[sun]n, ul[distant]v/i, uru[sow]v/t, dumu[c...     349   \n",
       "507  P355693  [aya[cry]n, šeš[brother]n, aya[cry]n, šeš[brot...     256   \n",
       "509  P357170  [nin[lady]n, nun[prince]n, gal[big]v/i, saŋ[he...     505   \n",
       "516  P464127  [e[leave]v/i, šu[hand]n_bala[turn]v/t, ed[asce...     382   \n",
       "531  Q000332  [iri[city]n, kug[pure]v/i, ane[he]ip, ba[allot...    1067   \n",
       "532  Q000333  [ud[sun]n, re[that]dp, ud[sun]n, an[sky]n, ki[...     725   \n",
       "533  Q000334  [en[lord]n, mahdi[exalted]aj, an[sky]n, ki[pla...    1941   \n",
       "534  Q000337  [zid[right]v/i, ŋar[place]v/t, u[admiration]n_...     971   \n",
       "535  Q000338  [eden[back]n, šugura[turban]n, men[tiara]n, ed...    2070   \n",
       "\n",
       "     lex_var       ttr        mtld  \n",
       "161      148  0.599190  106.419366  \n",
       "176      188  0.538682   38.943513  \n",
       "507      104  0.406250   14.775079  \n",
       "509      307  0.607921  174.090880  \n",
       "516      218  0.570681   78.609157  \n",
       "531      297  0.278351   16.450966  \n",
       "532      249  0.343448   38.251998  \n",
       "533      616  0.317362   89.335280  \n",
       "534      382  0.393409  143.963128  \n",
       "535      340  0.164251   21.064259  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum = 200\n",
    "lit_comp[\"length\"] = [len(lemmas) for lemmas in lit_comp[\"lemma_mwe\"]]\n",
    "lit_comp = lit_comp.loc[lit_comp.length >= minimum].copy()\n",
    "lit_comp[\"lex_var\"] = [len(set(lemmas)) for lemmas in lit_comp[\"lemma_mwe\"]]\n",
    "lit_comp[\"ttr\"] = [len(set(lemmas))/len(lemmas) for lemmas in lit_comp[\"lemma_mwe\"]]\n",
    "lit_comp['mtld'] = lit_comp['lemma_mwe'].progress_apply(lambda x: lr(x).mtld())\n",
    "lit_comp[25:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Document Term Matrix\n",
    "\n",
    "The corpus is transformed into a Document Term Matrix (or DTM), a table in which each column represents a word (or expression) that appears in a lexical text and each row represents a Sumerian composition. Each cell is a number, 0 or 1, indicating whether or not that word appears  in a particular composition. This is a binary DTM, non-binary DTMS give the number of times a word appears in a composition.\n",
    "\n",
    "Since DTMs are very commonly used in computational text analysis, it is worth spending a bit more time on various ways in which they can be created for cuneiform data. The function `CountVectorizer()` (from the `Sklearn` package) is a very flexible tool with many possible parameters. How `CountVectorizer()` and its counterpart `TfidfVectorizer()` are used depends on the structure of the input data. The most common use case is a corpus of raw documents (probably in English), each of them consisting of a text string that needs to be pre-processed and tokenized before anything else can be done. Default pre-processing includes, for instance, lowercasing the entire text. Default tokenizers assume that the text is in a modern (western) language and take spaces and punctuation marks as word dividers. Cuneiform data, whether in transliteration, lemmatization, or in normalization is much simpler than most modern language texts, because the only type of word boundary is a space (or a sequence of spaces). When using `CountVectorizer()` on transliterated, lemmatized, or normalized text we can use the parameter `token_pattern = r'[^ ]+'`, meaning \"any sequence of characters, except space.\" \n",
    "```python\n",
    "cv = CountVectorizer(token_pattern= r'[^ ]+')\n",
    "```\n",
    "A second situation is where we want to use data that is already in a list format (is already preprocessed and tokenized). This is uncommon in general, but very common for cuneiform data: all the [ORACC](http://oracc.org) and [ETCSL](http://etcsl.orinst.ox.ac.uk) data fall into that category. Rather than transforming the tokenized text back into raw strings and then tokenize those strings, we can use the parameters `tokenizer` and `preprocessor` to take care of that situation. These parameters take a function as their value, the function should return a list with tokenized text. If our input already is a list with tokenized text we may call a dummy function - a function that simply returns the list it receives. \n",
    "```python\n",
    "def dummy(l):\n",
    "    return(l)\n",
    "cv = CountVectorizer(tokenizer=dummy, preprocessor=dummy)\n",
    "```\n",
    "This will prevent `Countvectorizer()` from using a default tokenizer and preprocessor (which do not accept the list input) and it saves the trouble of untokenizing and then tokenizing again (See the [blog post](http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/) on this subject by David Batista). Instead of defining a `dummy()` function we can reach the same effect with a lambda function (see the code below).\n",
    "\n",
    "Finally, we can choose to use the `MWETokenizer()` discussed above (section ###). The `MWETokenizer()` expects a tokenized text (a list) and re-tokenizes that text by using a list of pre-defined Multiple Word Expressions, returning a new list. In case we use the original [epsd2/literary](http://oracc.org/epsd2/literary) data, in which the MWEs have not yet been marked, we can do the CountVectorizing and marking the MWEs in one go, as follows:\n",
    "```python\n",
    "def dummy(l):\n",
    "    return(l)\n",
    "tokenizer = MWETokenizer(lex_mwe) # initialize the tokenizer with the lexical MWEs\n",
    "cv = CountVectorizer(tokenizer=tokenizer.tokenize, preprocessor=dummy)\n",
    "```\n",
    "For our current purposes the best approach is to use a dummy tokenizer and preprocessor. The disadvantage of using the MWETokenizer on entire texts is that it will not honor line boundaries. See, for instance, Gilgameš and Huwawa 50-51 (text and translation [ETCSL](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=t.1.8.1.5&display=Crit&charenc=gcirc#)): \n",
    "\n",
    "> ama tuku ama-a-ni-še₃\\\n",
    "> nitah saŋ-dili ŋe₂₆-e-gin₇ ak a₂-ŋu₁₀-še₃ hu-mu-un-ak\\\n",
    "> \"Let him who has a mother go to his mother! \\\n",
    "> Let bachelor males, types like me, join me at my side!\"\n",
    "\n",
    "This will result in the Multiple Word Expression ama\\[mother\\]n_nita\\[male\\]N, an expression found in the list of human beings Lu ([OB Nippur Lu](http://oracc.org/dcclt/Q000047.351)), which is clearly not applicable here. The number of such errors, however, is fairly small (about 6 for a corpus of almost 400 texts). For other types of texts, where line boundaries are less significant, this method may well be an efficient way of doing things.\n",
    "\n",
    "The CountVectorizer is now applied to the corpus and the result is transformed into a new Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a[arm]n</th>\n",
       "      <th>a[arm]n_ak[do]v/t</th>\n",
       "      <th>a[arm]n_apin[plow]n</th>\n",
       "      <th>a[arm]n_bad[open]v/t</th>\n",
       "      <th>a[arm]n_bad[wall]n</th>\n",
       "      <th>a[arm]n_badsi[parapet]n</th>\n",
       "      <th>a[arm]n_be[diminish]v/t</th>\n",
       "      <th>a[arm]n_da[line]n</th>\n",
       "      <th>a[arm]n_dabašin[object]n</th>\n",
       "      <th>a[arm]n_daluš[sling]n</th>\n",
       "      <th>...</th>\n",
       "      <th>šuʾabdu[1]wn</th>\n",
       "      <th>šuʾi[barber]n</th>\n",
       "      <th>šuʾi[barber]n_egir[back]n</th>\n",
       "      <th>šuʾi[barber]n_gin[firm]v/i</th>\n",
       "      <th>šuʾi[barber]n_gina[offering]n</th>\n",
       "      <th>šuʾi[barber]n_gu[neck]n</th>\n",
       "      <th>šuʾi[barber]n_lugal[king]n</th>\n",
       "      <th>šuʾi[barber]n_saŋ[head]n</th>\n",
       "      <th>šuʾu[stone]n</th>\n",
       "      <th>šuʾura[goose]n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>P251713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>P252270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>P252333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>P346086</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>P346087</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q000818</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q000821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q000823</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q000825</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>X010001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 10199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         a[arm]n  a[arm]n_ak[do]v/t  a[arm]n_apin[plow]n  \\\n",
       "id_text                                                    \n",
       "P251713        0                  0                    0   \n",
       "P252270        0                  0                    0   \n",
       "P252333        1                  0                    0   \n",
       "P346086        1                  0                    0   \n",
       "P346087        1                  0                    0   \n",
       "...          ...                ...                  ...   \n",
       "Q000818        1                  0                    0   \n",
       "Q000821        1                  0                    0   \n",
       "Q000823        1                  0                    0   \n",
       "Q000825        1                  0                    0   \n",
       "X010001        0                  0                    0   \n",
       "\n",
       "         a[arm]n_bad[open]v/t  a[arm]n_bad[wall]n  a[arm]n_badsi[parapet]n  \\\n",
       "id_text                                                                      \n",
       "P251713                     0                   0                        0   \n",
       "P252270                     0                   0                        0   \n",
       "P252333                     0                   0                        0   \n",
       "P346086                     0                   0                        0   \n",
       "P346087                     0                   0                        0   \n",
       "...                       ...                 ...                      ...   \n",
       "Q000818                     0                   0                        0   \n",
       "Q000821                     0                   0                        0   \n",
       "Q000823                     0                   0                        0   \n",
       "Q000825                     0                   0                        0   \n",
       "X010001                     0                   0                        0   \n",
       "\n",
       "         a[arm]n_be[diminish]v/t  a[arm]n_da[line]n  a[arm]n_dabašin[object]n  \\\n",
       "id_text                                                                         \n",
       "P251713                        0                  0                         0   \n",
       "P252270                        0                  0                         0   \n",
       "P252333                        0                  0                         0   \n",
       "P346086                        0                  0                         0   \n",
       "P346087                        0                  0                         0   \n",
       "...                          ...                ...                       ...   \n",
       "Q000818                        0                  0                         0   \n",
       "Q000821                        0                  0                         0   \n",
       "Q000823                        0                  0                         0   \n",
       "Q000825                        0                  0                         0   \n",
       "X010001                        0                  0                         0   \n",
       "\n",
       "         a[arm]n_daluš[sling]n  ...  šuʾabdu[1]wn  šuʾi[barber]n  \\\n",
       "id_text                         ...                                \n",
       "P251713                      0  ...             0              0   \n",
       "P252270                      0  ...             0              0   \n",
       "P252333                      0  ...             0              0   \n",
       "P346086                      0  ...             0              0   \n",
       "P346087                      0  ...             0              0   \n",
       "...                        ...  ...           ...            ...   \n",
       "Q000818                      0  ...             0              0   \n",
       "Q000821                      0  ...             0              0   \n",
       "Q000823                      0  ...             0              0   \n",
       "Q000825                      0  ...             0              0   \n",
       "X010001                      0  ...             0              0   \n",
       "\n",
       "         šuʾi[barber]n_egir[back]n  šuʾi[barber]n_gin[firm]v/i  \\\n",
       "id_text                                                          \n",
       "P251713                          0                           0   \n",
       "P252270                          0                           0   \n",
       "P252333                          0                           0   \n",
       "P346086                          0                           0   \n",
       "P346087                          0                           0   \n",
       "...                            ...                         ...   \n",
       "Q000818                          0                           0   \n",
       "Q000821                          0                           0   \n",
       "Q000823                          0                           0   \n",
       "Q000825                          0                           0   \n",
       "X010001                          0                           0   \n",
       "\n",
       "         šuʾi[barber]n_gina[offering]n  šuʾi[barber]n_gu[neck]n  \\\n",
       "id_text                                                           \n",
       "P251713                              0                        0   \n",
       "P252270                              0                        0   \n",
       "P252333                              0                        0   \n",
       "P346086                              0                        0   \n",
       "P346087                              0                        0   \n",
       "...                                ...                      ...   \n",
       "Q000818                              0                        0   \n",
       "Q000821                              0                        0   \n",
       "Q000823                              0                        0   \n",
       "Q000825                              0                        0   \n",
       "X010001                              0                        0   \n",
       "\n",
       "         šuʾi[barber]n_lugal[king]n  šuʾi[barber]n_saŋ[head]n  šuʾu[stone]n  \\\n",
       "id_text                                                                       \n",
       "P251713                           0                         0             0   \n",
       "P252270                           0                         0             0   \n",
       "P252333                           0                         0             0   \n",
       "P346086                           0                         0             0   \n",
       "P346087                           0                         0             0   \n",
       "...                             ...                       ...           ...   \n",
       "Q000818                           0                         0             0   \n",
       "Q000821                           0                         0             0   \n",
       "Q000823                           0                         0             0   \n",
       "Q000825                           0                         0             0   \n",
       "X010001                           0                         0             0   \n",
       "\n",
       "         šuʾura[goose]n  \n",
       "id_text                  \n",
       "P251713               0  \n",
       "P252270               0  \n",
       "P252333               0  \n",
       "P346086               0  \n",
       "P346087               0  \n",
       "...                 ...  \n",
       "Q000818               0  \n",
       "Q000821               0  \n",
       "Q000823               0  \n",
       "Q000825               0  \n",
       "X010001               0  \n",
       "\n",
       "[179 rows x 10199 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, vocabulary=lex_vocab, binary=True)\n",
    "\n",
    "dtm = cv.fit_transform(lit_comp['lemma_mwe'])\n",
    "lit_df = pd.DataFrame(dtm.toarray(), columns= cv.get_feature_names(), index=lit_comp[\"id_text\"])\n",
    "lit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame lit_df has a row for each *literary* composition (excluding those with fewer than 200 lemmas) and it has a column for every lemma/expression in the *lexical* corpus. As we have seen in the previous notebook, many of these words/expressions do not appear in the [ETCSL](http://etcsl.orinst.ox.ac.uk) corpus, and thus all cells in that column are 0. The other way around, there are many words in the literary corpus that do not appear in lexical texts, and those words are not represented at all in this DTM. This DTM, therefore, should only be used to research *overlap* between the two (literary and lexical) vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Lexical/Literary Matches per Literary Composition. \n",
    "Since the DTM was built with the option `binary = True` the sum of each row equals the number of unique words/expressions that the composition shares with the lexical corpus. The code in the cell below may be simplified as:\n",
    "```python\n",
    "lit_df[\"n_matches\"] = lit_df.sum(axis=1)\n",
    "```\n",
    "which will yield exactly the same result. The extra elements in the code are added for two reasons. First, if we add additional columns to the DataFrame, for instance composition names, the code will fail unless we add the option `numeric_only = True`. Second, if the (simplified) code is run twice, even with the option `numeric_only=True` the column `n_matches` will become part of the summation and the result in the new `n_matches` column will be twice the correct outcome. By explicitly stating that only the columns named after the lemmas in `lex_vocab` should be used such accidents are avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df[\"n_matches\"] = lit_df[lex_vocab].sum(axis=1, numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns from `lit_comp` by using merge on `id_text`. The merge method is \"inner,\" which means that only those rows that exist in both DataFrames will end up in the new DataFrame. Thus we ensure that the short compositions (which are in `lit_comp` but not in `lit_df`) are not part of the merged DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"jsonzip/epsd2-literary.zip\"\n",
    "z = zipfile.ZipFile(file) \n",
    "st = z.read(\"epsd2/literary/catalogue.json\").decode(\"utf-8\")\n",
    "j = json.loads(st)\n",
    "cat_df = pd.DataFrame(j[\"members\"]).T\n",
    "cat_df.loc[cat_df.designation.str[:13] == \"CDLI Literary\", \"designation\"] = cat_df.subgenre\n",
    "cat_df[\"id_text\"] = cat_df[\"id_text\"].fillna(cat_df[\"id_composite\"])\n",
    "cat_df = cat_df[[\"id_text\", \"designation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>designation</th>\n",
       "      <th>length</th>\n",
       "      <th>mtld</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>n_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>P251713</td>\n",
       "      <td>CUSAS 38, 05</td>\n",
       "      <td>268</td>\n",
       "      <td>89.333333</td>\n",
       "      <td>0.555970</td>\n",
       "      <td>149</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>P252270</td>\n",
       "      <td>ETCSL 4.02.01 Baba A (witness)</td>\n",
       "      <td>210</td>\n",
       "      <td>189.249012</td>\n",
       "      <td>0.695238</td>\n",
       "      <td>146</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>P252333</td>\n",
       "      <td>ETCSL nn szir3-nam-szub-ba {d}en-ki-ka3-kam (w...</td>\n",
       "      <td>396</td>\n",
       "      <td>13.979005</td>\n",
       "      <td>0.371212</td>\n",
       "      <td>147</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>P346086</td>\n",
       "      <td>ETCSL 1.01.01 Enki and Ninhursaga (witness)</td>\n",
       "      <td>201</td>\n",
       "      <td>23.366089</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>117</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>P346087</td>\n",
       "      <td>ETCSL 1.06.03 Ninurta and Turtle (witness)</td>\n",
       "      <td>272</td>\n",
       "      <td>63.826897</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>148</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>Q000818</td>\n",
       "      <td>Proverbs: collection 26</td>\n",
       "      <td>242</td>\n",
       "      <td>82.879522</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>155</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>Q000821</td>\n",
       "      <td>Proverbs: from Nippur</td>\n",
       "      <td>413</td>\n",
       "      <td>176.549957</td>\n",
       "      <td>0.607748</td>\n",
       "      <td>251</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>Q000823</td>\n",
       "      <td>Proverbs: from Ur</td>\n",
       "      <td>1400</td>\n",
       "      <td>113.183842</td>\n",
       "      <td>0.377143</td>\n",
       "      <td>528</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>Q000825</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>538</td>\n",
       "      <td>80.986829</td>\n",
       "      <td>0.557621</td>\n",
       "      <td>300</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>X010001</td>\n",
       "      <td>Saeedi 0212</td>\n",
       "      <td>255</td>\n",
       "      <td>136.057036</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>156</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text                                        designation  length  \\\n",
       "0    P251713                                       CUSAS 38, 05     268   \n",
       "1    P252270                     ETCSL 4.02.01 Baba A (witness)     210   \n",
       "2    P252333  ETCSL nn szir3-nam-szub-ba {d}en-ki-ka3-kam (w...     396   \n",
       "3    P346086        ETCSL 1.01.01 Enki and Ninhursaga (witness)     201   \n",
       "4    P346087         ETCSL 1.06.03 Ninurta and Turtle (witness)     272   \n",
       "..       ...                                                ...     ...   \n",
       "174  Q000818                            Proverbs: collection 26     242   \n",
       "175  Q000821                              Proverbs: from Nippur     413   \n",
       "176  Q000823                                  Proverbs: from Ur    1400   \n",
       "177  Q000825                    Proverbs: of unknown provenance     538   \n",
       "178  X010001                                        Saeedi 0212     255   \n",
       "\n",
       "           mtld       ttr  lex_var  n_matches  \n",
       "0     89.333333  0.555970      149        129  \n",
       "1    189.249012  0.695238      146        137  \n",
       "2     13.979005  0.371212      147        126  \n",
       "3     23.366089  0.582090      117        107  \n",
       "4     63.826897  0.544118      148        139  \n",
       "..          ...       ...      ...        ...  \n",
       "174   82.879522  0.640496      155        149  \n",
       "175  176.549957  0.607748      251        225  \n",
       "176  113.183842  0.377143      528        478  \n",
       "177   80.986829  0.557621      300        280  \n",
       "178  136.057036  0.611765      156        140  \n",
       "\n",
       "[179 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df2 = pd.merge(lit_comp[[\"id_text\", \"length\", \"mtld\", \"ttr\", \"lex_var\"]], lit_df[\"n_matches\"], on=\"id_text\", how=\"inner\")\n",
    "lit_df2 = pd.merge(cat_df, lit_df2, on = 'id_text', how = 'inner')\n",
    "lit_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>designation</th>\n",
       "      <th>length</th>\n",
       "      <th>mtld</th>\n",
       "      <th>ttr</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>n_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>Q000351</td>\n",
       "      <td>Ninurta's exploits: a šir-sud (?) to Ninurta</td>\n",
       "      <td>3140</td>\n",
       "      <td>147.114580</td>\n",
       "      <td>0.261783</td>\n",
       "      <td>822</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>Q000380</td>\n",
       "      <td>The lament for Sumer and Ur</td>\n",
       "      <td>2667</td>\n",
       "      <td>55.814135</td>\n",
       "      <td>0.274091</td>\n",
       "      <td>731</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>Q000367</td>\n",
       "      <td>Lugalbanda in the mountain cave</td>\n",
       "      <td>1943</td>\n",
       "      <td>74.765008</td>\n",
       "      <td>0.326814</td>\n",
       "      <td>635</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>Q000750</td>\n",
       "      <td>The temple hymns</td>\n",
       "      <td>2498</td>\n",
       "      <td>86.039276</td>\n",
       "      <td>0.279824</td>\n",
       "      <td>699</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>Q000334</td>\n",
       "      <td>Enki and the world order</td>\n",
       "      <td>1941</td>\n",
       "      <td>89.335280</td>\n",
       "      <td>0.317362</td>\n",
       "      <td>616</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text                                   designation  length  \\\n",
       "42   Q000351  Ninurta's exploits: a šir-sud (?) to Ninurta    3140   \n",
       "57   Q000380                   The lament for Sumer and Ur    2667   \n",
       "50   Q000367               Lugalbanda in the mountain cave    1943   \n",
       "150  Q000750                              The temple hymns    2498   \n",
       "32   Q000334                      Enki and the world order    1941   \n",
       "\n",
       "           mtld       ttr  lex_var  n_matches  \n",
       "42   147.114580  0.261783      822        706  \n",
       "57    55.814135  0.274091      731        609  \n",
       "50    74.765008  0.326814      635        576  \n",
       "150   86.039276  0.279824      699        566  \n",
       "32    89.335280  0.317362      616        553  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df2 = lit_df2.sort_values(by = \"n_matches\", na_position=\"first\", ascending=False)\n",
    "lit_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "Lugal-e (or Ninurta's Exploits) has the highest number of matches (705) with the Old Babylonian lexical corpus in [DCCLT](http://oracc.org/dcclt). But this is also the longest composition in the corpus. We can normalize by dividing the total number of matches by the number of unique lemmas in the text (`norm`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df2[\"norm\"] = lit_df2[\"n_matches\"] / lit_df2[\"lex_var\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Results\n",
    "The following code displays the result in an interactive table that may be sorted (ascending or descending) in different ways for further exploration. The column `id_text` provides links to the editions in [epsd2/literary](http://oracc.org/epsd2/literary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.org/epsd2/literary/{}\", target=\"_blank\">{}</a>'\n",
    "lit = lit_df2.copy()\n",
    "lit['id_text'] = [anchor.format(val,val) for val in lit['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441fbf8455b04a1b8304c6fb57bb4841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='col', index=7, options=('id_text', 'designation', 'length', 'mtld'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(col = lit.columns, rows = (1, len(lit), 1))\n",
    "def sort_df(col = \"norm\", ascending = False, rows = 10):\n",
    "    return lit.sort_values(by = col, ascending = ascending).reset_index(drop=True)[:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Some Viz\n",
    "Provisional. Mainly as example. Save the figures by opening an Output View (right click on output) and then right click on that Output View, select Save As."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbins = 5\n",
    "column = 'norm'\n",
    "fig, ax = plt.subplots()\n",
    "counts, bins, patches = ax.hist(lit[column], bins = nbins, edgecolor='k', color = \"#5d92dd\")\n",
    "ax.set_xticks(bins)\n",
    "for i in range(len(bins) - 1):\n",
    "    plt.text(bins[i],counts[i]/2,str(counts[i]), fontsize = 16, color = '#fdb515')\n",
    "plt.ylabel('No. of Compositions')\n",
    "plt.xlabel(column)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative. Much simpler - but does not return the bins and the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit.mtld.hist(bins = 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit.plot.scatter(x = 'length', y = 'ttr', figsize = (10, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Creating Output Only\n",
    "The following code is used to create MarkDown tables from Pandas DataFrames. The tables can be included in the Compass Markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl_tab = etcsl_df2.copy()\n",
    "markdown = \"[{}](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text={}&display=Crit&charenc=gcirc#)\"\n",
    "etcsl_tab['id_text'] = [markdown.format(val,val) for val in etcsl_df2['id_text']]\n",
    "etcsl_tab = etcsl_tab.round({'ttr' : 3, 'norm': 3, 'mtld' : 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 10 # number of rows to be exported\n",
    "col = 'norm' # column by which to sort\n",
    "asc = True\n",
    "tab = tabulate(etcsl_tab.sort_values(by=col, ascending=asc)[:rows],\n",
    "         headers= etcsl_tab.columns , tablefmt=\"github\", showindex=False)\n",
    "with open('output/etcsl_tab.txt', 'w', encoding='utf8') as w:\n",
    "    w.write(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for id in etcsl_comp['id_text']:\n",
    "    c = etcsl_comp.loc[etcsl_comp['id_text'] == id, 'lemma_mwe']\n",
    "    c = c.iloc[0]\n",
    "\n",
    "    ttr_l = []\n",
    "    enum = range(1, len(c))\n",
    "    for ind in enum:\n",
    "        t = c[:ind]\n",
    "        ttr = lr(t).ttr\n",
    "        ttr_l.append(ttr)\n",
    "    plt.plot(enum, ttr_l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl_df2.loc[(86 < etcsl_df2.mtld) & (etcsl_df2.mtld < 162.6)].sort_values(by = 'mtld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetrad = {'c.2.5.8.1' : 1, 'c.2.5.3.2' : 1, 'c.2.5.5.2' : 1, 'c.4.16.1': 1}\n",
    "decad = {'c.2.4.2.01' : 2, 'c.2.5.5.1' : 2, 'c.5.5.4' : 2, 'c.4.07.2' : 2, 'c.4.05.1' : 2,\n",
    "         'c.4.80.2' : 2, 'c.1.1.4' : 2, 'c.1.3.2' : 2, 'c.4.28.1' : 2, 'c.1.8.1.5' : 2}\n",
    "houseF = {'c.5.1.2' : 3,'c.5.1.3' : 3, 'c.1.8.1.4' : 3, 'c.1.6.2' : 3, 'c.2.1.5' : 3,\n",
    "          'c.2.4.2.02' : 3, 'c.2.2.2' : 3, 'c.5.6.1' : 3, 'c.5.1.1' : 3, 'c.5.3.2' : 3,\n",
    "          'c.1.4.3' : 3, 'c.5.6.3' : 3, 'c.5.4.1' : 3, 'c.5.3.1' : 3}\n",
    "proverbs = {'c.6.1.01' : 4, 'c.6.1.02' : 4, 'c.6.1.03' : 4, 'c.6.1.04' : 4, 'c.6.1.05' : 4,\n",
    "            'c.6.1.06' : 4, 'c.6.1.07' : 4,'c.6.1.08' : 4, 'c.6.1.09' : 4, 'c.6.1.10' : 4,\n",
    "            'c.6.1.11' : 4, 'c.6.1.12' : 4, 'c.6.1.13' : 4, 'c.6.1.14' : 4,'c.6.1.15' : 4,\n",
    "            'c.6.1.16' : 4, 'c.6.1.17' : 4, 'c.6.1.18' : 4, 'c.6.1.19' : 4, 'c.6.1.20' : 4,\n",
    "            'c.6.1.21' : 4, 'c.6.1.22' : 4, 'c.6.1.23' : 4, 'c.6.1.24' : 4, 'c.6.1.25' : 4,\n",
    "            'c.6.1.26' : 4, 'c.6.1.27' : 4, 'c.6.1.28' : 4, 'c.6.2.1' : 4, 'c.6.2.2' : 4,\n",
    "            'c.6.2.3' : 4,'c.6.2.4' : 4,'c.6.2.5' : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educL = {}\n",
    "educL.update(tetrad)\n",
    "educL.update(decad)\n",
    "educL.update(houseF)\n",
    "educL.update(proverbs)\n",
    "educ = etcsl_df2.loc[etcsl_df2.id_text.isin(educL)].sort_values(by = 'norm')\n",
    "educ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educ['category'] = [educL[id] for id in educ.id_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educ.sort_values(by = 'mtld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl.norm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl.loc[round(etcsl.norm, 3) == 0.874].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#colors = {1 :'red', 2:'blue', 3:'green', 4:'black'}\n",
    "#plt.scatter(educ.norm, educ.mtld, s =75, c=educ['category'].apply(lambda x: colors[x]), alpha = 1)\n",
    "sns.scatterplot('norm', 'mtld', data=educ, hue='category', size = 'length', sizes = (50, 200), alpha = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educL['c.2.5.5.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hymns = etcsl_df2.loc[etcsl_df2.id_text.str.startswith('c.2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hymns.sort_values(by = 'id_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = etcsl_df2.groupby(etcsl_df2.id_text.str[:5]).aggregate({'norm' : 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl[['mtld', 'length', 'norm', 'lex_var', 'ttr', 'n_matches']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD = set(etcsl_comp.lemma_mwe.iloc[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DD - set(lex_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl_comp.iloc[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl_df2.loc[etcsl_df2.id_text == 'c.1.4.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl.norm.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
