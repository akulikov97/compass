{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Overlap in Lexical and Literary Vocabulary: Digging Deeper\n",
    "\n",
    "In order to research the relationship between lexical and literary vocabularies in more detail we will look at individual literary texts. Which compositions have more and which have less overlap with the lexical vocabulary?\n",
    "\n",
    "Longer texts will have more vocabulary items (and Multiple Word Expressions) in common with the lexical corpus than shorter texts, but that does not mean much. We will therefore compare literary compositions by means of a normalized measure.\n",
    "\n",
    "### 3.2.0 Preparation\n",
    "\n",
    "This notebook uses some files that were downloaded or produced in [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb). Run that notebook first, before this one. \n",
    "\n",
    "First import the necessary libraries. If you are running this notebook in Jupyter Lab you will need to install the Jupyter Lab ipywidgets extension (see [Introduction](../1_Preliminaries/1_Introduction.md), section 1.2.2.1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import zipfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file `litlines.p` which was produced in [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb). The file contains the pickled version of the DataFrame `lit_lines` in which the literary ([epsd2/literary](http://oracc.org/epsd2/literary)) corpus is represented in line-by-line format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lines = pd.read_pickle('output/litlines.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.0.1 Literary: By Composition\n",
    "The line-by-line representation that was prepared in the previous notebook will be transformed into a composition-by-composition representation. The DataFrame `lit_lines` includes the column `lemma_mwe` in which each line is represented as a sequence of lemmas and/or Multiple Word Expressions (lemmas connected by underscores). The `pandas` `groupby()` function is used here to group on `id_text` and `text_name`. The aggregate function for the `lemma_mwe` column in this case is simply `' '.join`: all the entries (representing lines) are concatenated to form one long sequence of lemmas in a single string representing one composition.\n",
    "\n",
    "The field `id_text` in the resulting DataFrame has the form 'epsd2/literary/P254863'. In fact, we only need the last 7 characters of that string (the P, Q, or X number of the text), because all texts derive from the same project. We can simplify the `id_text` string with a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>lemma_mwe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>P254863</td>\n",
       "      <td>iri[city]n silim[healthy]v/i tag[touch]v/t lul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>P254864</td>\n",
       "      <td>kugzu[wise]aj namkugzu[wisdom]n na-an-ak-x[na]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P254865</td>\n",
       "      <td>lu[person]n_niŋgina[truth]n zi[life]n utud[bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P254866</td>\n",
       "      <td>niŋ[thing]n_gu[neck]n_ŋar[place]v/t niŋ[thing]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>P254867</td>\n",
       "      <td>dubsar[scribe]n mu[name]n ni₂-x[na]na igi[eye]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P254868</td>\n",
       "      <td>x-x[na]na mu[name]n diš[one]nu zu[know]v/t x-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>P254869</td>\n",
       "      <td>dubsar[scribe]n emegir[sumerian]n nu-un-zu-x[n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>P254870</td>\n",
       "      <td>dubsar[scribe]n_tur[small]v/i bar[outside]n ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>P254871</td>\n",
       "      <td>šah[pig]n šu[hand]n kar[flee]v/i iginzu[as-if]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>P254872</td>\n",
       "      <td>amaʾatud[slave]n sulum[contempt]n in-na-x[na]n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_text                                          lemma_mwe\n",
       "25  P254863  iri[city]n silim[healthy]v/i tag[touch]v/t lul...\n",
       "26  P254864  kugzu[wise]aj namkugzu[wisdom]n na-an-ak-x[na]...\n",
       "27  P254865  lu[person]n_niŋgina[truth]n zi[life]n utud[bea...\n",
       "28  P254866  niŋ[thing]n_gu[neck]n_ŋar[place]v/t niŋ[thing]...\n",
       "29  P254867  dubsar[scribe]n mu[name]n ni₂-x[na]na igi[eye]...\n",
       "30  P254868  x-x[na]na mu[name]n diš[one]nu zu[know]v/t x-n...\n",
       "31  P254869  dubsar[scribe]n emegir[sumerian]n nu-un-zu-x[n...\n",
       "32  P254870  dubsar[scribe]n_tur[small]v/i bar[outside]n ni...\n",
       "33  P254871  šah[pig]n šu[hand]n kar[flee]v/i iginzu[as-if]...\n",
       "34  P254872  amaʾatud[slave]n sulum[contempt]n in-na-x[na]n..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_comp = lit_lines.groupby(\n",
    "    [lit_lines[\"id_text\"]]).aggregate(\n",
    "    {\"lemma_mwe\": ' '.join}).reset_index()\n",
    "lit_comp['id_text'] = [id[-7:] for id in lit_comp[\"id_text\"]]\n",
    "lit_comp[25:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a DataFrame with two columns: `id_text`, and `lemma_mwe`. Each row represents a literary composition from the [epsd2/literary](http://oracc.org/epsd2/literary) corpus. Each cell in the column `lemma_mwe` contains a sequence of lemmas of one composition (with MWEs connected by underscores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.0.2 Text Length and Number of Unique Lemmas\n",
    "For each literary composition we need to know the text length and the number of unique lemmas. We will use these numbers to weed out documents that are to short and to normalize the number of hits (the number of lemmas shared with the lexical corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lit_stats(lemmas):\n",
    "    lemmas = lemmas.split()\n",
    "    lemmas = [lemma for lemma in lemmas if not '[na]na' in lemma] # remove unlemmatized words\n",
    "    length = len(lemmas) # number of lemmatized words\n",
    "    lex_var = len(set(lemmas))\n",
    "    return ' '.join(lemmas), length, lex_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9802a8f6ab444448885f138acf470f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=913.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lit_comp['lemma_mwe'], lit_comp['length'], lit_comp['lex_var'] = \\\n",
    "    zip(*lit_comp['lemma_mwe'].progress_map(lit_stats))\n",
    "lit_comp = lit_comp.loc[lit_comp['length'] > 0] # remove compositions that have no lemmatized content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Document Term Matrix\n",
    "\n",
    "The literary corpus is transformed into a Document Term Matrix (or DTM), a table in which each column represents a lemma and each row represents a Sumerian composition. Each cell contains a number indicating the frequency of that word  in a particular composition.\n",
    "\n",
    "Since we are interested in the usage of lexical vocabulary in literary texts, we may skip all words that are not available in the lexical corpus, saving a considerable amount of memory. We can do that by defining a vocabulary, derived from the data produced in the previous notebook ([3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb)). The vocabulary is sorted (by alphabet) so that the columns in the DTM will be in alphabetical order as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/lex_vocab.txt', 'r', encoding = 'utf8') as r:\n",
    "    lex_vocab = r.read().splitlines()\n",
    "lex_vocab.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the DTM we use the function `CountVectorizer()` (from the `Sklearn` package), a very flexible tool with many possible parameters. The most common use case is a corpus of raw documents (probably in English), each of them consisting of a text string that needs to be pre-processed  and tokenized (turned into a list of words or lemmas) before anything else can be done. Default pre-processing includes, for instance, lowercasing the entire text (so that thursday, Thursday, and THURSDAY will all be recognized as the same lemma) and removal of punctuation and numbers. Default tokenizers assume that the text is in a modern (western) language, taking spaces, hyphens, and punctuation marks as word dividers. The structure of the [ORACC](http://oracc.org) data is much simpler than that. Pre-processing is unnecessary, and tokenization should split the string *only* at blank spaces.\n",
    "\n",
    "This can be achieved by defining custom tokenizer/preprocessor functions, and tell `Countvectorizer()` to use these. The custom tokenizer consists of the standard Python function `split()`; the preprocessor function does nothing at all.\n",
    "\n",
    "The parameter `vocabulary` is set to the variable `lex_vocab` (created above), which includes all lemmas and lexical entries in the lexical corpus. \n",
    "\n",
    "`CountVectorizer()` stores the results in a sparse matrix which notes the position and value of non-zero entries. In transforming the output to a Pandas DataFrame, we need to use the `toarray()` method, which will transform the sparse matrix into a regular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a[arm]n</th>\n",
       "      <th>a[arm]n_ak[do]v/t</th>\n",
       "      <th>a[arm]n_apin[plow]n</th>\n",
       "      <th>a[arm]n_bad[open]v/t</th>\n",
       "      <th>a[arm]n_bad[wall]n</th>\n",
       "      <th>a[arm]n_badsi[parapet]n</th>\n",
       "      <th>a[arm]n_bala[turn]v/t</th>\n",
       "      <th>a[arm]n_be[diminish]v/t</th>\n",
       "      <th>a[arm]n_da[line]n</th>\n",
       "      <th>a[arm]n_dabašin[object]n</th>\n",
       "      <th>...</th>\n",
       "      <th>šuʾabdu[1]wn</th>\n",
       "      <th>šuʾi[barber]n</th>\n",
       "      <th>šuʾi[barber]n_egir[back]n</th>\n",
       "      <th>šuʾi[barber]n_gin[firm]v/i</th>\n",
       "      <th>šuʾi[barber]n_gina[offering]n</th>\n",
       "      <th>šuʾi[barber]n_gu[neck]n</th>\n",
       "      <th>šuʾi[barber]n_lugal[king]n</th>\n",
       "      <th>šuʾi[barber]n_saŋ[head]n</th>\n",
       "      <th>šuʾu[stone]n</th>\n",
       "      <th>šuʾura[goose]n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P209784</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251427</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251728</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P252215</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000823</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000825</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q002338</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X010001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>902 rows × 10362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         a[arm]n  a[arm]n_ak[do]v/t  a[arm]n_apin[plow]n  \\\n",
       "id_text                                                    \n",
       "P209784        0                  0                    0   \n",
       "P251427        0                  0                    0   \n",
       "P251713        0                  0                    0   \n",
       "P251728        0                  0                    0   \n",
       "P252215        1                  0                    0   \n",
       "...          ...                ...                  ...   \n",
       "Q000823        4                  0                    0   \n",
       "Q000824        0                  0                    0   \n",
       "Q000825        2                  0                    0   \n",
       "Q002338        0                  0                    0   \n",
       "X010001        0                  0                    0   \n",
       "\n",
       "         a[arm]n_bad[open]v/t  a[arm]n_bad[wall]n  a[arm]n_badsi[parapet]n  \\\n",
       "id_text                                                                      \n",
       "P209784                     0                   0                        0   \n",
       "P251427                     0                   0                        0   \n",
       "P251713                     0                   0                        0   \n",
       "P251728                     0                   0                        0   \n",
       "P252215                     0                   0                        0   \n",
       "...                       ...                 ...                      ...   \n",
       "Q000823                     0                   0                        0   \n",
       "Q000824                     0                   0                        0   \n",
       "Q000825                     0                   0                        0   \n",
       "Q002338                     0                   0                        0   \n",
       "X010001                     0                   0                        0   \n",
       "\n",
       "         a[arm]n_bala[turn]v/t  a[arm]n_be[diminish]v/t  a[arm]n_da[line]n  \\\n",
       "id_text                                                                      \n",
       "P209784                      0                        0                  0   \n",
       "P251427                      0                        0                  0   \n",
       "P251713                      0                        0                  0   \n",
       "P251728                      0                        0                  0   \n",
       "P252215                      0                        0                  0   \n",
       "...                        ...                      ...                ...   \n",
       "Q000823                      0                        0                  0   \n",
       "Q000824                      0                        0                  0   \n",
       "Q000825                      0                        0                  0   \n",
       "Q002338                      0                        0                  0   \n",
       "X010001                      0                        0                  0   \n",
       "\n",
       "         a[arm]n_dabašin[object]n  ...  šuʾabdu[1]wn  šuʾi[barber]n  \\\n",
       "id_text                            ...                                \n",
       "P209784                         0  ...             0              0   \n",
       "P251427                         0  ...             0              0   \n",
       "P251713                         0  ...             0              0   \n",
       "P251728                         0  ...             0              0   \n",
       "P252215                         0  ...             0              0   \n",
       "...                           ...  ...           ...            ...   \n",
       "Q000823                         0  ...             0              0   \n",
       "Q000824                         0  ...             0              0   \n",
       "Q000825                         0  ...             0              0   \n",
       "Q002338                         0  ...             0              0   \n",
       "X010001                         0  ...             0              0   \n",
       "\n",
       "         šuʾi[barber]n_egir[back]n  šuʾi[barber]n_gin[firm]v/i  \\\n",
       "id_text                                                          \n",
       "P209784                          0                           0   \n",
       "P251427                          0                           0   \n",
       "P251713                          0                           0   \n",
       "P251728                          0                           0   \n",
       "P252215                          0                           0   \n",
       "...                            ...                         ...   \n",
       "Q000823                          0                           0   \n",
       "Q000824                          0                           0   \n",
       "Q000825                          0                           0   \n",
       "Q002338                          0                           0   \n",
       "X010001                          0                           0   \n",
       "\n",
       "         šuʾi[barber]n_gina[offering]n  šuʾi[barber]n_gu[neck]n  \\\n",
       "id_text                                                           \n",
       "P209784                              0                        0   \n",
       "P251427                              0                        0   \n",
       "P251713                              0                        0   \n",
       "P251728                              0                        0   \n",
       "P252215                              0                        0   \n",
       "...                                ...                      ...   \n",
       "Q000823                              0                        0   \n",
       "Q000824                              0                        0   \n",
       "Q000825                              0                        0   \n",
       "Q002338                              0                        0   \n",
       "X010001                              0                        0   \n",
       "\n",
       "         šuʾi[barber]n_lugal[king]n  šuʾi[barber]n_saŋ[head]n  šuʾu[stone]n  \\\n",
       "id_text                                                                       \n",
       "P209784                           0                         0             0   \n",
       "P251427                           0                         0             0   \n",
       "P251713                           0                         0             0   \n",
       "P251728                           0                         0             0   \n",
       "P252215                           0                         0             0   \n",
       "...                             ...                       ...           ...   \n",
       "Q000823                           0                         0             0   \n",
       "Q000824                           0                         0             0   \n",
       "Q000825                           0                         0             0   \n",
       "Q002338                           0                         0             0   \n",
       "X010001                           0                         0             0   \n",
       "\n",
       "         šuʾura[goose]n  \n",
       "id_text                  \n",
       "P209784               0  \n",
       "P251427               0  \n",
       "P251713               0  \n",
       "P251728               0  \n",
       "P252215               0  \n",
       "...                 ...  \n",
       "Q000823               0  \n",
       "Q000824               0  \n",
       "Q000825               0  \n",
       "Q002338               0  \n",
       "X010001               0  \n",
       "\n",
       "[902 rows x 10362 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=lambda x: x.split(), preprocessor=lambda x: x, vocabulary=lex_vocab)\n",
    "#Alternative way to do the same thing:\n",
    "#cv = CountVectorizer(token_pattern = r'[^ ]+', vocabulary=lex_vocab)\n",
    "dtm = cv.fit_transform(lit_comp['lemma_mwe'])\n",
    "lit_df = pd.DataFrame(dtm.toarray(), columns= cv.get_feature_names(), index=lit_comp[\"id_text\"])\n",
    "lit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame lit_df has a row for each *literary* composition and it has a column for every lemma/expression in the *lexical* corpus. The number of columns, therefore, should correspond to the size of the lexical vocabulary in the Venn diagram produced in the previous notebook:\n",
    "\n",
    "![venn diagram 3](viz/venn_3.png)\n",
    "\n",
    "As we have seen in the previous notebook, many of these words/expressions do not appear in the [epsd2/literary](http://oracc.org/epsd2/literary) corpus, and thus all cells in those columns are 0. If we remove those columns, we get the vocabulary that is shared between the lexical corpus and the literary corpus (the intersection of the circles in the Venn diagram). The left side of the Venn diagram (the literary vocabulary that does not appear in lexical texts) is not represented in the DTM. This DTM, therefore, should only be used to research the *intersection* between the two (literary and lexical) vocabularies.\n",
    "\n",
    "> The number of non-zero columns does not *exactly* correspond to the size of the intersection in the Venn diagram. The reason is that a word like **ašrinna\\[object\\]n**, a word that in the literary corpus only appears in the MWE **kid\\[mat\\]n_ašrinna\\[object\\]n**, is counted as a match in the Venn diagram, but only appears in the column **kid\\[mat\\]n_ašrinna\\[object\\]n** in the DTM. See [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb) section 3.1.3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a[arm]n</th>\n",
       "      <th>a[arm]n_ak[do]v/t</th>\n",
       "      <th>a[arm]n_bad[open]v/t</th>\n",
       "      <th>a[arm]n_bala[turn]v/t</th>\n",
       "      <th>a[arm]n_dar[split]v/t</th>\n",
       "      <th>a[arm]n_daŋal[wide]v/i</th>\n",
       "      <th>a[arm]n_durah[goat]n</th>\n",
       "      <th>a[arm]n_e[leave]v/i</th>\n",
       "      <th>a[arm]n_gab[left]n</th>\n",
       "      <th>a[arm]n_gal[big]v/i</th>\n",
       "      <th>...</th>\n",
       "      <th>šutum[storehouse]n</th>\n",
       "      <th>šutur[garment]n</th>\n",
       "      <th>šuziʾana[1]dn</th>\n",
       "      <th>šuš[cover]v/t</th>\n",
       "      <th>šušana[one-third]nu</th>\n",
       "      <th>šuši[sixty]nu</th>\n",
       "      <th>šušin[1]sn</th>\n",
       "      <th>šušru[distressed]v/i</th>\n",
       "      <th>šuʾi[barber]n</th>\n",
       "      <th>šuʾura[goose]n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P209784</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251427</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P251728</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P252215</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000823</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q000825</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q002338</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X010001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>902 rows × 3604 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         a[arm]n  a[arm]n_ak[do]v/t  a[arm]n_bad[open]v/t  \\\n",
       "id_text                                                     \n",
       "P209784        0                  0                     0   \n",
       "P251427        0                  0                     0   \n",
       "P251713        0                  0                     0   \n",
       "P251728        0                  0                     0   \n",
       "P252215        1                  0                     0   \n",
       "...          ...                ...                   ...   \n",
       "Q000823        4                  0                     0   \n",
       "Q000824        0                  0                     0   \n",
       "Q000825        2                  0                     0   \n",
       "Q002338        0                  0                     0   \n",
       "X010001        0                  0                     0   \n",
       "\n",
       "         a[arm]n_bala[turn]v/t  a[arm]n_dar[split]v/t  a[arm]n_daŋal[wide]v/i  \\\n",
       "id_text                                                                         \n",
       "P209784                      0                      0                       0   \n",
       "P251427                      0                      0                       0   \n",
       "P251713                      0                      0                       0   \n",
       "P251728                      0                      0                       0   \n",
       "P252215                      0                      0                       0   \n",
       "...                        ...                    ...                     ...   \n",
       "Q000823                      0                      0                       0   \n",
       "Q000824                      0                      0                       0   \n",
       "Q000825                      0                      0                       0   \n",
       "Q002338                      0                      0                       0   \n",
       "X010001                      0                      0                       0   \n",
       "\n",
       "         a[arm]n_durah[goat]n  a[arm]n_e[leave]v/i  a[arm]n_gab[left]n  \\\n",
       "id_text                                                                  \n",
       "P209784                     0                    0                   0   \n",
       "P251427                     0                    0                   0   \n",
       "P251713                     0                    0                   0   \n",
       "P251728                     0                    0                   0   \n",
       "P252215                     0                    0                   0   \n",
       "...                       ...                  ...                 ...   \n",
       "Q000823                     0                    0                   0   \n",
       "Q000824                     0                    0                   0   \n",
       "Q000825                     0                    0                   0   \n",
       "Q002338                     0                    0                   0   \n",
       "X010001                     0                    0                   0   \n",
       "\n",
       "         a[arm]n_gal[big]v/i  ...  šutum[storehouse]n  šutur[garment]n  \\\n",
       "id_text                       ...                                        \n",
       "P209784                    0  ...                   0                0   \n",
       "P251427                    0  ...                   0                0   \n",
       "P251713                    0  ...                   0                0   \n",
       "P251728                    0  ...                   0                0   \n",
       "P252215                    0  ...                   0                0   \n",
       "...                      ...  ...                 ...              ...   \n",
       "Q000823                    0  ...                   0                0   \n",
       "Q000824                    0  ...                   0                0   \n",
       "Q000825                    0  ...                   0                0   \n",
       "Q002338                    0  ...                   0                0   \n",
       "X010001                    0  ...                   0                0   \n",
       "\n",
       "         šuziʾana[1]dn  šuš[cover]v/t  šušana[one-third]nu  šuši[sixty]nu  \\\n",
       "id_text                                                                     \n",
       "P209784              0              0                    0              0   \n",
       "P251427              0              0                    0              0   \n",
       "P251713              0              0                    0              0   \n",
       "P251728              0              0                    0              0   \n",
       "P252215              0              0                    0              0   \n",
       "...                ...            ...                  ...            ...   \n",
       "Q000823              0              0                    0              0   \n",
       "Q000824              0              0                    0              0   \n",
       "Q000825              0              0                    0              0   \n",
       "Q002338              0              0                    0              0   \n",
       "X010001              0              0                    0              0   \n",
       "\n",
       "         šušin[1]sn  šušru[distressed]v/i  šuʾi[barber]n  šuʾura[goose]n  \n",
       "id_text                                                                   \n",
       "P209784           0                     0              0               0  \n",
       "P251427           0                     0              0               0  \n",
       "P251713           0                     0              0               0  \n",
       "P251728           0                     0              0               0  \n",
       "P252215           0                     0              0               0  \n",
       "...             ...                   ...            ...             ...  \n",
       "Q000823           0                     0              0               0  \n",
       "Q000824           0                     0              0               0  \n",
       "Q000825           0                     0              0               0  \n",
       "Q002338           0                     0              0               0  \n",
       "X010001           0                     0              0               0  \n",
       "\n",
       "[902 rows x 3604 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df = lit_df.loc[: , lit_df.sum(axis=0) != 0].copy()\n",
    "vocab = lit_df.columns # `vocab` is a list with all the vocabulary items currently in `lit_df`\n",
    "lit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `vocab` is a list that includes all lemmas and MWEs that are shared by the literary corpus and the lexical corpus. Save this list for use in section 3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/lit_lex_vocab.txt', 'w', encoding = 'utf8') as w:\n",
    "    w.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Number of Lexical/Literary Matches per Literary Composition. \n",
    "The sum of each row of the DTM equals the sum of the frequencies of all words/expressions that a composition shares with the lexical corpus. Instead of adding up the frequencies, however, it makes more sense to count the number of non-zero entries. This number (`nmatches`) represents the number of unique words and MWEs used in a particular literary composition (represented by a row) that are found in the lexical corpus. We can do so with the Pandas method `astype(bool)`, which will yield 0 for each zero entry and 1 for each non-zero entry. \n",
    "```python\n",
    "lit_df[\"n_matches\"] = lit_df.astype(bool).sum(axis=1)\n",
    "```\n",
    "By adding more columns to the DTM (in this case the column `n_matches`) it becomes necessary to indicate on which columns we want to perform our calculations. The variable `vocab` is a list of all lemmas and MWEs that are shared by the lexical and literary corpora - and therefore it is also a list of all the relevant column names in the DTM. At this point in the script, adding this restriction to the code (in the form `lit_df[vocab]`) is a safety measure that ensures that running the line twice (for whatever reason) results in the same output and does not take `n_matches` as part of the summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df[\"n_matches\"] = lit_df[vocab].astype(bool).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2.1 Adding Metadata\n",
    "Above, we computed various statistics for each of the literary compositions. The catalog file for [epsd2/literary](http://oracc.org/epsd2/literary) contains further information (such as the composition name). Parsing the `catalogue.json` of an [ORACC](http://oracc.org) project is discussed in more detail in section [2.1.1](../2_1_Data_Acquisition_ORACC/2_1_1_parse-json-cat.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"jsonzip/epsd2-literary.zip\" # The ZIP file was downloaded in the previous notebook\n",
    "z = zipfile.ZipFile(file) \n",
    "st = z.read(\"epsd2/literary/catalogue.json\").decode(\"utf-8\")\n",
    "j = json.loads(st)\n",
    "cat_df = pd.DataFrame(j[\"members\"]).T\n",
    "#The important information, giving the title of the literary text is sometimes found in \n",
    "# `designation` and sometimes in `subgenre`. Merge those two fields.\n",
    "cat_df.loc[cat_df.designation.str[:13] == \"CDLI Literary\", \"designation\"] = cat_df.subgenre\n",
    "# Exemplars have a P number (`id_text`), composite texts have a Q number (`id_composite`).\n",
    "# Merge those two in `id_text`.\n",
    "cat_df[\"id_text\"] = cat_df[\"id_text\"].fillna(cat_df[\"id_composite\"])\n",
    "# Keep only `id_text` and `designation`.\n",
    "cat_df = cat_df[[\"id_text\", \"designation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge text length and number of unique lemmas in `lit_comp` with the number of lexical matches from the DTM (`lit_df`) by the shared field `id_text`. Merge the resulting DataFrame (`lit_df2`) with the metadata (`cat_df`). The merge method is \"inner,\" which means that only those rows that exist in all three DataFrames will end up in the new DataFrame. Thus, the compositions with 0 lemmatized words, which were eliminated above, will not re-surface in the new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>designation</th>\n",
       "      <th>length</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>n_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P209784</td>\n",
       "      <td>Ontario 2, 506</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P251427</td>\n",
       "      <td>CUSAS 38, 09</td>\n",
       "      <td>163</td>\n",
       "      <td>108</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P251713</td>\n",
       "      <td>CUSAS 38, 05</td>\n",
       "      <td>265</td>\n",
       "      <td>149</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P251728</td>\n",
       "      <td>ETCSL 2.05.01.05 Ishbi-Erra E (witness)</td>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P252215</td>\n",
       "      <td>ETCSL 4.10.01 Lament of Lisin (witness)</td>\n",
       "      <td>51</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>Q000823</td>\n",
       "      <td>Proverbs: from Ur</td>\n",
       "      <td>1395</td>\n",
       "      <td>534</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>Q000824</td>\n",
       "      <td>Proverbs: from Uruk</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Q000825</td>\n",
       "      <td>Proverbs: of unknown provenance</td>\n",
       "      <td>530</td>\n",
       "      <td>301</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Q002338</td>\n",
       "      <td>Public announcement of the loss of a seal</td>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>X010001</td>\n",
       "      <td>Saeedi 0212</td>\n",
       "      <td>252</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>902 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text                                designation  length  lex_var  \\\n",
       "0    P209784                             Ontario 2, 506       5        5   \n",
       "1    P251427                               CUSAS 38, 09     163      108   \n",
       "2    P251713                               CUSAS 38, 05     265      149   \n",
       "3    P251728    ETCSL 2.05.01.05 Ishbi-Erra E (witness)      58       49   \n",
       "4    P252215    ETCSL 4.10.01 Lament of Lisin (witness)      51       43   \n",
       "..       ...                                        ...     ...      ...   \n",
       "897  Q000823                          Proverbs: from Ur    1395      534   \n",
       "898  Q000824                        Proverbs: from Uruk      44       36   \n",
       "899  Q000825            Proverbs: of unknown provenance     530      301   \n",
       "900  Q002338  Public announcement of the loss of a seal      37       34   \n",
       "901  X010001                                Saeedi 0212     252      154   \n",
       "\n",
       "     n_matches  \n",
       "0            5  \n",
       "1          102  \n",
       "2          129  \n",
       "3           47  \n",
       "4           40  \n",
       "..         ...  \n",
       "897        484  \n",
       "898         33  \n",
       "899        282  \n",
       "900         25  \n",
       "901        143  \n",
       "\n",
       "[902 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df2 = pd.merge(lit_comp[[\"id_text\", \"length\", \"lex_var\"]], lit_df['n_matches'], on = 'id_text', how = 'inner')\n",
    "lit_df2 = pd.merge(cat_df, lit_df2, on = 'id_text', how = 'inner')\n",
    "lit_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by the number of lexical matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>designation</th>\n",
       "      <th>length</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>n_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Q000351</td>\n",
       "      <td>Ninurta's exploits: a šir-sud (?) to Ninurta</td>\n",
       "      <td>3095</td>\n",
       "      <td>835</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>Q000380</td>\n",
       "      <td>The lament for Sumer and Ur</td>\n",
       "      <td>2644</td>\n",
       "      <td>740</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>Q000750</td>\n",
       "      <td>The temple hymns</td>\n",
       "      <td>2486</td>\n",
       "      <td>697</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Q000367</td>\n",
       "      <td>Lugalbanda in the mountain cave</td>\n",
       "      <td>1939</td>\n",
       "      <td>637</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>Q000334</td>\n",
       "      <td>Enki and the world order</td>\n",
       "      <td>1925</td>\n",
       "      <td>620</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text                                   designation  length  lex_var  \\\n",
       "537  Q000351  Ninurta's exploits: a šir-sud (?) to Ninurta    3095      835   \n",
       "562  Q000380                   The lament for Sumer and Ur    2644      740   \n",
       "839  Q000750                              The temple hymns    2486      697   \n",
       "551  Q000367               Lugalbanda in the mountain cave    1939      637   \n",
       "521  Q000334                      Enki and the world order    1925      620   \n",
       "\n",
       "     n_matches  \n",
       "537        730  \n",
       "562        634  \n",
       "839        583  \n",
       "551        581  \n",
       "521        565  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df2 = lit_df2.sort_values(by = \"n_matches\", na_position=\"first\", ascending=False)\n",
    "lit_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2.2 Normalizing\n",
    "Lugal-e (or [Ninurta's Exploits](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.6.2&display=Crit&charenc=gcirc#) has the highest number of matches (more than 700) with the Old Babylonian lexical corpus in [DCCLT](http://oracc.org/dcclt). But this is also the longest composition in the corpus. We can normalize by dividing the total number of matches (`n_matches`) by the number of unique lemmas (`lex_var`) in the text (`norm`). Such numbers mean little for very short texts with just a few (lemmatized) words. In the next section we will add the possibility of excluding texts that fall under a certain minimum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>designation</th>\n",
       "      <th>length</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>n_matches</th>\n",
       "      <th>norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>P254886</td>\n",
       "      <td>ETCSL 6.01.05 Proverbs Collection 05 (witness)</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>P346731</td>\n",
       "      <td>UET 6, 0694</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>P346329</td>\n",
       "      <td>UET 6, 0253</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>P346379</td>\n",
       "      <td>UET 6, 0331</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>P346355</td>\n",
       "      <td>UET 6, 0292</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>P346829</td>\n",
       "      <td>UET 6, 0792</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>P346827</td>\n",
       "      <td>UET 6, 0790</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>P346831</td>\n",
       "      <td>UET 6, 0794</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>P346826</td>\n",
       "      <td>UET 6, 0789</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>P346855</td>\n",
       "      <td>UET 6, 0818</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>902 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text                                     designation  length  lex_var  \\\n",
       "48   P254886  ETCSL 6.01.05 Proverbs Collection 05 (witness)       6        6   \n",
       "424  P346731                                     UET 6, 0694      14       14   \n",
       "224  P346329                                     UET 6, 0253       5        5   \n",
       "274  P346379                                     UET 6, 0331      15       14   \n",
       "250  P346355                                     UET 6, 0292      15       14   \n",
       "..       ...                                             ...     ...      ...   \n",
       "455  P346829                                     UET 6, 0792       5        3   \n",
       "453  P346827                                     UET 6, 0790       3        3   \n",
       "457  P346831                                     UET 6, 0794       1        1   \n",
       "452  P346826                                     UET 6, 0789       3        3   \n",
       "477  P346855                                     UET 6, 0818       1        1   \n",
       "\n",
       "     n_matches  norm  \n",
       "48           6   1.0  \n",
       "424         14   1.0  \n",
       "224          5   1.0  \n",
       "274         14   1.0  \n",
       "250         14   1.0  \n",
       "..         ...   ...  \n",
       "455          0   0.0  \n",
       "453          0   0.0  \n",
       "457          0   0.0  \n",
       "452          0   0.0  \n",
       "477          0   0.0  \n",
       "\n",
       "[902 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df2[\"norm\"] = lit_df2[\"n_matches\"] / lit_df2[\"lex_var\"]\n",
    "lit_df2.sort_values(by = \"norm\", na_position=\"first\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Exploring the Results\n",
    "The following code displays the results in an interactive table that may be sorted (ascending or descending) in different ways for further exploration. By default, texts shorter than 50 lemmatized words are excluded and only the first 10 columns are displayed. One may change these numbers by moving the slides. The column `id_text` provides links to the editions in [epsd2/literary](http://oracc.org/epsd2/literary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.org/epsd2/literary/{}\", target=\"_blank\">{}</a>'\n",
    "lit = lit_df2.copy()\n",
    "lit['id_text'] = [anchor.format(val,val) for val in lit['id_text']]\n",
    "lit['PQ'] = ['Composite' if i[0] == 'Q' else 'Exemplar' for i in lit_df2['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd1c456eeb14f6084159185fb85b28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='sort_by', index=5, options=('id_text', 'designation', 'length', 'l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(sort_by = lit.drop('PQ', axis=1).columns, rows = (1, len(lit), 1), min_length = (0,500,5), show = [\"Exemplars\", \"Composites\", \"All\"])\n",
    "def sort_df(sort_by = \"norm\", ascending = False, rows = 10, min_length = 200, show = \"All\"):\n",
    "    if not show == 'All':\n",
    "        l = lit.loc[lit['PQ'] == show[:-1]]\n",
    "    else:\n",
    "        l = lit\n",
    "    l = l.drop('PQ', axis = 1)\n",
    "    l = l.loc[l.length >= min_length].sort_values(by = sort_by, ascending = ascending).reset_index(drop=True)[:rows].style\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Discussion\n",
    "We may now come back to our initial question: is there a meaningful way to classify literary texts on the basis of the intersection of the vocabulary of that composition with the lexical vocabulary? In other words: are there certain composition that draw more heavily from the lexical vocabulary than others? \n",
    "\n",
    "Exploring the table above seems to indicate that this is not the case. Of the compositions longer than 200 lemmatized words, Inana E has the highest `norm` value of almost .99. Indeed, of the 96 unique lemmas in this text, 95 are known from the lexical corpus. But a review of the spread of `norm` will show that this is hardly exceptional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4.1 Norm: Descriptive Statistics\n",
    "In order to explore the spread of `norm` we will restrict the data to documents that are at least 200 lemmas (and MWEs) in length. Very short texts (such as lentils or small fragments) yield extreme results that are not very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    204.000000\n",
       "mean       0.910484\n",
       "std        0.052321\n",
       "min        0.359833\n",
       "25%        0.898303\n",
       "50%        0.917355\n",
       "75%        0.933890\n",
       "max        0.989691\n",
       "Name: norm, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_length = 200\n",
    "lit.loc[lit.length >= min_length, 'norm'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows that the 25%, 50%, and 75% points are all very close to each other, around 0.92. In other words: in the great majority of literary compositions, more than 90% of the lemmas and Multiple Word Expressions are attested in the lexical corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4.2 Histogram\n",
    "The histogram of `norm` is a way to visualize the (very) skewed distribution of its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbins = 5\n",
    "column = 'norm'\n",
    "fig, ax = plt.subplots()\n",
    "counts, bins, patches = ax.hist(lit.loc[lit.length >= min_length, column], bins = nbins)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f')) # tick labels with two decimals\n",
    "ax.set_xticks(bins)\n",
    "for i in range(nbins):\n",
    "    plt.text(bins[i],counts[i]/2,str(counts[i]), fontsize = 16)\n",
    "plt.ylabel('No. of Compositions')\n",
    "plt.xlabel(column)\n",
    "plt.savefig(f'viz/hist_norm.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we restrict the display to documents of at least 200 lemmas, the lowest scoring one is [The Sumerian King List](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.2.1.1&display=Crit&charenc=gcirc#), a rather repetitive text that lists cities, kings, and regnal years, with only occasionally a brief anecdote about one of those kings. Many of these king names are not found in the lexical corpus - hence the `norm` value of 0.36. But, as the histogram shows, this is exceptional, it is much more common to see a literary text with more than 90% of its lemmas represented in the lexical corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4.3 Dumuzid's Dream\n",
    "In the Venn diagrams in 3.1 we saw that between 55% and 65% (depending on the way of counting) of the literary vocabulary is not attested in the lexical corpus. How can we square that outcome with the fact that the great majority of literary compositions shares more than 90% of their vocabulary with the lexical corpus? In order to explore that question we will look at a concrete example: the story of [Dumuzid's Dream](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.3&display=Crit&charenc=gcirc#). In [epsd2/literary](http://oracc.org/epsd2/literary) this composition has the ID Q000347."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>designation</th>\n",
       "      <th>length</th>\n",
       "      <th>lex_var</th>\n",
       "      <th>n_matches</th>\n",
       "      <th>norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>Q000347</td>\n",
       "      <td>Dumuzid's dream</td>\n",
       "      <td>1146</td>\n",
       "      <td>274</td>\n",
       "      <td>258</td>\n",
       "      <td>0.941606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_text      designation  length  lex_var  n_matches      norm\n",
       "533  Q000347  Dumuzid's dream    1146      274        258  0.941606"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_df2[lit_df2.id_text == \"Q000347\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dumuzid's Dream](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.3&display=Crit&charenc=gcirc#) has a `norm` value of 0.94, slightly above the median value of 0.92. The number means that 94% of the lemmas (and MWEs) in [Dumuzid's Dream](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.3&display=Crit&charenc=gcirc#) are found in the lexical corpus. Which words are not found?\n",
    "\n",
    "We have to go back to the data set as represented by `lit_lines` - this contains all words, lemmatized and unlemmatized, those represented in the lexical corpus and those not attested there. We select the lines that belong to [Dumuzid's Dream](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.3&display=Crit&charenc=gcirc#) (Q000347), join the lines to a single string and then split this string at the blank spaces. This gives a list of all lemmas and MWEs in [Dumuzid's Dream](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.3&display=Crit&charenc=gcirc#). We remove the unlemmatized forms and reduce the list to a set of unique elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD = lit_lines[lit_lines.id_text.str[-7:] == \"Q000347\"]\n",
    "DD_lemmas = ' '.join(DD['lemma_mwe']).split()\n",
    "DD_lemmas = [l for l in DD_lemmas if not '[na]na' in l]\n",
    "DD_lemmas_s = set(DD_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now transform the list `lex_vocab` (which contains all lexical lemmas and MWEs) into a set, so that we can subtract one set from the other. The result (another set) contains all the items that are present in the first set (the vocabulary of [Dumuzid's Dream](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.3&display=Crit&charenc=gcirc#)), but not in the other (the Old Babylonian lexical vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amaŋeštinanak[1]dn',\n",
       " 'arali[1]gn',\n",
       " 'dubban[fence]n',\n",
       " 'durtur[1]dn',\n",
       " 'kubireš[1]gn',\n",
       " 'kubirešdildareš[1]gn',\n",
       " 'mašuzudak[goat]n',\n",
       " 'men[go]v/i',\n",
       " 'ne[cvne]n',\n",
       " 'sug[full]v/i',\n",
       " 'zipatum[cord]n',\n",
       " 'ŋeštindudu[1]dn',\n",
       " 'šarag[dry]v/t',\n",
       " 'še[tear]n',\n",
       " 'širkalkal[subscript]n',\n",
       " 'šudu[handcuffs]n'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_vocab_s = set(lex_vocab)\n",
    "DD_lemmas_s - lex_vocab_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the words, currenly not found in the lexical corpus, are a remarkable number of proper nouns: Divine Names (Amaŋeštinanak, Durtur, and Ŋeštindudu), and Geographical names (Arali, Kubireš, and Kubirešdildareš). The case of Arali is instructive. This word appears in OB Nippur Izi, but is lemmatized there as a noun (a word for the Netherworld), rather than as a Geographical Name. The goddess Amaŋeštinanak (**{d}ama-ŋeštin-an-na**) is well-attested in third millennium administartive texts and royal inscriptions, but is uncommon in the Old Babylonian period. In fact, the text of [Dumuzid's Dream](http://etcsl.orinst.ox.ac.uk/cgi-bin/etcsl.cgi?text=c.1.4.3&display=Crit&charenc=gcirc#) writes ama {d}ŋeštin-an-na \"Mother Ŋeštinanak\" - where \"Mother\" is an apparent honorific. Read that way, the word **ama** (mother) and the Divine Name **{d}ŋeštin-an-na** are present in the lexical corpus.\n",
    "\n",
    "The word men\\[go\\]v/i is an Emesal word. Emesal is a variety of Sumerian that is used in cultic laments and in speeches by female divinities in Sumerian myths. Although the Old Babylonian lexical corpus includes some Emesal words, they are truly rare. Generally speaking, Emesal is associated with the liturgical use of Sumerian, whereas the scribal school focuses primarily on regular Sumerian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
