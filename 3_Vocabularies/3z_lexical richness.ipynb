{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.z Lexical Richness\n",
    "\n",
    "Research lexical richness measures for Sumerian literature.\n",
    "\n",
    "### 3.z.0 Preparation\n",
    "\n",
    "This notebook uses some files that were downloaded or produced in [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb). Run that notebook first, before this one. \n",
    "\n",
    "First import the necessary libraries. If you are running this notebook in Jupyter Lab you will need to install the Jupyter Lab ipywidgets extension (see [Introduction](../1_Preliminaries/1_Introduction.md), section 1.2.2.1). \n",
    "\n",
    "> The new version of Lexical Richness addresses the problem discussed below - but this version is not (yet) issued on PyPi. Thus %pip install lexicalrichness will get you the wrong version. For that reason this notebook is still using the adjusted version of LexicalRichness, as discussed below.\n",
    "\n",
    "> The [Lexicalrichness](https://pypi.org/project/lexicalrichness/) package by Lucas Shen has been slightly adapted for the present purposes. The package expects a data set in the English language in a raw text format that must be pre-processed (removal of interpunction, digits, etc.) and tokenized (cut up into individual words). These steps do not work well for the present data set. The adapted version, named `lexicalrichness_v` is imported from the `utils` directory. The usage information in the [Lexicalrichness](https://pypi.org/project/lexicalrichness/) website is valid for `lexicalrichness_v` with the following exceptions:\n",
    "> - the option use_TextBlob in LexicalRichness() is removed\n",
    "> - the option use_tokenizer in LexicalRichness is added; default is use_tokenizer = False.\n",
    "\n",
    "> If `use_tokenizer = False` (default) the main function expects a list as input; no tokenizing or preprocessing is performed. If `use_tokenizer = True` the function expects a string, which is preprocessed and tokenized (default behaviour in the original package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "import os\n",
    "import sys\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from lexicalrichness_v import LexicalRichness as lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file `litlines.p` which was produced in [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb). The file contains the pickled version of the DataFrame `lit_lines` in which the literary ([epsd2/literary](http://oracc.org/epsd2/literary)) corpus is represented in line-by-line format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lines = pd.read_pickle('output/litlines.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.z.0.1 Literary: By Composition\n",
    "The line-by-line representation that was prepared in the previous notebook will be transformed into a composition-by-composition representation. The DataFrame `lit_lines` includes the column `lemma` in which each line is represented as a sequence of lemmas. The `pandas` `groupby()` function is used here to group on `id_text` and `text_name`. The aggregate function for the `lemma` column in this case is simply `' '.join`: all the entries (representing lines) are concatenated to form one long sequence of lemmas in a single string representing one composition.\n",
    "\n",
    "The field `id_text` in the resulting DataFrame has the form 'epsd2/literary/P254863'. In fact, we only need the last 7 characters of that string (the P, Q, or X number of the text), because all texts derive from the same project. We can simplify the `id_text` string with a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_comp = lit_lines.groupby(\n",
    "    [lit_lines[\"id_text\"]]).aggregate(\n",
    "    {\"lemma\": ' '.join}).reset_index()\n",
    "lit_comp['id_text'] = [id[-7:] for id in lit_comp[\"id_text\"]]\n",
    "lit_comp[25:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a DataFrame with two columns: `id_text`, and `lemma_mwe`. Each row represents a literary composition from the [epsd2/literary](http://oracc.org/epsd2/literary) corpus. Each cell in the column `lemma_mwe` contains a sequence of lemmas of one composition (with MWEs connected by underscores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.z.1 Lexical Richness Statistics for the Literary Corpus\n",
    "\n",
    "In the following we will compute the number of vocabulary matches between each text in the literary corpus and the full lexical corpus. In order to interpret the number of matches properly, we will first compute a number of basic text measures, such as text length, type-token ratio, etc. In all the measures discussed below only the words that are properly lemmatized are counted.\n",
    "\n",
    "| Measurement       |                                             |\n",
    "|-------------------|---------------------------------------------|\n",
    "| Text Length       | Number of lemmatized words         |\n",
    "| Lexical Variation | Number of unique lemmas            |\n",
    "| Type Token Ration | Lexical Variation divided by Text Length    |\n",
    "| MTLD              | (see below)                                 |\n",
    "\n",
    "Lexical richness measures the variation in vocabulary usage. Texts that use a relatively low number of unique lexemes (repeat the same words all over the place) receive a low lexical richness score. Texts that use the lexicon more ingenuously, using synonyms or circumscriptions to refer to the same concept, receive a higher lexical richness score. Lexical richness is used, among other things, to identify texts written by language learners, to assess aphasia, or to measure the difficulty of a text. For Sumerian literature, we may expect that compositions with high levels of repetition, such as certain hymns and narratives, end up with a low lexical richness score, whereas disputation texts or other compositions that actively explore the lexicon may have a higher score.\n",
    "\n",
    "The most straightforward lexical richness score is the Type Token Ratio (or TTR), which simply divides the number of unique lexemes by the total number of lexemes. This is a fine measurement to compare texts of (approximately) equal length, but does not work well for a corpus with texts of very different length, as is the case here. Short texts have higher TTR values than long texts, because longer texts will by necessity use the same words over and over again and (in English) function words such as \"the\" or \"in\" will be repeated many times whatever the lexical ingenuity of the author. A better measurement is called MTLD or Measure of Textual Lexical Diversity ([McCarthy and Jarvis 2010](https://doi.org/10.3758/BRM.42.2.381)). The MTLD value is calculated as the mean number of words in a text that will bring TTR from 1 (at the first word in the text) down to a threshold value (default is 0.720). In practice that means that a text is cut in many small units, each with approximately the same TTR - eliminating the effect of text length. This is a promising approach that may well work for Sumerian and a Python module that includes MTLD is available ([lexicalrichness](https://pypi.org/project/lexicalrichness/)). Its usage here, however, is experimental and preliminary. The threshold value is based on the observation that when going through a text sequentially the TTR in any text will drop drastically as soon as the first repeated word is encountered. At some place in the text the TTR will stabilize and drop only very gradually later on. That place is approximated by the default TTR threshold value of 0.720. It seems likely, however, that a valid threshold value is language dependent and that a language with very few function words, such as the literary register of Sumerian, might need a higher value (function words drag the TTR down). On the other hand, a corpus of texts with very substantial repetition (occasionaly repetition of entire lengthy passages) may well require a lower threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lit_stats(lemmas):\n",
    "    lemmas = lemmas.split()\n",
    "    lemmas = [lemma for lemma in lemmas if not '[na]na' in lemma] # remove unlemmatized words\n",
    "    lex = lr(lemmas) #, tokenizer = None, preprocessor = None)\n",
    "    length = lex.words # number of lemmatized words\n",
    "    lex_var = lex.terms # number of unique lemmas\n",
    "    if length > 0:  # prevent division by 0\n",
    "        ttr = lex.ttr\n",
    "        mtld = lex.mtld()\n",
    "    else:\n",
    "        ttr = 0\n",
    "        mtld = 0\n",
    "    return ' '.join(lemmas), length, lex_var, ttr, mtld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_comp['lemma'], lit_comp['length'], lit_comp['lex_var'], lit_comp['ttr'], lit_comp['mtld'] = \\\n",
    "    zip(*lit_comp['lemma'].progress_map(lit_stats))\n",
    "lit_comp = lit_comp.loc[lit_comp['length'] > 0] # remove compositions that have no lemmatized content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may get a first glimpse of the results by inspecting the basic descriptive statistics. For this, we ignore texts shorter than 50 lemmas, because measures like TTR and MTLD become rather meaningless for very short compositions. It appears that MTLD varies from 9.5 all the way up to 487.3, with a mean value of 78.1. That means that there is a text that, on average, needs only 10 words (two or three lines) to push the TTR under 0.720 - meaning a lot of repeated words (or repeated phrases) all over the place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_comp.loc[lit_comp.length > 50].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Adding Metadata\n",
    "Above, we computed various statistics for each of the literary compositions. The catalog file for [epsd2/literary](http://oracc.org/epsd2/literary) contains further information (such as the composition name). Parsing the `catalogue.json` of an [ORACC](http://oracc.org) project is discussed in more detail in section [2.1.1](../2_1_Data_Acquisition_ORACC/2_1_1_parse-json-cat.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"jsonzip/epsd2-literary.zip\" # The ZIP file was downloaded in the previous notebook\n",
    "z = zipfile.ZipFile(file) \n",
    "st = z.read(\"epsd2/literary/catalogue.json\").decode(\"utf-8\")\n",
    "j = json.loads(st)\n",
    "cat_df = pd.DataFrame(j[\"members\"]).T\n",
    "#The important information, giving the title of the literary text is sometimes found in \n",
    "# `designation` and sometimes in `subgenre`. Merge those two fields.\n",
    "cat_df.loc[cat_df.designation.str[:13] == \"CDLI Literary\", \"designation\"] = cat_df.subgenre\n",
    "# Exemplars have a P number (`id_text`), composite texts have a Q number (`id_composite`).\n",
    "# Merge those two in `id_text`.\n",
    "cat_df[\"id_text\"] = cat_df[\"id_text\"].fillna(cat_df[\"id_composite\"])\n",
    "# Keep only `id_text` and `designation`.\n",
    "cat_df = cat_df[[\"id_text\", \"designation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the Lexical Richness statistics (`lit_comp`) with the number of lexical matches from the DTM (`lit_df`) by the shared field `id_text`. Merge the resulting DataFrame (`lit_df2`) with the metadata (`cat_df`). The merge method is \"inner,\" which means that only those rows that exist in all three DataFrames will end up in the new DataFrame. Thus, the compositions with 0 lemmatized words, which were eliminated above, will not re-surface in the new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df2 = pd.merge(lit_comp[[\"id_text\", \"length\", \"mtld\", \"ttr\", \"lex_var\"]], cat_df, on = 'id_text', how = 'inner')\n",
    "lit_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.z.3 Exploring the Results\n",
    "The following code displays the results in an interactive table that may be sorted (ascending or descending) in different ways for further exploration. By default, texts shorter than 50 lemmatized words are excluded and only the first 10 columns are displayed. One may change these numbers by moving the slides. The column `id_text` provides links to the editions in [epsd2/literary](http://oracc.org/epsd2/literary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.org/epsd2/literary/{}\", target=\"_blank\">{}</a>'\n",
    "lit = lit_df2.copy()\n",
    "lit['id_text'] = [anchor.format(val,val) for val in lit['id_text']]\n",
    "lit['PQ'] = ['Composite' if i[0] == 'Q' else 'Exemplar' for i in lit_df2['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sort_by = lit.drop('PQ', axis=1).columns, rows = (1, len(lit), 1), min_length = (0,500,5), show = [\"Exemplars\", \"Composites\", \"All\"])\n",
    "def sort_df(sort_by = \"mtld\", ascending = False, rows = 10, min_length = 200, show = \"All\"):\n",
    "    if not show == 'All':\n",
    "        l = lit.loc[lit['PQ'] == show[:-1]]\n",
    "    else:\n",
    "        l = lit\n",
    "    l = l.drop('PQ', axis = 1)\n",
    "    l = l.loc[l.length >= min_length].sort_values(by = sort_by, ascending = ascending).reset_index(drop=True)[:rows].style\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.z.4 Discussion\n",
    "A second issue of discussion is the usefulness of the MTLD measure. Which compositions score high on MTLD, and what does that tell us about these compositions? If we restrict the table to compositions (leaving exemplars out) and only take compositions that are at least 200 words long, the for top-scoring compositions on MTLD are all royal hymns: Enlil Bani A (253); Šulgi A (247); Išme-Dagan I (229); and Lipit-Eštar A (205). These numbers are very high, indeed. Enlil Bani A is only 312 words long, meaning that the TTR value only gets to the threshold value of .720 in the last quarter of the text (and that the text has only one full factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
