{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is going to be 3.3\n",
    "\n",
    "# 3.3 Looking at the Lexical Vocabulary from the Perspective of the Literary Material\n",
    "\n",
    "In section 3.2 we asked whether we can see differences between Old Babylonian literary compositions in their usage of vocabulary (lemmas and MWEs) attested in the lexical corpus. In this notebook we will change perspective and ask: are there particular lexical texts (or groups of lexical texts) that show a greater engagement with literary vocabulary than others?\n",
    "\n",
    "In large part, this notebook uses the same techniques and the same code as section 3.2 did, and the reader is referred there for further explanation. In some aspects, however, the process is different. In particular, we will use various aspects of `CountVectorizer()` and the related function `TfidfVectorizer()` to understand the relationship in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "import zipfile\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file `lexlines.p` which was produced in [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb). The file contains the pickled version of the DataFrame `lex_lines` in which the lexical ([dcclt](http://oracc.org/dcclt)) corpus is represented in line-by-line format.\n",
    "\n",
    "The field `id_text` is represented as `dcclt/P227743` or `dcclt/signlists/Q000057`. In practice, we only need the list seven characters, the P, Q, or X number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_lines = pd.read_pickle('output/lexlines.p')\n",
    "lex_lines['id_text'] = lex_lines['id_text'].str[-7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Case: OB Nippur Ura 6\n",
    "The sixth chapter of the Old Babylonian Nippur version of the thematic list Ura deals with foodstuffs and drinks. This chapter was not standardized (each exemplar has its own order of items and sections) and therefore no composite text has been created in [DCCLT](http://oracc.org/dcclt). Instead, the \"composite\" of [OB Nippur Ura 6](http://oracc.org/dcclt/Q000043) consists of the concatenation of all known Nippur exemplars of the list of foodstuffs. In our current dataframe, therefore, there are no lines where the field `id_text` equals \"Q000043\".\n",
    "\n",
    "We create a \"composite\" by changing the field `id_text` in all exemplars of [OB Nippur Ura 6](http://oracc.org/dcclt/Q000043) to \"Q000043\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ura6 = [\"P227657\",\n",
    "\"P227743\",\n",
    "\"P227791\",\n",
    "\"P227799\",\n",
    "\"P227925\",\n",
    "\"P227927\",\n",
    "\"P227958\",\n",
    "\"P227967\",\n",
    "\"P227979\",\n",
    "\"P228005\",\n",
    "\"P228008\",\n",
    "\"P228200\",\n",
    "\"P228359\",\n",
    "\"P228368\",\n",
    "\"P228488\",\n",
    "\"P228553\",\n",
    "\"P228562\",\n",
    "\"P228663\",\n",
    "\"P228726\",\n",
    "\"P228831\",\n",
    "\"P228928\",\n",
    "\"P229015\",\n",
    "\"P229093\",\n",
    "\"P229119\",\n",
    "\"P229304\",\n",
    "\"P229332\",\n",
    "\"P229350\",\n",
    "\"P229351\",\n",
    "\"P229352\",\n",
    "\"P229353\",\n",
    "\"P229354\",\n",
    "\"P229356\",\n",
    "\"P229357\",\n",
    "\"P229358\",\n",
    "\"P229359\",\n",
    "\"P229360\",\n",
    "\"P229361\",\n",
    "\"P229362\",\n",
    "\"P229365\",\n",
    "\"P229366\",\n",
    "\"P229367\",\n",
    "\"P229890\",\n",
    "\"P229925\",\n",
    "\"P230066\",\n",
    "\"P230208\",\n",
    "\"P230230\",\n",
    "\"P230530\",\n",
    "\"P230586\",\n",
    "\"P231095\",\n",
    "\"P231128\",\n",
    "\"P231424\",\n",
    "\"P231446\",\n",
    "\"P231453\",\n",
    "\"P231458\",\n",
    "\"P231742\",\n",
    "\"P266520\"]\n",
    "lex_lines.loc[lex_lines[\"id_text\"].isin(Ura6), \"id_text\"] = \"Q000043\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing text length\n",
    "In order to evaluate the number of matches between a lexical text and the literary corpus we need a measure of text length. Text length is defined here as the number of lemmatized words in a text.\n",
    "\n",
    "First the lines of `lit_lines` are aggregated to lexical compositions in the DataFrame `lex_comp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_comp = lex_lines.groupby(\n",
    "    [lex_lines[\"id_text\"]]).aggregate(\n",
    "    {\"lemma\": ' '.join}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `lex_length()` computes the number of lemmas in each composition by first splitting the field `lemmas` into individual lemmas. A list comprehension removes all unlemmatized words, and the length of the resulting list is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_length(lemmas):\n",
    "    lemmas = lemmas.split()\n",
    "    lemmas = [lemma for lemma in lemmas if not '[na]na' in lemma] # remove unlemmatized words\n",
    "    length = len(lemmas)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First add the new field `length` by calling the function `lex_length()` for every row.\n",
    "\n",
    "The DataFrame `lex_comp` has data from all Old Babylonian lexical texts currently in [dcclt](http://oracc.org/dcclt). Not all of these texts are lemmatized. In particular, documents that have been linked to a composite text are usually not lemmatized. Such documents have no lemmatized contents and therefore have length 0. These documents are removed from `lex_comp`.\n",
    "\n",
    "- remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_comp['length'] = lex_comp['lemma'].progress_map(lex_length)\n",
    "lex_comp = lex_comp.loc[lex_comp['length'] > 0] # remove compositions that have no lemmatized content\n",
    "lex_comp = lex_comp.sort_values(by = 'length', ascending=False)\n",
    "lex_comp = lex_comp.drop_duplicates(subset = 'id_text', keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open list of Vocabulary Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/lit_lex_vocab.txt', 'r', encoding = 'utf8') as l:\n",
    "    lit_lex_vocab = l.read().splitlines()\n",
    "lit_lex_vocab = [v.replace('_', ' ') for v in lit_lex_vocab]\n",
    "lit_lex_vocab[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM\n",
    "Go back to `lex_lines` so that ngrams do not jump over line boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(preprocessor = lambda x: x, tokenizer = lambda x: x.split(), vocabulary = lit_lex_vocab, ngram_range=(1, 5))\n",
    "dtm = cv.fit_transform(lex_lines['lemma'])\n",
    "lex_lines_dtm = pd.DataFrame(dtm.toarray(), columns= cv.get_feature_names(), index=lex_lines[\"id_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_lines_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_comp_dtm = lex_lines_dtm.groupby('id_text').agg(sum).reset_index()\n",
    "vocab = lex_comp_dtm.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicates and empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lex_comp_dtm['id_text'] = [i[-7:] for i in lex_comp_dtm['id_text']]\n",
    "#lex_comp_dtm['length'] = lex_comp_dtm.sum(axis=1)\n",
    "#lex_comp_dtm = lex_comp_dtm.sort_values(by = 'length', ascending = False)\n",
    "#lex_comp_dtm = lex_comp_dtm.drop_duplicates(subset = 'id_text', keep = 'first')\n",
    "#lex_comp_dtm = lex_comp_dtm[lex_comp_dtm.length > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_comp_dtm[\"n_matches\"] = lex_comp_dtm[vocab].astype(bool).sum(axis = 1, numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata. \n",
    "cat = {}\n",
    "for proj in ['dcclt', 'dcclt/signlists', 'dcclt/nineveh', 'dcclt/ebla']:\n",
    "    f = proj.replace('/', '-')\n",
    "    file = f\"jsonzip/{f}.zip\" # The ZIP file was downloaded in notebook 3_1\n",
    "    z = zipfile.ZipFile(file) \n",
    "    st = z.read(f\"{proj}/catalogue.json\").decode(\"utf-8\")\n",
    "    j = (json.loads(st))\n",
    "    cat.update(j[\"members\"])\n",
    "cat_df = pd.DataFrame(cat).T\n",
    "cat_df[\"id_text\"] = cat_df[\"id_text\"].fillna(cat_df[\"id_composite\"])\n",
    "cat_df = cat_df.fillna('')\n",
    "cat_df = cat_df[[\"id_text\", \"designation\", \"subgenre\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = pd.merge(cat_df, lex_comp_dtm[['n_matches', 'id_text']], on = 'id_text', how = 'inner')\n",
    "lex = pd.merge(lex, lex_comp[['length', 'id_text']], on = 'id_text', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex['norm'] = lex['n_matches'] / lex['length']\n",
    "lex = lex.sort_values(by = 'norm', ascending = False)\n",
    "lex.loc[lex.length > 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.org/dcclt/{}\", target=\"_blank\">{}</a>'\n",
    "lex2 = lex.copy()\n",
    "lex2['id_text'] = [anchor.format(val,val) for val in lex['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sort_by = lex2.columns, rows = (1, len(lex2), 1), min_length = (1,500,5))\n",
    "def sort_df(sort_by = \"norm\", ascending = False, rows = 25, min_length = 250):\n",
    "    return lex2.loc[lex2.length >= min_length].sort_values(by = sort_by, ascending = ascending).reset_index(drop=True)[:rows].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: look at important words with tfidf.\n",
    "\n",
    "Note: first make ngrams (as above) then TfidfVectorizer() with vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lines = pd.read_pickle('output/litlines.p')\n",
    "lit_comp2 = lit_lines.groupby(['id_text']).agg({'lemma' : ' '.join}).reset_index()\n",
    "lit_comp2['id_text'] = [i[-7:] for i in lit_comp2['id_text']]\n",
    "tv = TfidfVectorizer(token_pattern = r'[^ ]+', ngram_range = (1,5), vocabulary = vocab)\n",
    "dtm = tv.fit_transform(lit_comp2['lemma'])\n",
    "lit_df = pd.DataFrame(dtm.toarray(), columns= tv.get_feature_names(), index=lit_comp2[\"id_text\"])\n",
    "lit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = lit_df[vocab].sum(axis=0) / lit_df[vocab].astype(bool).sum(axis=0) #total weights by total hits\n",
    "mean = mean.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf = lex_comp_dtm.copy()\n",
    "lit_lex_tfidf[vocab] = lit_lex_tfidf[vocab].mul(mean, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf['weighted'] = lit_lex_tfidf[vocab].sum(axis=1, numeric_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lex_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex2 = pd.merge(cat_df, lit_lex_tfidf[['weighted', 'id_text']], on = 'id_text', how = 'inner')\n",
    "lex2 = pd.merge(lex2, lex[['length', 'n_matches', 'id_text']], on = 'id_text', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of dividing by length look at mean value of weighted\n",
    "```python\n",
    "lex2['norm'] = lex2['weigthed'] / lex2.astype(bool).sum(axis = 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lex2['norm'] = lex2['weighted'] / lex2['n_matches']\n",
    "lex2['norm'] = lex2['weighted'] / lex2['length']\n",
    "lex2.sort_values(by = 'norm', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = '<a href=\"http://oracc.org/dcclt/{}\", target=\"_blank\">{}</a>'\n",
    "lex3 = lex2.copy()\n",
    "lex3['id_text'] = [anchor.format(val,val) for val in lex2['id_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sort_by = lex3.columns, rows = (1, len(lex3), 1), min_length = (1,500,5))\n",
    "def sort_df(sort_by = \"weighted\", ascending = False, rows = 25, min_length = 200):\n",
    "    return lex3.loc[lex3.length >= min_length].sort_values(by = sort_by, ascending = ascending).reset_index(drop=True)[:rows].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
