{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "import os\n",
    "import sys\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from lexicalrichness_v import LexicalRichness as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lines = pd.read_pickle('output/litlines.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_comp = lit_lines.groupby(\n",
    "    [lit_lines[\"id_text\"]]).aggregate(\n",
    "    {\"lemma_mwe\": ' '.join}).reset_index()\n",
    "lit_comp['id_text'] = [id[-7:] for id in lit_comp[\"id_text\"]]\n",
    "lit_comp[25:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lit_stats(lemmas):\n",
    "    lemmas = lemmas.split()\n",
    "    lemmas = [lemma for lemma in lemmas if not '[na]na' in lemma] # remove unlemmatized words\n",
    "    length = len(lemmas) # number of lemmatized words\n",
    "    lex_var = len(set(lemmas))\n",
    "    return ' '.join(lemmas), length, lex_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_comp['lemma_mwe'], lit_comp['length'], lit_comp['lex_var'] = \\\n",
    "    zip(*lit_comp['lemma_mwe'].progress_map(lit_stats))\n",
    "lit_comp = lit_comp.loc[lit_comp['length'] > 0] # remove compositions that have no lemmatized content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_comp.loc[lit_comp.length > 50].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/lex_vocab.txt', 'r', encoding = 'utf8') as r:\n",
    "    lex_vocab = r.read().splitlines()\n",
    "lex_vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=lambda x: x.split(), preprocessor=lambda x: x, vocabulary=lex_vocab)\n",
    "#Alternative way to do the same thing:\n",
    "#cv = CountVectorizer(token_pattern = r'[^ ]+', vocabulary=lex_vocab, binary=True)\n",
    "dtm = cv.fit_transform(lit_comp['lemma_mwe'])\n",
    "lit_df = pd.DataFrame(dtm.toarray(), columns= cv.get_feature_names(), index=lit_comp[\"id_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df = lit_df.loc[: , lit_df.sum(axis=0) != 0].copy()\n",
    "vocab = lit_df.columns # `vocab` is a list with all the vocabulary items currently in `lit_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/lit_lex_vocab.txt', 'w', encoding = 'utf8') as w:\n",
    "    w.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df[\"n_matches\"] = lit_df[vocab].astype(bool).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"jsonzip/epsd2-literary.zip\" # The ZIP file was downloaded in the previous notebook\n",
    "z = zipfile.ZipFile(file) \n",
    "st = z.read(\"epsd2/literary/catalogue.json\").decode(\"utf-8\")\n",
    "j = json.loads(st)\n",
    "cat_df = pd.DataFrame(j[\"members\"]).T\n",
    "#The important information, giving the title of the literary text is sometimes found in \n",
    "# `designation` and sometimes in `subgenre`. Merge those two fields.\n",
    "cat_df.loc[cat_df.designation.str[:13] == \"CDLI Literary\", \"designation\"] = cat_df.subgenre\n",
    "# Exemplars have a P number (`id_text`), composite texts have a Q number (`id_composite`).\n",
    "# Merge those two in `id_text`.\n",
    "cat_df[\"id_text\"] = cat_df[\"id_text\"].fillna(cat_df[\"id_composite\"])\n",
    "# Keep only `id_text` and `designation`.\n",
    "cat_df = cat_df[[\"id_text\", \"designation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df2 = pd.merge(lit_comp[[\"id_text\", \"length\", \"lex_var\"]], lit_df[\"n_matches\"], on=\"id_text\", how=\"inner\")\n",
    "lit_df2 = pd.merge(cat_df, lit_df2, on = 'id_text', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df2[\"norm\"] = lit_df2[\"n_matches\"] / lit_df2[\"lex_var\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Creating Output Only\n",
    "The following code is used to create MarkDown tables from Pandas DataFrames. The tables can be included in the Compass Markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 200\n",
    "lit_tab = lit_df2.copy()\n",
    "lit_tab = lit_tab.loc[lit_tab.length >= min_length]\n",
    "markdown = \"[{}](http://oracc.org/epsd2/literary/{})\"\n",
    "lit_tab['id_text'] = [markdown.format(val,val) for val in lit_df2.loc[lit_df2.length >= min_length, 'id_text']]\n",
    "lit_tab = lit_tab.round({'ttr' : 3, 'norm': 3, 'mtld' : 3})\n",
    "#lit_tab = lit_tab.loc[lit_tab.length >= 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 10 # number of rows to be exported\n",
    "col = 'norm' # column by which to sort\n",
    "asc = True\n",
    "tab = tabulate(lit_tab.sort_values(by=col, ascending=asc)[:rows],\n",
    "         headers= lit_tab.columns , tablefmt=\"github\", showindex=False)\n",
    "with open('output/lit_tab.txt', 'w', encoding='utf8') as w:\n",
    "    w.write(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Add one word at a time and see how that influences ttr. Does ttr arrive at a plateau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for id in lit_comp['id_text']:\n",
    "    c = lit_comp.loc[lit_comp['id_text'] == id, 'lemma_mwe']\n",
    "    c = c.iloc[0]\n",
    "\n",
    "    ttr_l = []\n",
    "    enum = range(1, len(c))\n",
    "    for ind in enum:\n",
    "        t = c[:ind]\n",
    "        ttr = lr(t).ttr\n",
    "        ttr_l.append(ttr)\n",
    "    plt.plot(enum, ttr_l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df2.loc[(86 < lit_df2.mtld) & (lit_df2.mtld < 162.6)].sort_values(by = 'mtld')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following needs to be redone with Q numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetrad = {'c.2.5.8.1' : 1, 'c.2.5.3.2' : 1, 'c.2.5.5.2' : 1, 'c.4.16.1': 1}\n",
    "decad = {'c.2.4.2.01' : 2, 'c.2.5.5.1' : 2, 'c.5.5.4' : 2, 'c.4.07.2' : 2, 'c.4.05.1' : 2,\n",
    "         'c.4.80.2' : 2, 'c.1.1.4' : 2, 'c.1.3.2' : 2, 'c.4.28.1' : 2, 'c.1.8.1.5' : 2}\n",
    "houseF = {'c.5.1.2' : 3,'c.5.1.3' : 3, 'c.1.8.1.4' : 3, 'c.1.6.2' : 3, 'c.2.1.5' : 3,\n",
    "          'c.2.4.2.02' : 3, 'c.2.2.2' : 3, 'c.5.6.1' : 3, 'c.5.1.1' : 3, 'c.5.3.2' : 3,\n",
    "          'c.1.4.3' : 3, 'c.5.6.3' : 3, 'c.5.4.1' : 3, 'c.5.3.1' : 3}\n",
    "proverbs = {'c.6.1.01' : 4, 'c.6.1.02' : 4, 'c.6.1.03' : 4, 'c.6.1.04' : 4, 'c.6.1.05' : 4,\n",
    "            'c.6.1.06' : 4, 'c.6.1.07' : 4,'c.6.1.08' : 4, 'c.6.1.09' : 4, 'c.6.1.10' : 4,\n",
    "            'c.6.1.11' : 4, 'c.6.1.12' : 4, 'c.6.1.13' : 4, 'c.6.1.14' : 4,'c.6.1.15' : 4,\n",
    "            'c.6.1.16' : 4, 'c.6.1.17' : 4, 'c.6.1.18' : 4, 'c.6.1.19' : 4, 'c.6.1.20' : 4,\n",
    "            'c.6.1.21' : 4, 'c.6.1.22' : 4, 'c.6.1.23' : 4, 'c.6.1.24' : 4, 'c.6.1.25' : 4,\n",
    "            'c.6.1.26' : 4, 'c.6.1.27' : 4, 'c.6.1.28' : 4, 'c.6.2.1' : 4, 'c.6.2.2' : 4,\n",
    "            'c.6.2.3' : 4,'c.6.2.4' : 4,'c.6.2.5' : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educL = {}\n",
    "educL.update(tetrad)\n",
    "educL.update(decad)\n",
    "educL.update(houseF)\n",
    "educL.update(proverbs)\n",
    "educ = lit_df2.loc[lit_df2.id_text.isin(educL)].sort_values(by = 'norm')\n",
    "educ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educ['category'] = [educL[id] for id in educ.id_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educ.sort_values(by = 'mtld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl.norm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl.loc[round(etcsl.norm, 3) == 0.874].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#colors = {1 :'red', 2:'blue', 3:'green', 4:'black'}\n",
    "#plt.scatter(educ.norm, educ.mtld, s =75, c=educ['category'].apply(lambda x: colors[x]), alpha = 1)\n",
    "sns.scatterplot('norm', 'mtld', data=educ, hue='category', size = 'length', sizes = (50, 200), alpha = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df2[\"mtld\"].corr(lit_df2['norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df2.loc[lit_df2.norm > .5].plot.scatter(x = 'mtld', y = 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educL['c.2.5.5.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = lit_df2.loc[lit_df2.norm > .6].copy()\n",
    "test_df['norm'].corr(test_df['mtld'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df2.loc[lit_df2.mtld > 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hymns = etcsl_df2.loc[etcsl_df2.id_text.str.startswith('c.2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hymns.sort_values(by = 'id_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = etcsl_df2.groupby(etcsl_df2.id_text.str[:5]).aggregate({'norm' : 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl[['mtld', 'length', 'norm', 'lex_var', 'ttr', 'n_matches']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD = set(etcsl_comp.lemma_mwe.iloc[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DD - set(lex_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl_comp.iloc[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl_df2.loc[etcsl_df2.id_text == 'c.1.4.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etcsl.norm.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Some thoughts\n",
    "\n",
    "> * Step 1. Measure length of lemma_mwe in etcsl_comp and remove rows with len < 200.\n",
    "> * Step 2. Create DTM (see below) of etcsl_comp, binary = True and vocabulary = lemma_mwe from lex (use lex_lines)\n",
    "> * Step 3. Order compositions by highest match\n",
    "> * Step 4. Normalize for text length (from Step 1)\n",
    "> * Step 5. Same process for individual lex texts (which has highest match for Ura 4?)\n",
    "> * Step 6. TF-IDF\n",
    "\n",
    "> In future iteration: do *not* select among lexical texts - let the script figure out which lex compositions are most relevant.\n",
    "\n",
    "> Perhaps: make DTM first - show that DTM.shape gives same numbers for lex vocabulary as second Venn diagram above. Remove all columns where sum == 0. Show that DTM.shape now gives total of overlap as in Venn diagram above. Then remove rows <= minimum. Tricky!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
