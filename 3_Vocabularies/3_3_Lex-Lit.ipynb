{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Looking at the Lexical Vocabulary from the Perspective of the Literary Material\n",
    "\n",
    "Goal of this notebook is to explore the connection between the literary corpus and individual lexical texts. In order to do so we will construct a full DTM of the literary vocabulary with trigrams and see which lexical texts have a larger or smaller intersection with that vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "from nltk import trigrams, bigrams\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lines = pd.read_pickle('output/litlines.p')\n",
    "lit_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make ngrams: unigrams, bigrams, and trigrams. Represent bigrams and trigrams as MWEs, connected by underscores. Create a full list of all lemmas and ngrams, omitting all non-lemmatized words (or ngrams that include non-lemmatized words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(lemmas):\n",
    "    lemmas = lemmas.split()\n",
    "    lemmas_bi = bigrams(lemmas)\n",
    "    lemmas_tri = trigrams(lemmas)\n",
    "    lemmas_n = list(lemmas_bi) + list(lemmas_tri)\n",
    "    lemmas_n = ['_'.join(lem) for lem in lemmas_n]\n",
    "    lemmas = set(lemmas + lemmas_n)\n",
    "    lemmas = [lem for lem in lemmas if not '[na]na' in lem]\n",
    "    lit_vocab.extend(lemmas)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_vocab = []\n",
    "lit_lines['lemma'].progress_apply(make_ngrams)\n",
    "lit_vocab = list(set(lit_vocab))\n",
    "lit_vocab.sort()\n",
    "lit_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: This step can be done with Countvectorizer, with setting ngrams = (1,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_comp = lit_lines.groupby(['id_text']).agg({'lemma' : ' '.join}).reset_index()\n",
    "#lit_comp['lemma'] = [lem for lem in lit_comp['lemma'] if not '[na]na' in lem] # remove unlemmatized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(token_pattern = r'[^ ]+' ngram_range = (1,3))\n",
    "dtm = tv.fit_transform(lit_comp['lemma'])\n",
    "lit_df = pd.DataFrame(dtm.toarray(), columns= tv.get_feature_names(), index=lit_comp[\"id_text\"])\n",
    "cols = [col for col in lit_df.columns if not '[na]na' in col]\n",
    "lit_df = lit_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues with the TfidfVectorizer\n",
    "\n",
    "- do we need the tf-idf scores?\n",
    "- unlemmatized words are removed after vectorizing\n",
    "- this ensures that words separated by unlemmatized words do not get into bigram/trigram\n",
    "- but it makes the tf-idf score invalid (probably very small difference)\n",
    "- words on consecutive lines become part of bigram/trigram\n",
    "- other way: use ngrams as determined above to create MWEs with MWEtokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df.columns[12000:12100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_comp.loc[lit_comp.lemma.str.contains('Å‹i\\[night\\]n ud\\[sun\\]n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
