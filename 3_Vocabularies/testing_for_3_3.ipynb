{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Looking at the Lexical Vocabulary from the Perspective of the Literary Material\n",
    "\n",
    "In section 3.2 we asked whether we can see differences between Old Babylonian literary compositions in their usage of vocabulary (lemmas and MWEs) attested in the lexical corpus. In this notebook we will change perspective and ask: are there particular lexical texts (or groups of lexical texts) that show a greater affinity with literary vocabulary than others?\n",
    "\n",
    "In 3.1 and 3.2 we used Multiple Word Expressions, connecting words that are found in a lexical entry by underscrores (using `MWEtokenizer()` from the nltk module). The lemmas and MWE were fed into the `Countvectorizer()` to create a Document Term Matrix.\n",
    "\n",
    "In this notebook we will use the ngram option of the `CountVectorizer()` function in order to find sequences of lemmas that are shared between lexical and literary texts. N-gram is a continuous sequence of *n* words (or lemmas). Any sequence of *n* words will form an ngram, but only sequences that are more or less standardized will appear with some frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # this suppresses a warning about pandas from tqdm\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() # initiate pandas support in tqdm, allowing progress_apply() and progress_map()\n",
    "import os\n",
    "import sys\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from lexicalrichness_v import LexicalRichness as lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the files `litlines.p` and `lexlines.p` which were produced in [3_1_Lit_Lex_Vocab.ipynb](./3_1_Lit_Lex_Vocab.ipynb). The files contain the pickled versions of the DataFrames `lit_lines` and `lex_lines` in which the literary ([epsd2/literary](http://oracc.org/epsd2/literary)) and lexical ([dcclt](http://oracc.org/dcclt)) corpora are represented in line-by-line format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_lines = pd.read_pickle('output/litlines.p')\n",
    "lex_lines = pd.read_pickle('output/lexlines.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of all lexical vocabulary items, including both lexical *entries* and all the individual lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(lex_lines['lemma'])\n",
    "vocab2 = [v.split() for v in vocab] # this creates a list of list\n",
    "vocab2 = [item for sublist in vocab2 for item in sublist] # flatten the list of lists\n",
    "vocab_s = set(vocab) | set(vocab2) # join the lexical entries (vocab) and the individual words (vocab2)\n",
    "vocab_l = list(vocab_s) \n",
    "vocab_l = [v for v in vocab_l if not '[na]na' in v] # remove unlemmatized words and entries that contain\n",
    "                                                    # unlemmatized words.\n",
    "vocab_l.sort()\n",
    "length = [len(v.split()) for v in vocab_l] # determine the length (in words) of lexical entries\n",
    "m = max(length) # determine the maximum length of a lexical entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer with Ngrams\n",
    "We will use `CountVectorizer()` slightly differently from how it was used in [3_2_Lit_Lex.ipynb](./3_2_Lit_Lex.ipynb). The main difference is the inclusion of the argument ngram_range = (1, m), where `m` represents the maximum length of a lexical entry, computed above. `Countvectorizer()` will count the number of times each wordd appears (ngram n=1), but also the number of times each sequence of two words (or rather two lemmas) appears (bigram; n=2), etc. The sentence **inana-ra lugal-e e₂-a-ni mu-un-du₃** (\"The king build her temple for Inana\") lemmatized as Inana\\[1\\]dn lugal\\[king\\]n e\\[house\\]n du\\[build\\]v/t, will be represented as:\n",
    "\n",
    "| type             | representation  |\n",
    "|------------------|-----------------|\n",
    "| unigram        | Inana\\[1\\]dn |\n",
    "|                    | lugal\\[king\\]n |\n",
    "|                    | e\\[house\\]n |\n",
    "|                    | du\\[build\\]v/t |\n",
    "| bigram             | Inana\\[1\\]dn lugal\\[king\\]n |\n",
    "|                    | lugal\\[king\\]n e\\[house\\]n |\n",
    "|                    | e\\[house\\]n du\\[build\\]v/t |\n",
    "| trigram | Inana\\[1\\]dn lugal\\[king\\]n e\\[house\\]n |\n",
    "|                     | lugal\\[king\\]n e\\[house\\]n du\\[build\\]v/t |\n",
    "| 4-gram            | Inana\\[1\\]dn lugal\\[king\\]n e\\[house\\]n du\\[build\\]v/t |\n",
    "\n",
    "This four-word sentence thus results in 10 columns in the Document Term Matrix for `m=4`. We will use `CountVectorizer()` on the representation of literary texts in *lines* so that the ngrams do not extent over the end of a line. Afterwards, lines are combined into compositions. \n",
    "\n",
    "`CountVectorizer()` uses the vocabulary `vocab_l`, which contains all lexical lemmas and lexical entries (produced above). Lemmas and ngrams not found in `vocab_l` are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=lambda x: x.split(), preprocessor=lambda x: x, \\\n",
    "                     ngram_range = (1,m), vocabulary = vocab_l)\n",
    "dtm = cv.fit_transform(lit_lines['lemma'])\n",
    "df = pd.DataFrame(dtm.toarray(), columns= cv.get_feature_names(), index=lit_lines[\"id_text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Note\n",
    "It may be more efficient to remove zero-columns from the sparse matrix before converting to DataFrame. This is possible but tricky.\n",
    "1. remove 0 columns\n",
    "```python\n",
    "nonzeros = np.unique(dtm.nonzero()[1])\n",
    "dtm = dtm[:, nonzeros]\n",
    "```\n",
    "2. remove the right column names\n",
    "```python\n",
    "col = cv.get_feature_names()\n",
    "col = [col[i] for i in nonzeros]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Lines into Compositions\n",
    "Combine lines into compositions with groupby and aggregate. Note that the number of columns is identical to the number of lexical items in the Venn diagram in 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['id_text']).agg(sum)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Empty Columns\n",
    "Lemmas and entries in `vocab_l` (the lexical vocabulary) that have no match in the literary corpus are represented in the DataFrame with a column filled with zeros; such columns are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[: , df.sum(axis=0) != 0].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may notice that the shape of the DataFrame suggests that the outcome here is slightly different from what we saw in 3.1. The number of columns represents the number of shared entries (lemmas, ngrams) between the lexical and the literary corpus. The intersection in the Venn diagram in 3.1 had a slightly lower number.\n",
    "\n",
    "The difference is a difference in approach: Multiple Word Expressions vs. ngrams. In 3.1 (and 3.2) we used MWEs, connecting words with underscores. In the MWE approach the literary line 'amar\\[young\\]n_ga\\[milk\\]n_gu\\[eat\\]v/t' matches the lexical entry 'amar\\[young\\]n_ga\\[milk\\]n_gu\\[eat\\]v/t' but not 'amar\\[young\\]n_ga\\[milk\\]n'. In the ngram approach this will yield matches for unigrams, bigrams and a trigram, namely: 'amar\\[young\\]n', 'ga\\[milk\\]n', 'gu\\[eat\\]v/t', 'amar\\[young\\]n ga\\[milk\\]n', and 'amar\\[young\\]n ga\\[milk\\]n gu\\[eat\\]v/t' (the bigram 'ga\\[milk\\]n gu\\[eat\\]v/t' is not a lexical entry and will therefore not result in a match).\n",
    "\n",
    "While this may seem like a big difference, in outcome the differences between these two approaches are minor because most literary ngrams do not result in a match with the literary vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Lexical Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_lines = lex_lines.loc[~lex_lines.lemma.str.contains('\\[na\\]na')]\n",
    "lex_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Case: OB Nippur Ura 6\n",
    "The sixth chapter of the Old Babylonian Nippur version of the thematic list Ura deals with foodstuffs and drinks. This chapter was not standardized (each exemplar has its own order of items and sections) and therefore no composite text has been created in [DCCLT](http://oracc.org/dcclt). Instead, the \"composite\" of [OB Nippur Ura 6](http://oracc.org/dcclt/Q000043) consists of the concatenation of all known Nippur exemplars of the list of foodstuffs. In our current dataframe, therefore, there are no lines where the field `id_text` equals \"dcclt/Q000043\".\n",
    "\n",
    "We create a \"composite\" by changing the field `id_text` in all exemplars of [OB Nippur Ura 6](http://oracc.org/dcclt/Q000043) to \"dcclt/Q000043\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ura6 = [\"dcclt/P227657\",\n",
    "\"dcclt/P227743\",\n",
    "\"dcclt/P227791\",\n",
    "\"dcclt/P227799\",\n",
    "\"dcclt/P227925\",\n",
    "\"dcclt/P227927\",\n",
    "\"dcclt/P227958\",\n",
    "\"dcclt/P227967\",\n",
    "\"dcclt/P227979\",\n",
    "\"dcclt/P228005\",\n",
    "\"dcclt/P228008\",\n",
    "\"dcclt/P228200\",\n",
    "\"dcclt/P228359\",\n",
    "\"dcclt/P228368\",\n",
    "\"dcclt/P228488\",\n",
    "\"dcclt/P228553\",\n",
    "\"dcclt/P228562\",\n",
    "\"dcclt/P228663\",\n",
    "\"dcclt/P228726\",\n",
    "\"dcclt/P228831\",\n",
    "\"dcclt/P228928\",\n",
    "\"dcclt/P229015\",\n",
    "\"dcclt/P229093\",\n",
    "\"dcclt/P229119\",\n",
    "\"dcclt/P229304\",\n",
    "\"dcclt/P229332\",\n",
    "\"dcclt/P229350\",\n",
    "\"dcclt/P229351\",\n",
    "\"dcclt/P229352\",\n",
    "\"dcclt/P229353\",\n",
    "\"dcclt/P229354\",\n",
    "\"dcclt/P229356\",\n",
    "\"dcclt/P229357\",\n",
    "\"dcclt/P229358\",\n",
    "\"dcclt/P229359\",\n",
    "\"dcclt/P229360\",\n",
    "\"dcclt/P229361\",\n",
    "\"dcclt/P229362\",\n",
    "\"dcclt/P229365\",\n",
    "\"dcclt/P229366\",\n",
    "\"dcclt/P229367\",\n",
    "\"dcclt/P229890\",\n",
    "\"dcclt/P229925\",\n",
    "\"dcclt/P230066\",\n",
    "\"dcclt/P230208\",\n",
    "\"dcclt/P230230\",\n",
    "\"dcclt/P230530\",\n",
    "\"dcclt/P230586\",\n",
    "\"dcclt/P231095\",\n",
    "\"dcclt/P231128\",\n",
    "\"dcclt/P231424\",\n",
    "\"dcclt/P231446\",\n",
    "\"dcclt/P231453\",\n",
    "\"dcclt/P231458\",\n",
    "\"dcclt/P231742\",\n",
    "\"dcclt/P266520\"]\n",
    "lex_lines.loc[lex_lines[\"id_text\"].isin(Ura6), \"id_text\"] = \"dcclt/Q000043\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_comp = lex_lines.groupby(['id_text']).agg({'lemma': ' '.join}).reset_index()\n",
    "lex_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data are drawn from multiple (sub)projects, it is possible that there are duplicates. We take the version with the largest number of (lemmatized) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_comp['id_text'] = [i[-7:] for i in lex_comp['id_text']]\n",
    "lex_comp['length'] = [len(lem.split()) for lem in lex_comp['lemma']]\n",
    "lex_comp = lex_comp.sort_values(by = 'length', ascending = False)\n",
    "lex_comp = lex_comp.drop_duplicates(subset = 'id_text', keep = 'first')\n",
    "lex_comp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
